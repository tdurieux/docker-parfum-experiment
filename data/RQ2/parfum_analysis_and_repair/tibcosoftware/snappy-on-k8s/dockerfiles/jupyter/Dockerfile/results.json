{
  "startTime": 1674220886184,
  "endTime": 1674220887208,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 93,
        "lineEnd": 96,
        "columnStart": 5,
        "columnEnd": 20
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\n\n# Refer to https://github.com/SnappyDataInc/spark-on-k8s/tree/master/docs/building-images.md#jupyter-image\n# for instructions to build the Docker image.\n# This Dockerfile should be present in the same directory where spark-on-k8s distribution\n# directory (spark-2.2.0-k8s-0.5.0-bin-2.7.3) is kept.\n\nFROM jupyter/scipy-notebook\n\nUSER root\n\n# Copied from pyspark notebook- start\n# Spark dependencies\nENV APACHE_SPARK_VERSION 2.2.0\nENV HADOOP_VERSION 2.7\n\nRUN apt-get -y update && \\\n    apt-get install --no-install-recommends -y openjdk-8-jre-headless ca-certificates-java && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Copied from pyspark notebook- end\n\n####### Begin changes for Spark-on-k8s #################\n\nRUN mkdir -p /opt/spark && \\\n    mkdir -p /opt/spark/work-dir \\\n    touch /opt/spark/RELEASE && \\\n    rm -f /bin/sh && \\\n    ln -sv /bin/bash /bin/sh && \\\n    chgrp root /etc/passwd && chmod ug+rw /etc/passwd\n\nCOPY spark-2.2.0-k8s-0.5.0-bin-2.7.3/jars /opt/spark/jars\nCOPY spark-2.2.0-k8s-0.5.0-bin-2.7.3/bin /opt/spark/bin\nCOPY spark-2.2.0-k8s-0.5.0-bin-2.7.3/sbin /opt/spark/sbin\nCOPY spark-2.2.0-k8s-0.5.0-bin-2.7.3/conf /opt/spark/conf\nCOPY spark-2.2.0-k8s-0.5.0-bin-2.7.3/dockerfiles/spark-base/entrypoint.sh /opt/\n\nADD spark-2.2.0-k8s-0.5.0-bin-2.7.3/examples /opt/spark/examples\nADD spark-2.2.0-k8s-0.5.0-bin-2.7.3/python /opt/spark/python\n\n# Copy aws and gcp jars\n# COPY aws_gcp_jars/hadoop-aws-2.7.3.jar /opt/spark/jars\n# COPY aws_gcp_jars/aws-java-sdk-1.7.4.jar /opt/spark/jars\n# COPY aws_gcp_jars/gcs-connector-latest-hadoop2.jar /opt/spark/jars\n\nENV SPARK_HOME /opt/spark\n\nENV PYTHON_VERSION 2.7.13\nENV PYSPARK_PYTHON python\nENV PYSPARK_DRIVER_PYTHON python\nENV PYTHONPATH ${SPARK_HOME}/python/:${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${PYTHONPATH}\n\nCMD SPARK_CLASSPATH=\"${SPARK_HOME}/jars/*\" && \\\n    env | grep SPARK_JAVA_OPT_ | sed 's/[^=]*=\\(.*\\)/\\1/g' > /tmp/java_opts.txt && \\\n    readarray -t SPARK_DRIVER_JAVA_OPTS < /tmp/java_opts.txt && \\\n    if ! [ -z ${SPARK_MOUNTED_CLASSPATH+x} ]; then SPARK_CLASSPATH=\"$SPARK_MOUNTED_CLASSPATH:$SPARK_CLASSPATH\"; fi && \\\n    if ! [ -z ${SPARK_SUBMIT_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH=\"$SPARK_SUBMIT_EXTRA_CLASSPATH:$SPARK_CLASSPATH\"; fi && \\\n    if ! [ -z ${SPARK_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH=\"$SPARK_EXTRA_CLASSPATH:$SPARK_CLASSPATH\"; fi && \\\n    if ! [ -z ${SPARK_MOUNTED_FILES_DIR+x} ]; then cp -R \"$SPARK_MOUNTED_FILES_DIR/.\" .; fi && \\\n    if ! [ -z ${SPARK_MOUNTED_FILES_FROM_SECRET_DIR+x} ]; then cp -R \"$SPARK_MOUNTED_FILES_FROM_SECRET_DIR/.\" .; fi && \\\n    ${JAVA_HOME}/bin/java \"${SPARK_DRIVER_JAVA_OPTS[@]}\" -cp \"$SPARK_CLASSPATH\" -Xms$SPARK_DRIVER_MEMORY -Xmx$SPARK_DRIVER_MEMORY -Dspark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS $SPARK_DRIVER_CLASS $PYSPARK_PRIMARY $PYSPARK_FILES $SPARK_DRIVER_ARGS\n\n\n# Copied from pyspark notebook- start\n# Spark config\nENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip\nENV SPARK_OPTS --driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info\n# Copied from pyspark notebook- end\n\nRUN chown -R $NB_USER:users /opt/spark\n\n####### End changes for Spark-on-k8s ##########################\n\nUSER $NB_USER\n\n# Added to anble python2 notebooks\nRUN conda create --quiet --yes \\\n     -n ipykernel_py2 python=2 ipykernel && \\\n     source activate ipykernel_py2 && \\\n     python -m ipykernel install --user\n\n\nRUN source activate ipykernel_py2 && \\\n     conda install --yes \\\n     matplotlib \\\n     scipy \\\n     numpy \\\n     pandas \\ \n     nltk \\\n     tensorflow && \\\n     source activate ipykernel_py2 && \\\n     pip install --no-cache-dir \\\n     sklearn \\\n     wordcloud \\\n     treeinterpreter\n\n"
}
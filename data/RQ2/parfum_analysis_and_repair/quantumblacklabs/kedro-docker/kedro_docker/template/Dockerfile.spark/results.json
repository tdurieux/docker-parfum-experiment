{
  "startTime": 1674250360661,
  "endTime": 1674250361785,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 8,
        "lineEnd": 8,
        "columnStart": 4,
        "columnEnd": 109
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 22,
        "lineEnd": 22,
        "columnStart": 4,
        "columnEnd": 115
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 44,
        "lineEnd": 44,
        "columnStart": 4,
        "columnEnd": 40
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 4,
        "lineEnd": 4,
        "columnStart": 22,
        "columnEnd": 61
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "ARG BASE_IMAGE=python:3.6-buster\nFROM $BASE_IMAGE\n\n# install Hadoop\nRUN apt-get update && apt-get install --no-install-recommends -y default-jdk-headless && rm -rf /var/lib/apt/lists/*\nENV JAVA_HOME \"/usr/lib/jvm/default-java\"\nARG HADOOP_VERSION=\"3.1.1\"\nENV HADOOP_HOME \"/opt/hadoop\"\nRUN curl -f https://archive.apache.org/dist/hadoop/core/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz \\\n    | tar xz -C /opt && mv /opt/hadoop-${HADOOP_VERSION} ${HADOOP_HOME}\nENV HADOOP_COMMON_HOME \"${HADOOP_HOME}\"\nENV HADOOP_CLASSPATH \"${HADOOP_HOME}/share/hadoop/tools/lib/*\"\nENV HADOOP_CONF_DIR \"${HADOOP_HOME}/etc/hadoop\"\nENV PATH \"$PATH:${HADOOP_HOME}/bin\"\nENV HADOOP_OPTS \"$HADOOP_OPTS -Djava.library.path=${HADOOP_HOME}/lib\"\nENV HADOOP_COMMON_LIB_NATIVE_DIR \"${HADOOP_HOME}/lib/native\"\nENV YARN_CONF_DIR \"${HADOOP_HOME}/etc/hadoop\"\n\n# install Spark\nARG SPARK_VERSION=\"2.3.1\"\nARG PY4J_VERSION=\"0.10.7\"\nENV SPARK_HOME \"/opt/spark\"\nRUN curl -f https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz \\\n    | tar xz -C /opt && mv /opt/spark-${SPARK_VERSION}-bin-without-hadoop ${SPARK_HOME}\nENV PATH \"$PATH:${SPARK_HOME}/bin\"\nENV LD_LIBRARY_PATH \"${HADOOP_HOME}/lib/native\"\nENV SPARK_DIST_CLASSPATH \"${HADOOP_HOME}/etc/hadoop\\\n:${HADOOP_HOME}/share/hadoop/common/lib/*\\\n:${HADOOP_HOME}/share/hadoop/common/*\\\n:${HADOOP_HOME}/share/hadoop/hdfs\\\n:${HADOOP_HOME}/share/hadoop/hdfs/lib/*\\\n:${HADOOP_HOME}/share/hadoop/hdfs/*\\\n:${HADOOP_HOME}/share/hadoop/yarn/lib/*\\\n:${HADOOP_HOME}/share/hadoop/yarn/*\\\n:${HADOOP_HOME}/share/hadoop/mapreduce/lib/*\\\n:${HADOOP_HOME}/share/hadoop/mapreduce/*\\\n:${HADOOP_HOME}/share/hadoop/tools/lib/*\\\n:${HADOOP_HOME}/contrib/capacity-scheduler/*.jar\"\nENV PYSPARK_PYTHON \"/usr/local/bin/python\"\nENV PYTHONPATH \"${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-${PY4J_VERSION}-src.zip:${PYTHONPATH}\"\nENV SPARK_OPTS \"--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info\"\n\n# install project requirements\nCOPY src/requirements.txt /tmp/requirements.txt\nRUN pip install --no-cache-dir -r /tmp/requirements.txt && rm -f /tmp/requirements.txt\n\n# add kedro user\nARG KEDRO_UID=999\nARG KEDRO_GID=0\nRUN groupadd -f -g ${KEDRO_GID} kedro_group && \\\nuseradd -d /home/kedro -s /bin/bash -g ${KEDRO_GID} -u ${KEDRO_UID} kedro\n\n# copy the whole project except what is in .dockerignore\nWORKDIR /home/kedro\nCOPY . .\nRUN chown -R kedro:${KEDRO_GID} /home/kedro\nUSER kedro\nRUN chmod -R a+w /home/kedro\n\nEXPOSE 8888\n\nCMD [\"kedro\", \"run\"]\n"
}
{
  "startTime": 1674254233251,
  "endTime": 1674254234066,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 16,
        "lineEnd": 16,
        "columnStart": 7,
        "columnEnd": 38
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 10,
        "lineEnd": 14,
        "columnStart": 7,
        "columnEnd": 20
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# To run spidy in a container and write all files back to the host filesystem:\n#   docker run --rm -it -v $PWD:/data spidy\n\nFROM python:3.6\nLABEL maintainer \"Peter Benjamin <petermbenjamin@gmail.com>\"\nWORKDIR /src/app/\nCOPY . .\nVOLUME [ \"/data\" ]\n\nRUN apt-get update \\\n    && apt-get install -y \\\n    --no-install-recommends \\\n    python3 \\\n    python3-lxml \\\n    python3-requests \\\n    && rm -rf /var/cache/apt/* \\\n    && pip install --no-cache-dir -r requirements.txt && rm -rf /var/lib/apt/lists/*;\n\nENTRYPOINT [ \"python\", \"spidy/crawler.py\" ]\n"
}
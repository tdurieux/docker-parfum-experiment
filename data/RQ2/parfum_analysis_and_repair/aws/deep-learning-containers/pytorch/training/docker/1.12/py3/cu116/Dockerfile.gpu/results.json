{
  "startTime": 1674255500781,
  "endTime": 1674255503275,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 117,
        "lineEnd": 117,
        "columnStart": 4,
        "columnEnd": 104
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 145,
        "lineEnd": 145,
        "columnStart": 4,
        "columnEnd": 150
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 206,
        "lineEnd": 206,
        "columnStart": 4,
        "columnEnd": 92
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 302,
        "lineEnd": 302,
        "columnStart": 4,
        "columnEnd": 108
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 519,
        "lineEnd": 519,
        "columnStart": 4,
        "columnEnd": 108
      }
    },
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 485,
        "lineEnd": 485,
        "columnStart": 4,
        "columnEnd": 76
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 180,
        "lineEnd": 180,
        "columnStart": 4,
        "columnEnd": 91
      }
    },
    {
      "rule": "configureShouldUseBuildFlag",
      "position": {
        "lineStart": 134,
        "lineEnd": 134,
        "columnStart": 4,
        "columnEnd": 86
      }
    },
    {
      "rule": "configureShouldUseBuildFlag",
      "position": {
        "lineStart": 258,
        "lineEnd": 261,
        "columnStart": 4,
        "columnEnd": 44
      }
    },
    {
      "rule": "configureShouldUseBuildFlag",
      "position": {
        "lineStart": 386,
        "lineEnd": 389,
        "columnStart": 4,
        "columnEnd": 44
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 132,
        "lineEnd": 132,
        "columnStart": 4,
        "columnEnd": 42
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 252,
        "lineEnd": 252,
        "columnStart": 22,
        "columnEnd": 49
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 380,
        "lineEnd": 380,
        "columnStart": 22,
        "columnEnd": 49
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 252,
        "lineEnd": 252,
        "columnStart": 22,
        "columnEnd": 49
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 380,
        "lineEnd": 380,
        "columnStart": 22,
        "columnEnd": 49
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "ARG PYTHON=python3\nARG PYTHON_VERSION=3.8.13\nARG PYTHON_SHORT_VERSION=3.8\nARG MAMBA_VERSION=4.12.0-2\n\n# PyTorch Binaries\nARG PT_E3_TRAINING_URL=https://aws-pytorch-unified-cicd-binaries.s3.us-west-2.amazonaws.com/r1.12.0_e3/20220628-223427/c1e155d3cbd8d4115c80866bed4c717775363e7a/torch-1.12.0%2Bcu116-cp38-cp38-linux_x86_64.whl\nARG PT_SM_TRAINING_URL=https://aws-pytorch-cicd-v3-binaries.s3.us-west-2.amazonaws.com/r1.11.0_v3_sm/20220404-193901/15c50b561b1a638dacc59db932cab0fcb8d60d10/gpu/torch-1.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl\nARG PT_TORCHVISION_URL=https://download.pytorch.org/whl/cu116/torchvision-0.13.0%2Bcu116-cp38-cp38-linux_x86_64.whl\nARG PT_TORCHAUDIO_URL=https://download.pytorch.org/whl/cu116/torchaudio-0.12.0%2Bcu116-cp38-cp38-linux_x86_64.whl\nARG PT_TORCHDATA_URL=https://download.pytorch.org/whl/test/torchdata-0.4.0-cp38-cp38-linux_x86_64.whl\nARG PT_S3_WHL_GPU=https://aws-s3-plugin.s3.us-west-2.amazonaws.com/binaries/0.0.1/1c3e69e/awsio-0.0.1-cp38-cp38-manylinux1_x86_64.whl\n\n# SMD binaries\nARG SMD_DATA_PARALLEL_URL=https://smdataparallel.s3.amazonaws.com/binary/pytorch/1.11.0/cu113/2022-04-14/smdistributed_dataparallel-1.4.1-cp38-cp38-linux_x86_64.whl\nARG SMD_MODEL_PARALLEL_URL=https://sagemaker-distributed-model-parallel.s3.us-west-2.amazonaws.com/pytorch-1.11.0/build-artifacts/2022-04-20-17-05/smdistributed_modelparallel-1.9.0-cp38-cp38-linux_x86_64.whl\n\nFROM nvidia/cuda:11.6.2-base-ubuntu20.04 AS common\n\nLABEL maintainer=\"Amazon AI\"\nLABEL dlc_major_version=\"1\"\n\nARG PYTHON\nARG PYTHON_VERSION\nARG PYTHON_SHORT_VERSION\nARG MAMBA_VERSION\n\nARG CUBLAS_VERSION=11.9.2.110\nARG EFA_PATH=/opt/amazon/efa\nARG CUDA_HOME=/usr/local/cuda\n\n# This arg required to stop docker build waiting for region configuration while installing tz data from ubuntu 20\nARG DEBIAN_FRONTEND=noninteractive\n\n# Python wonâ€™t try to write .pyc or .pyo files on the import of source modules\n# Force stdin, stdout and stderr to be totally unbuffered. Good for logging\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nENV LD_LIBRARY_PATH=\"/usr/local/lib:${LD_LIBRARY_PATH}\"\nENV LD_LIBRARY_PATH=\"/opt/conda/lib:${LD_LIBRARY_PATH}\"\nENV PYTHONIOENCODING=UTF-8\nENV LANG=C.UTF-8\nENV LC_ALL=C.UTF-8\nENV PATH /opt/conda/bin:$PATH\nENV TORCH_CUDA_ARCH_LIST=\"3.7 5.0 7.0+PTX 8.0\"\nENV TORCH_NVCC_FLAGS=\"-Xfatbin -compress-all\"\n\nENV CUDNN_VERSION=8.4.0.27\nENV NCCL_VERSION=2.10.3\nENV HOROVOD_VERSION=0.24.3\nENV EFA_VERSION=1.16.0\nENV OMPI_VERSION=4.1.1\nENV BRANCH_OFI=1.3.0-aws\n\nENV CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\"\nENV OPEN_MPI_PATH=/opt/amazon/openmpi\n\nENV DGLBACKEND=pytorch\nENV MANUAL_BUILD=0\nENV RDMAV_FORK_SAFE=1\nENV DLC_CONTAINER_TYPE=training\n\nRUN apt-get update \\\n # TODO: Remove systemd upgrade once it is updated in base image\n && apt-get -y upgrade --only-upgrade systemd \\\n && apt-get install -y --allow-change-held-packages --no-install-recommends \\\n    build-essential \\\n    ca-certificates \\\n    cmake \\\n    cuda-command-line-tools-11-6 \\\n    cuda-cudart-11-6 \\\n    cuda-libraries-11-6 \\\n    curl \\\n    emacs \\\n    git \\\n    hwloc \\\n    jq \\\n    libcublas-11-6=${CUBLAS_VERSION}-1 \\\n    libcublas-dev-11-6=${CUBLAS_VERSION}-1 \\\n    libcudnn8=$CUDNN_VERSION-1+cuda11.6 \\\n    libcufft-dev-11-6 \\\n    libcurand-dev-11-6 \\\n    libcurl4-openssl-dev \\\n    libcusolver-dev-11-6 \\\n    libcusparse-dev-11-6 \\\n    libglib2.0-0 \\\n    libgl1-mesa-glx \\\n    libsm6 \\\n    libxext6 \\\n    libxrender-dev \\\n    libgomp1 \\\n    libibverbs-dev \\\n    libhwloc-dev \\\n    libnuma1 \\\n    libnuma-dev \\\n    libssl1.1 \\\n    libssl-dev \\\n    libtool \\\n    openssl \\\n    python3-dev \\\n    unzip \\\n    vim \\\n    wget \\\n    zlib1g-dev \\\n && rm -rf /var/lib/apt/lists/* \\\n && apt-get clean\n\n# Install NCCL\nRUN cd /tmp \\\n && git clone https://github.com/NVIDIA/nccl.git -b v${NCCL_VERSION}-1 \\\n && cd nccl \\\n && make -j64 src.build BUILDDIR=/usr/local \\\n && rm -rf /tmp/nccl\n\n# Install EFA alone without AWS OPEN_MPI\nRUN mkdir /tmp/efa \\\n && cd /tmp/efa \\\n && curl -f -O https://s3-us-west-2.amazonaws.com/aws-efa-installer/aws-efa-installer-${EFA_VERSION}.tar.gz \\\n && tar -xf aws-efa-installer-${EFA_VERSION}.tar.gz \\\n && cd aws-efa-installer \\\n && apt-get update \\\n && ./efa_installer.sh -y --skip-kmod -g \\\n && rm -rf $OPEN_MPI_PATH \\\n && rm -rf /tmp/efa \\\n && rm -rf /tmp/aws-efa-installer-${EFA_VERSION}.tar.gz \\\n && rm -rf /var/lib/apt/lists/* \\\n && apt-get clean\n\n# Install OpenMPI without libfabric support\nRUN mkdir /tmp/openmpi \\\n && cd /tmp/openmpi \\\n && wget --quiet https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-${OMPI_VERSION}.tar.gz \\\n && tar zxf openmpi-${OMPI_VERSION}.tar.gz \\\n && cd openmpi-${OMPI_VERSION} \\\n && ./configure --build=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" --enable-orterun-prefix-by-default --prefix=$OPEN_MPI_PATH --with-cuda \\\n && make -j $(nproc) all \\\n && make install \\\n && ldconfig \\\n && cd / \\\n && rm -rf /tmp/openmpi && rm openmpi-${OMPI_VERSION}.tar.gz\n\nENV PATH=\"$OPEN_MPI_PATH/bin:$PATH\"\nENV LD_LIBRARY_PATH=$OPEN_MPI_PATH/lib/:$EFA_PATH/lib/:$LD_LIBRARY_PATH\n\nRUN ompi_info --parsable --all | grep mpi_built_with_cuda_support:value \\\n && curl -f -L -o ~/mambaforge.sh https://github.com/conda-forge/miniforge/releases/download/${MAMBA_VERSION}/Mambaforge-${MAMBA_VERSION}-Linux-x86_64.sh \\\n && chmod +x ~/mambaforge.sh \\\n && ~/mambaforge.sh -b -p /opt/conda \\\n && rm ~/mambaforge.sh \\\n && /opt/conda/bin/conda install -c conda-forge \\\n    python=$PYTHON_VERSION \\\n    cython \\\n    mkl \\\n    mkl-include \\\n    parso \\\n    typing \\\n    h5py \\\n    requests \\\n    future \\\n    \"pyopenssl>=17.5.0\" \\\n    libgcc \\\n    # Below 2 are included in miniconda base, but not mamba so need to install\n    conda-content-trust \\\n    charset-normalizer \\\n && /opt/conda/bin/conda install -c dglteam -y dgl-cuda11.6 \\\n && /opt/conda/bin/conda install -c pytorch magma-cuda116 \\\n # Upstream conda looks to have moved to 4.13 which is incompatible with mamba 0.22.1 and will fail the conda-forge installs.\n # having \"conda update conda\" before the \"conda -c conda-forge\" commands will automatically update conda to 4.13.\n # Moving conda update conda\" after the \"conda -c conda-forge\" commands keep conda at 4.12 but will update other packages using\n # the current conda 4.12\n && /opt/conda/bin/conda update conda \\\n && /opt/conda/bin/conda clean -ya\n\n# Conda installs links for libtinfo.so.6 and libtinfo.so.6.2 both\n# Which causes \"/opt/conda/lib/libtinfo.so.6: no version information available\" warning\n# Removing link for libtinfo.so.6. This change is needed only for ubuntu 20.04-conda, and can be reverted\n# once conda fixes the issue\nRUN rm -rf /opt/conda/lib/libtinfo.so.6\n\nRUN /opt/conda/bin/conda config --set ssl_verify False \\\n && pip install --no-cache-dir --upgrade pip --trusted-host pypi.org --trusted-host \\\n && ln -s /opt/conda/bin/pip /usr/local/bin/pip3\n\nWORKDIR /root\n\nRUN pip install --no-cache-dir -U \\\n    \"awscli<2\" \\\n    boto3 \\\n    click \\\n    cmake==3.18.2.post1 \\\n    \"cryptography>3.2\" \\\n    ipython==8.1.0 \\\n    mpi4py==3.1.3 \\\n    numpy==1.22.2 \\\n    opencv-python \\\n    packaging \\\n    \"Pillow>=9.0.0\" \\\n    psutil \\\n    pybind11 \\\n    \"pyyaml>=5.4,<5.5\" \\\n    scipy\n\nCOPY deep_learning_container.py /usr/local/bin/deep_learning_container.py\n\nRUN chmod +x /usr/local/bin/deep_learning_container.py\n\nRUN curl -f -o /license.txt https://aws-dlc-licenses.s3.amazonaws.com/pytorch-1.12/license.txt\n\n##############################################################################\n#  _____ _____   ___                              ____           _\n# | ____|___ /  |_ _|_ __ ___   __ _  __ _  ___  |  _ \\ ___  ___(_)_ __   ___\n# |  _|   |_ \\   | || '_ ` _ \\ / _` |/ _` |/ _ \\ | |_) / _ \\/ __| | '_ \\ / _ \\\n# | |___ ___) |  | || | | | | | (_| | (_| |  __/ |  _ <  __/ (__| | |_) |  __/\n# |_____|____/  |___|_| |_| |_|\\__,_|\\__, |\\___| |_| \\_\\___|\\___|_| .__/ \\___|\n#                                    |___/                        |_|\n##############################################################################\n\nFROM common AS e3\n\nARG PYTHON\nARG PYTHON_VERSION\nARG PYTHON_SHORT_VERSION\n\n# PyTorch Binaries\nARG PT_E3_TRAINING_URL\nARG PT_TORCHVISION_URL\nARG PT_TORCHAUDIO_URL\nARG PT_TORCHDATA_URL\nARG PT_S3_WHL_GPU\n\n# Install AWS-PyTorch and other torch packages\nRUN pip uninstall -y torch torchvision torchaudio torchdata \\\n && pip install --no-cache-dir -U ${PT_E3_TRAINING_URL} ${PT_TORCHVISION_URL} ${PT_TORCHAUDIO_URL} ${PT_TORCHDATA_URL} torchnet\n\n# Install Nvidia Apex\n## Pin apex commit requested by sm-model-parallel team\nRUN git clone https://github.com/NVIDIA/apex && \\\n    cd apex && \\\n    git checkout aa756ce && \\\n    pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n\n# Configure Open MPI and configure NCCL parameters\nRUN mv $OPEN_MPI_PATH/bin/mpirun $OPEN_MPI_PATH/bin/mpirun.real \\\n && echo '#!/bin/bash' > $OPEN_MPI_PATH/bin/mpirun \\\n && echo \"${OPEN_MPI_PATH}/bin/mpirun.real --allow-run-as-root \\\"\\$@\\\"\" >> $OPEN_MPI_PATH/bin/mpirun \\\n && chmod a+x $OPEN_MPI_PATH/bin/mpirun \\\n && echo \"hwloc_base_binding_policy = none\" >> $OPEN_MPI_PATH/etc/openmpi-mca-params.conf \\\n && echo \"rmaps_base_mapping_policy = slot\" >> $OPEN_MPI_PATH/etc/openmpi-mca-params.conf \\\n && echo NCCL_DEBUG=INFO >> /etc/nccl.conf \\\n && echo NCCL_SOCKET_IFNAME=^docker0 >> /etc/nccl.conf\n\n# Install AWS OFI NCCL plug-in\nRUN apt-get update && apt-get install --no-install-recommends -y autoconf && rm -rf /var/lib/apt/lists/*;\nRUN mkdir /tmp/efa-ofi-nccl \\\n && cd /tmp/efa-ofi-nccl \\\n && git clone https://github.com/aws/aws-ofi-nccl.git -b v${BRANCH_OFI} \\\n && cd aws-ofi-nccl \\\n && ./autogen.sh \\\n && ./configure --build=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" --with-libfabric=/opt/amazon/efa \\\n  --with-mpi=/opt/amazon/openmpi \\\n  --with-cuda=/usr/local/cuda \\\n  --with-nccl=/usr/local --prefix=/usr/local \\\n && make \\\n && make install \\\n && rm -rf /tmp/efa-ofi-nccl \\\n && rm -rf /var/lib/apt/lists/* \\\n && apt-get clean\n\n# Install OpenSSH for MPI to communicate between containers, allow OpenSSH to talk to containers without asking for confirmation\nRUN apt-get update \\\n && apt-get install -y  --allow-downgrades --allow-change-held-packages --no-install-recommends \\\n && apt-get install -y --no-install-recommends openssh-client openssh-server \\\n && mkdir -p /var/run/sshd \\\n && cat /etc/ssh/ssh_config | grep -v StrictHostKeyChecking > /etc/ssh/ssh_config.new \\\n && echo \"    StrictHostKeyChecking no\" >> /etc/ssh/ssh_config.new \\\n && mv /etc/ssh/ssh_config.new /etc/ssh/ssh_config \\\n && rm -rf /var/lib/apt/lists/* \\\n && apt-get clean\n\n# Configure OpenSSH so that nodes can communicate with each other\nRUN mkdir -p /var/run/sshd && \\\n sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd\n\nRUN rm -rf /root/.ssh/ && \\\n mkdir -p /root/.ssh/ && \\\n ssh-keygen -q -t rsa -N '' -f /root/.ssh/id_rsa && \\\n cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys \\\n && printf \"Host *\\n StrictHostKeyChecking no\\n\" >> /root/.ssh/config\n\n# Install Horovod\nRUN pip uninstall -y horovod \\\n && ldconfig /usr/local/cuda-11.6/targets/x86_64-linux/lib/stubs \\\n && HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_CUDA_HOME=/usr/local/cuda-11.6 HOROVOD_WITH_PYTORCH=1 pip install --no-cache-dir horovod==${HOROVOD_VERSION} \\\n && ldconfig\n\n# Install PT S3 plugin\nRUN pip install --no-cache-dir -U ${PT_S3_WHL_GPU}\nRUN mkdir -p /etc/pki/tls/certs && cp /etc/ssl/certs/ca-certificates.crt /etc/pki/tls/certs/ca-bundle.crt\n\nWORKDIR /\n\nRUN HOME_DIR=/root \\\n && curl -f -o ${HOME_DIR}/oss_compliance.zip https://aws-dlinfra-utilities.s3.amazonaws.com/oss_compliance.zip \\\n && unzip ${HOME_DIR}/oss_compliance.zip -d ${HOME_DIR}/ \\\n && cp ${HOME_DIR}/oss_compliance/test/testOSSCompliance /usr/local/bin/testOSSCompliance \\\n && chmod +x /usr/local/bin/testOSSCompliance \\\n && chmod +x ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh \\\n && ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh ${HOME_DIR} ${PYTHON} \\\n && rm -rf ${HOME_DIR}/oss_compliance* \\\n && rm -rf /tmp/tmp*\n\n# Starts framework\nCMD [\"/bin/bash\"]\n\n#################################################################\n#  ____                   __  __       _\n# / ___|  __ _  __ _  ___|  \\/  | __ _| | _____ _ __\n# \\___ \\ / _` |/ _` |/ _ \\ |\\/| |/ _` | |/ / _ \\ '__|\n#  ___) | (_| | (_| |  __/ |  | | (_| |   <  __/ |\n# |____/ \\__,_|\\__, |\\___|_|  |_|\\__,_|_|\\_\\___|_|\n#              |___/\n#  ___                              ____           _\n# |_ _|_ __ ___   __ _  __ _  ___  |  _ \\ ___  ___(_)_ __   ___\n#  | || '_ ` _ \\ / _` |/ _` |/ _ \\ | |_) / _ \\/ __| | '_ \\ / _ \\\n#  | || | | | | | (_| | (_| |  __/ |  _ <  __/ (__| | |_) |  __/\n# |___|_| |_| |_|\\__,_|\\__, |\\___| |_| \\_\\___|\\___|_| .__/ \\___|\n#                      |___/                        |_|\n#################################################################\n\nFROM common AS sagemaker\n\nLABEL maintainer=\"Amazon AI\"\nLABEL dlc_major_version=\"1\"\n\nARG PYTHON\nARG PYTHON_VERSION\nARG PYTHON_SHORT_VERSION\n\nARG METIS=metis-5.1.0\nARG RMM_VERSION=0.15.0\n\n# The smdebug pipeline relies for following format to perform string replace and trigger DLC pipeline for validating\n# the nightly builds. Therefore, while updating the smdebug version, please ensure that the format is not disturbed.\nARG SMDEBUG_VERSION=1.0.16\n\nENV SAGEMAKER_TRAINING_MODULE=sagemaker_pytorch_container.training:main\n\n# SMD model parallel and data parallel binaries\nARG SMD_DATA_PARALLEL_URL\nARG SMD_MODEL_PARALLEL_URL\n\n# PyTorch Binaries\nARG PT_SM_TRAINING_URL\nARG PT_TORCHVISION_URL\nARG PT_TORCHAUDIO_URL\nARG PT_TORCHDATA_URL\nARG PT_S3_WHL_GPU\n\n# Install AWS-PyTorch and other torch packages\nRUN pip uninstall -y torch torchvision torchaudio torchdata \\\n && pip install --no-cache-dir -U ${PT_SM_TRAINING_URL} ${PT_TORCHVISION_URL} ${PT_TORCHAUDIO_URL} ${PT_TORCHDATA_URL} torchnet\n\n# Install Nvidia Apex\n## Pin apex commit requested by sm-model-parallel team\nRUN git clone https://github.com/NVIDIA/apex && \\\n    cd apex && \\\n    git checkout aa756ce && \\\n    pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n\n# Configure Open MPI and configure NCCL parameters\nRUN mv $OPEN_MPI_PATH/bin/mpirun $OPEN_MPI_PATH/bin/mpirun.real \\\n && echo '#!/bin/bash' > $OPEN_MPI_PATH/bin/mpirun \\\n && echo \"${OPEN_MPI_PATH}/bin/mpirun.real --allow-run-as-root \\\"\\$@\\\"\" >> $OPEN_MPI_PATH/bin/mpirun \\\n && chmod a+x $OPEN_MPI_PATH/bin/mpirun \\\n && echo \"hwloc_base_binding_policy = none\" >> $OPEN_MPI_PATH/etc/openmpi-mca-params.conf \\\n && echo \"rmaps_base_mapping_policy = slot\" >> $OPEN_MPI_PATH/etc/openmpi-mca-params.conf \\\n && echo NCCL_DEBUG=INFO >> /etc/nccl.conf \\\n && echo NCCL_SOCKET_IFNAME=^docker0 >> /etc/nccl.conf\n\n# Install AWS OFI NCCL plug-in\nRUN apt-get update && apt-get install --no-install-recommends -y autoconf && rm -rf /var/lib/apt/lists/*;\nRUN mkdir /tmp/efa-ofi-nccl \\\n && cd /tmp/efa-ofi-nccl \\\n && git clone https://github.com/aws/aws-ofi-nccl.git -b v${BRANCH_OFI} \\\n && cd aws-ofi-nccl \\\n && ./autogen.sh \\\n && ./configure --build=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" --with-libfabric=/opt/amazon/efa \\\n  --with-mpi=/opt/amazon/openmpi \\\n  --with-cuda=/usr/local/cuda \\\n  --with-nccl=/usr/local --prefix=/usr/local \\\n && make \\\n && make install \\\n && rm -rf /tmp/efa-ofi-nccl \\\n && rm -rf /var/lib/apt/lists/* \\\n && apt-get clean\n\n# Install OpenSSH for MPI to communicate between containers, allow OpenSSH to talk to containers without asking for confirmation\nRUN apt-get update \\\n && apt-get install -y  --allow-downgrades --allow-change-held-packages --no-install-recommends \\\n && apt-get install -y --no-install-recommends openssh-client openssh-server \\\n && mkdir -p /var/run/sshd \\\n && cat /etc/ssh/ssh_config | grep -v StrictHostKeyChecking > /etc/ssh/ssh_config.new \\\n && echo \"    StrictHostKeyChecking no\" >> /etc/ssh/ssh_config.new \\\n && mv /etc/ssh/ssh_config.new /etc/ssh/ssh_config \\\n && rm -rf /var/lib/apt/lists/* \\\n && apt-get clean\n\n# Configure OpenSSH so that nodes can communicate with each other\nRUN mkdir -p /var/run/sshd && \\\n sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd\n\nRUN rm -rf /root/.ssh/ && \\\n mkdir -p /root/.ssh/ && \\\n ssh-keygen -q -t rsa -N '' -f /root/.ssh/id_rsa && \\\n cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys \\\n && printf \"Host *\\n StrictHostKeyChecking no\\n\" >> /root/.ssh/config\n\n# Install Horovod\nRUN pip uninstall -y horovod \\\n && ldconfig /usr/local/cuda-11.6/targets/x86_64-linux/lib/stubs \\\n && HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_CUDA_HOME=/usr/local/cuda-11.6 HOROVOD_WITH_PYTORCH=1 pip install --no-cache-dir horovod==${HOROVOD_VERSION} \\\n && ldconfig\n\n# Install PT S3 plugin\nRUN pip install --no-cache-dir -U ${PT_S3_WHL_GPU}\nRUN mkdir -p /etc/pki/tls/certs && cp /etc/ssl/certs/ca-certificates.crt /etc/pki/tls/certs/ca-bundle.crt\n\nWORKDIR /\n\n# Install scikit-learn and pandas\nRUN conda install -y -c conda-forge \\\n    scikit-learn \\\n    pandas\n\nWORKDIR /\n\n# Install libboost from source. This package is needed for smdataparallel functionality [for networking asynchronous IO].\nRUN wget https://sourceforge.net/projects/boost/files/boost/1.73.0/boost_1_73_0.tar.gz/download -O boost_1_73_0.tar.gz \\\n && tar -xzf boost_1_73_0.tar.gz \\\n && cd boost_1_73_0 \\\n && ./bootstrap.sh \\\n && ./b2 threading=multi --prefix=/opt/conda -j 64 cxxflags=-fPIC cflags=-fPIC install || true \\\n && cd .. \\\n && rm -rf boost_1_73_0.tar.gz \\\n && rm -rf boost_1_73_0 \\\n && cd /opt/conda/include/boost\n\nWORKDIR /opt/pytorch\n\n# Copy workaround script for incorrect hostname\nCOPY changehostname.c /\nCOPY start_with_right_hostname.sh /usr/local/bin/start_with_right_hostname.sh\n\nRUN chmod +x /usr/local/bin/start_with_right_hostname.sh\n\nWORKDIR /root\n\nRUN pip install --no-cache-dir -U \\\n    # disable smdebug pip install until available stable smdebug releases\n    # smdebug==${SMDEBUG_VERSION} \\\n    smclarify \\\n    \"sagemaker>=2,<3\" \\\n    sagemaker-experiments==0.* \\\n    sagemaker-pytorch-training\n\n# Install smdebug from souce\nRUN cd /tmp \\\n  && git clone -b ${SMDEBUG_VERSION} https://github.com/awslabs/sagemaker-debugger \\\n  && cd sagemaker-debugger \\\n  && python setup.py install \\\n  && rm -rf /tmp/*\n\n# Install extra packages\n# numba 0.54 only works with numpy>=1.20. See https://github.com/numba/numba/issues/7339\nRUN pip install --no-cache-dir -U \\\n    \"bokeh>=2.3,<3\" \\\n    \"imageio>=2.9,<3\" \\\n    \"opencv-python>=4.3,<5\" \\\n    \"plotly>=5.1,<6\" \\\n    \"seaborn>=0.11,<1\" \\\n    \"numba<0.54\" \\\n    \"shap>=0.39,<1\"\n\n# install metis\nRUN rm /etc/apt/sources.list.d/* \\\n && wget -nv https://glaros.dtc.umn.edu/gkhome/fetch/sw/metis/${METIS}.tar.gz \\\n && gunzip -f ${METIS}.tar.gz \\\n && tar -xvf ${METIS}.tar \\\n && cd ${METIS} \\\n && apt-get update \\\n && make config shared=1 \\\n && make install \\\n && cd .. \\\n && rm -rf ${METIS}.tar* \\\n && rm -rf ${METIS} \\\n && rm -rf /var/lib/apt/lists/* \\\n && apt-get clean\n\n# Install RAPIDSMemoryManager.\n# Requires cmake>=3.14.\nRUN  wget -nv https://github.com/rapidsai/rmm/archive/v${RMM_VERSION}.tar.gz \\\n && tar -xvf v${RMM_VERSION}.tar.gz \\\n && cd rmm-${RMM_VERSION} \\\n && INSTALL_PREFIX=/usr/local ./build.sh librmm \\\n && cd .. \\\n && rm -rf v${RMM_VERSION}.tar* \\\n && rm -rf rmm-${RMM_VERSION}\n\n# Install SM Distributed Modelparallel binary\nRUN pip install --no-cache-dir -U ${SMD_MODEL_PARALLEL_URL}\n\n# Install SM Distributed DataParallel binary\nRUN SMDATAPARALLEL_PT=1 pip install --no-cache-dir ${SMD_DATA_PARALLEL_URL}\n\nENV LD_LIBRARY_PATH=\"/opt/conda/lib/python${PYTHON_SHORT_VERSION}/site-packages/smdistributed/dataparallel/lib:$LD_LIBRARY_PATH\"\n\nWORKDIR /\n\nRUN HOME_DIR=/root \\\n && curl -f -o ${HOME_DIR}/oss_compliance.zip https://aws-dlinfra-utilities.s3.amazonaws.com/oss_compliance.zip \\\n && unzip ${HOME_DIR}/oss_compliance.zip -d ${HOME_DIR}/ \\\n && cp ${HOME_DIR}/oss_compliance/test/testOSSCompliance /usr/local/bin/testOSSCompliance \\\n && chmod +x /usr/local/bin/testOSSCompliance \\\n && chmod +x ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh \\\n && ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh ${HOME_DIR} ${PYTHON} \\\n && rm -rf ${HOME_DIR}/oss_compliance* \\\n && rm -rf /tmp/tmp*\n\nENTRYPOINT [\"bash\", \"-m\", \"start_with_right_hostname.sh\"]\nCMD [\"/bin/bash\"]\n"
}
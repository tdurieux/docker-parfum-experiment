{
  "startTime": 1674254972097,
  "endTime": 1674254972805,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 52,
        "lineEnd": 52,
        "columnStart": 4,
        "columnEnd": 150
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 113,
        "lineEnd": 113,
        "columnStart": 4,
        "columnEnd": 91
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 127,
        "lineEnd": 127,
        "columnStart": 4,
        "columnEnd": 108
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 68,
        "lineEnd": 68,
        "columnStart": 4,
        "columnEnd": 91
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 70,
        "lineEnd": 72,
        "columnStart": 4,
        "columnEnd": 22
      }
    },
    {
      "rule": "configureShouldUseBuildFlag",
      "position": {
        "lineStart": 77,
        "lineEnd": 77,
        "columnStart": 4,
        "columnEnd": 39
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM ubuntu:20.04\n\nLABEL maintainer=\"Amazon AI\"\nLABEL dlc_major_version=\"1\"\n\n# Specify accept-bind-to-port LABEL for inference pipelines to use SAGEMAKER_BIND_TO_PORT\n# https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipeline-real-time.html\nLABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n# Specify multi-models LABEL to indicate container is capable of loading and serving multiple models concurrently\n# https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html\nLABEL com.amazonaws.sagemaker.capabilities.multi-models=true\n\nARG MMS_VERSION=1.1.8\nARG PYTHON=python3\nARG PYTHON_VERSION=3.8.10\nARG OPEN_MPI_VERSION=4.0.1\nARG MAMBA_VERSION=4.12.0-0\n# HF ARGS\nARG PT_INFERENCE_URL=https://pytorch-ei-binaries.s3.us-west-2.amazonaws.com/r1.10.2_inference/20220211-013556/528b6ca3747ec951396383f2a70d1354283ad29f/cpu/torch-1.10.2%2Bcpu-cp38-cp38-manylinux1_x86_64.whl\nARG PT_TORCHAUDIO_URL=https://framework-binaries.s3.us-west-2.amazonaws.com/pytorch/1.10.2/torchaudio-0.10.2%2Bcpu-cp38-cp38-linux_x86_64.whl\nARG TRANSFORMERS_VERSION\n\nENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1 \\\n    LD_LIBRARY_PATH=\"/opt/conda/lib/:${LD_LIBRARY_PATH}:/usr/local/lib\" \\\n    PYTHONIOENCODING=UTF-8 \\\n    LANG=C.UTF-8 \\\n    LC_ALL=C.UTF-8 \\\n    TEMP=/home/model-server/tmp \\\n    DEBIAN_FRONTEND=noninteractive\n\nENV PATH /opt/conda/bin:$PATH\n\nRUN apt-get update \\\n # TODO: Remove upgrade statements once packages are updated in base image\n && apt-get -y upgrade --only-upgrade systemd openssl cryptsetup \\\n && apt-get install -y --no-install-recommends \\\n    ca-certificates \\\n    build-essential \\\n    openssl \\\n    openjdk-8-jdk-headless \\\n    vim \\\n    wget \\\n    curl \\\n    emacs \\\n    unzip \\\n    git \\\n    libsndfile1-dev \\\n    ffmpeg \\\n && apt-get clean \\\n && rm -rf /var/lib/apt/lists/*\n\nRUN curl -f -L -o ~/mambaforge.sh https://github.com/conda-forge/miniforge/releases/download/${MAMBA_VERSION}/Mambaforge-${MAMBA_VERSION}-Linux-x86_64.sh \\\n && chmod +x ~/mambaforge.sh \\\n && ~/mambaforge.sh -b -p /opt/conda \\\n && rm ~/mambaforge.sh \\\n && /opt/conda/bin/conda update conda \\\n && /opt/conda/bin/conda install -c conda-forge \\\n    python=$PYTHON_VERSION \\\n    cython \\\n    \"mkl-include==2021.4.0\" \\\n    \"mkl==2021.4.0\" \\\n    botocore \\\n    # Below 2 are included in miniconda base, but not mamba so need to install\n    conda-content-trust \\\n    charset-normalizer \\\n && /opt/conda/bin/conda clean -ya\n\nRUN pip install --no-cache-dir --upgrade pip --trusted-host pypi.org --trusted-host \\\n && ln -s /opt/conda/bin/pip /usr/local/bin/pip3 \\\n && pip install --no-cache-dir packaging==20.4 \\\n    enum-compat==0.0.3 \\\n    \"cryptography>3.2\"\n\nRUN wget https://www.open-mpi.org/software/ompi/v4.0/downloads/openmpi-$OPEN_MPI_VERSION.tar.gz \\\n && gunzip -c openmpi-$OPEN_MPI_VERSION.tar.gz | tar xf - \\\n && cd openmpi-$OPEN_MPI_VERSION \\\n && ./configure --build=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" --prefix=/home/.openmpi \\\n && make all install \\\n && cd .. \\\n && rm openmpi-$OPEN_MPI_VERSION.tar.gz \\\n && rm -rf openmpi-$OPEN_MPI_VERSION\n\n# The ENV variables declared below are changed in the previous section\n# Grouping these ENV variables in the first section causes\n# ompi_info to fail. This is only observed in CPU containers\nENV PATH=\"$PATH:/home/.openmpi/bin\"\nENV LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/home/.openmpi/lib/\"\nRUN ompi_info --parsable --all | grep mpi_built_with_cuda_support:value\n\nWORKDIR /\n\nRUN pip install --no-cache-dir \\\n    multi-model-server==$MMS_VERSION \\\n    sagemaker-inference\n\nRUN useradd -m model-server \\\n && mkdir -p /home/model-server/tmp \\\n && chown -R model-server /home/model-server\n\nCOPY mms-entrypoint.py /usr/local/bin/dockerd-entrypoint.py\nCOPY config.properties /etc/sagemaker-mms.properties\n\nRUN chmod +x /usr/local/bin/dockerd-entrypoint.py\n\nADD https://raw.githubusercontent.com/aws/deep-learning-containers/master/src/deep_learning_container.py /usr/local/bin/deep_learning_container.py\n\nRUN chmod +x /usr/local/bin/deep_learning_container.py\n\n#################################\n# Hugging Face specific section #\n#################################\n\nRUN curl -f https://aws-dlc-licenses.s3.amazonaws.com/pytorch-1.10/license.txt -o /license.txt\n\n# Uninstall and re-install torch and torchvision from the PyTorch website\nRUN pip uninstall -y torch \\\n && pip install --no-cache-dir -U $PT_INFERENCE_URL $PT_TORCHAUDIO_URL\n\n# install Hugging Face libraries and its dependencies\nRUN pip install --no-cache-dir \\\n\ttransformers[sentencepiece,audio,vision]==${TRANSFORMERS_VERSION} \\\n\tprotobuf==3.12.0 \\\n\tnumpy>=1.21.5 \\\n\t\"sagemaker-huggingface-inference-toolkit<3\"\n\nRUN HOME_DIR=/root \\\n && curl -f -o ${HOME_DIR}/oss_compliance.zip https://aws-dlinfra-utilities.s3.amazonaws.com/oss_compliance.zip \\\n && unzip ${HOME_DIR}/oss_compliance.zip -d ${HOME_DIR}/ \\\n && cp ${HOME_DIR}/oss_compliance/test/testOSSCompliance /usr/local/bin/testOSSCompliance \\\n && chmod +x /usr/local/bin/testOSSCompliance \\\n && chmod +x ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh \\\n && ${HOME_DIR}/oss_compliance/generate_oss_compliance.sh ${HOME_DIR} ${PYTHON} \\\n && rm -rf ${HOME_DIR}/oss_compliance*\n\nEXPOSE 8080 8081\nENTRYPOINT [\"python\", \"/usr/local/bin/dockerd-entrypoint.py\"]\nCMD [\"serve\"]\n"
}
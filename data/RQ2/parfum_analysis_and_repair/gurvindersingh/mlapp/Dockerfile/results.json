{
  "startTime": 1674255865145,
  "endTime": 1674255865939,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 8,
        "lineEnd": 8,
        "columnStart": 4,
        "columnEnd": 182
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 7,
        "lineEnd": 7,
        "columnStart": 4,
        "columnEnd": 47
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 9,
        "lineEnd": 9,
        "columnStart": 4,
        "columnEnd": 98
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 5,
        "lineEnd": 5,
        "columnStart": 18,
        "columnEnd": 57
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 5,
        "lineEnd": 5,
        "columnStart": 18,
        "columnEnd": 57
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM nvidia/cuda:9.2-cudnn7-runtime-ubuntu18.04\n\nWORKDIR /app\nCOPY requirements.txt .\n\nRUN apt update && apt install --no-install-recommends -y python3 python3-pip curl && rm -rf /var/lib/apt/lists/*;\n\nRUN pip3 install --no-cache-dir --no-cache -r requirements.txt && \\\n    curl -f -o /tmp/torch_nightly-1.0.0.dev20181116-cp36-cp36m-linux_x86_64.whl https://download.pytorch.org/whl/nightly/cu92/torch_nightly-1.0.0.dev20181116-cp36-cp36m-linux_x86_64.whl && \\\n    pip3 install --no-cache-dir torch_nightly -f /tmp/torch_nightly-1.0.0.dev20181116-cp36-cp36m-linux_x86_64.whl && \\\n    apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/torch_nightly-1.0.0.dev20181116-cp36-cp36m-linux_x86_64.whl\n\nCOPY *.py config.json /app/\nCOPY --chown=65534 models models\nRUN mkdir -m 777 /nonexistent /app/feedback\nUSER nobody\nENTRYPOINT [ \"gunicorn\", \"app:app\", \"--access-logfile\", \"-\", \"--error-logfile\", \"-\" ]\nCMD [ \"-k\", \"gthread\",  \"-t\", \"60\", \"--workers\", \"1\", \"--threads\", \"2\", \"-b\", \"0.0.0.0\" ]"
}
diff --git a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/modelbox-ai/modelbox/docker/Dockerfile.cuda.develop.openeuler b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/modelbox-ai/modelbox/docker/Dockerfile.cuda.develop.openeuler/repaired.Dockerfile
index 8b80aac..a77c51a 100644
--- a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/modelbox-ai/modelbox/docker/Dockerfile.cuda.develop.openeuler
+++ b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/modelbox-ai/modelbox/docker/Dockerfile.cuda.develop.openeuler/repaired.Dockerfile
@@ -36,19 +36,19 @@ RUN mkdir -p /root/.pip && \
     NVIDIA_GPGKEY_SUM=d1be581509378368edeec8c1eb2958702feedf3bc3d17011adbf24efacce4ab5 && \
     curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/7fa2af80.pub | sed '/^Version/d' > /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA && \
     echo "$NVIDIA_GPGKEY_SUM  /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA" | sha256sum -c --strict - && \
-    if [ "${CUDA_VERSION}" = "10.2" ]; then \
+    if [ "${CUDA_VERSION}" = "10.2" ];then \
         dnf install -y --nogpgcheck --setopt=obsoletes=0 \
             libcublas10-10.2.2.89-1 \
             libcublas-devel-10.2.2.89-1; \
     elif [ "${CUDA_VERSION}" = "11.2" ]; then \
-        curl -LO https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.6.0.tar.gz && \
+        curl -f -LO https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.6.0.tar.gz && \
         tar zxf libtensorflow-gpu-linux-x86_64-2.6.0.tar.gz -C /usr/local/ && \
         python3 -m pip install --no-cache-dir tensorflow-gpu==2.6.0 && \
         dnf install -y --nogpgcheck --setopt=obsoletes=0 \
             libcudnn8-8.1.1.33-1.cuda11.2 \
             libcudnn8-devel-8.1.1.33-1.cuda11.2 \
             libcublas-11-2-11.4.1.1043-1 \
-            libcublas-devel-11-2-11.4.1.1043-1;fi && \
+            libcublas-devel-11-2-11.4.1.1043-1; rm libtensorflow-gpu-linux-x86_64-2.6.0.tar.gz fi && \
     dnf clean all && rm -rf /var/cache/dnf/* /root/*
 
 RUN dnf install -y --nogpgcheck --setopt=obsoletes=0 \
@@ -57,7 +57,7 @@ RUN dnf install -y --nogpgcheck --setopt=obsoletes=0 \
         cuda-libraries-$([ "${CUDA_VERSION}" = "11.2" ] && echo "devel" || echo "dev")-${CUDA_VER} \
         cuda-command-line-tools-${CUDA_VER} && \
     ln -s cuda-${CUDA_VERSION} /usr/local/cuda && \
-    curl https://nodejs.org/dist/v16.13.2/node-v16.13.2-linux-x64.tar.xz|tar -xJ && \
+    curl -f https://nodejs.org/dist/v16.13.2/node-v16.13.2-linux-x64.tar.xz | tar -xJ && \
     cp -af node-v16.13.2-linux-x64/{bin,include,lib,share} /usr/local/ && \
     npm install -g npm@latest && npm -v && node -v && \
     npm install -g @angular/cli && \
@@ -66,7 +66,7 @@ RUN dnf install -y --nogpgcheck --setopt=obsoletes=0 \
         dnf install -y --nogpgcheck --setopt=obsoletes=0 \
             libcudnn8-8.0.0.180-1.cuda10.2 \
             libcudnn8-devel-8.0.0.180-1.cuda10.2 && \
-        curl -LO https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.9.1%2Bcu102.zip && \
+        curl -f -LO https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.9.1%2Bcu102.zip && \
         unzip libtorch-*.zip -d /root >/dev/null 2>&1 && \
         cp -af libtorch/{include,lib,share} /usr/local/; \
     elif [ -n "${TRT_VERSION}" ]; then \
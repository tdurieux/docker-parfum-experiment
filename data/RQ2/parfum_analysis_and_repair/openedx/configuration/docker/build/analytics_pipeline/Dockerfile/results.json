{
  "startTime": 1674251412162,
  "endTime": 1674251413629,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 133,
        "lineEnd": 133,
        "columnStart": 4,
        "columnEnd": 104
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 103,
        "lineEnd": 103,
        "columnStart": 7,
        "columnEnd": 72
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 50,
        "lineEnd": 50,
        "columnStart": 25,
        "columnEnd": 70
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "ARG BASE_IMAGE_TAG=latest\nFROM edxops/xenial-common:${BASE_IMAGE_TAG}\nLABEL maintainer=\"edxops\"\n\nUSER root\nENV BOTO_CONFIG=/dev/null \\\n    JDK_URL=http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz \\\n    JDK_DIST_FILE=jdk-8u131-linux-x64.tar.gz \\\n    JAVA_HOME=/usr/lib/jvm/java-8-oracle \\\n    HADOOP_URL=https://archive.apache.org/dist/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz \\\n    HADOOP_DIST_FILE=hadoop-2.7.2.tar.gz \\\n    HADOOP_HOME=/edx/app/hadoop/hadoop \\\n    HADOOP_PREFIX=/edx/app/hadoop/hadoop \\\n    HIVE_URL=https://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz \\\n    HIVE_DIST_FILE=apache-hive-2.1.1-bin.tar.gz \\\n    HIVE_HOME=/edx/app/hadoop/hive \\\n    SQOOP_URL=http://archive.apache.org/dist/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz \\\n    SQOOP_DIST_FILE=sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz \\\n    SQOOP_MYSQL_CONNECTOR_URL=http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.29.tar.gz \\\n    SQOOP_MYSQL_CONNECTOR_FILE=mysql-connector-java-5.1.29 \\\n    SQOOP_HOME=/edx/app/hadoop/sqoop \\\n    SQOOP_LIB=/edx/app/hadoop/sqoop/lib \\\n    SQOOP_VERTICA_CONNECTOR_URL=https://vertica.com/client_drivers/9.1.x/9.1.1-0/vertica-jdbc-9.1.1-0.jar \\\n    SQOOP_VERTICA_CONNECTOR_FILE=vertica-jdbc-9.1.1-0.jar \\\n    SPARK_URL=https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz \\\n    SPARK_DIST_FILE=spark-2.1.0-bin-hadoop2.7.tgz \\\n    SPARK_HOME=/edx/app/hadoop/spark \\\n    LUIGI_CONFIG_PATH=/edx/app/analytics_pipeline/analytics_pipeline/config/luigi_docker.cfg \\\n    ANALYTICS_PIPELINE_VENV=/edx/app/analytics_pipeline/venvs \\\n    BOOTSTRAP=/etc/bootstrap.sh \\\n    COMMON_BASE_DIR=/edx \\\n    COMMON_PIP_PACKAGES_PIP='pip==21.2.1' \\\n    COMMON_PIP_PACKAGES_SETUPTOOLS='setuptools==44.1.0' \\\n    COMMON_PIP_PACKAGES_VIRTUALENV='virtualenv==20.1.0' \\\n    COMMON_MYSQL_READ_ONLY_USER='read_only' \\\n    COMMON_MYSQL_READ_ONLY_PASS='password' \\\n    ANALYTICS_PIPELINE_OUTPUT_DATABASE_USER='pipeline001' \\\n    ANALYTICS_PIPELINE_OUTPUT_DATABASE_PASSWORD='password' \\\n    EDX_PPA_KEY_SERVER='keyserver.ubuntu.com' \\\n    EDX_PPA_KEY_ID='69464050'\n\n\nENV PATH=\"/edx/app/analytics_pipeline/venvs/analytics_pipeline/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${HIVE_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${SQOOP_HOME}/bin:$PATH\" \\\n    COMMON_DATA_DIR=$COMMON_BASE_DIR/var \\\n    COMMON_APP_DIR=$COMMON_BASE_DIR/app \\\n    COMMON_LOG_DIR=$COMMON_BASE_DIR/var/log \\\n    COMMON_BIN_DIR=$COMMON_BASE_DIR/bin \\\n    COMMON_CFG_DIR=$COMMON_BASE_DIR/etc\n\n# add custom PPAs & install packages\nRUN apt-get update -y && apt-get install --no-install-recommends -y software-properties-common \\\n    && apt-key adv --keyserver $EDX_PPA_KEY_SERVER --recv-keys $EDX_PPA_KEY_ID \\\n    && add-apt-repository -y 'deb http://ppa.edx.org xenial main' \\\n    && apt-get update -y \\\n    && apt-get install --no-install-recommends -y \\\n           python2.7 python2.7-dev python-pip python-apt python-yaml python-jinja2 libmysqlclient-dev libffi-dev libssl-dev \\\n           libatlas-base-dev libblas-dev liblapack-dev libpq-dev sudo make build-essential git-core \\\n           openssh-server openssh-client rsync software-properties-common vim net-tools curl netcat mysql-client-5.6 \\\n           apt-transport-https ntp acl lynx-cur logrotate rsyslog unzip \\\n           ack-grep mosh tree screen tmux dnsutils inetutils-telnet \\\n    && rm -rf /var/lib/apt/lists/*\n\n# creating directory structure\nRUN mkdir -p $HADOOP_HOME $JAVA_HOME $ANALYTICS_PIPELINE_VENV /edx/app/hadoop/lib $HIVE_HOME /etc/luigi \\\n    $SPARK_HOME $SQOOP_HOME $COMMON_DATA_DIR $COMMON_APP_DIR $COMMON_LOG_DIR $COMMON_BIN_DIR $COMMON_CFG_DIR/edx-analytics-pipeline\n\n# create user & group for hadoop\nRUN groupadd hadoop\nRUN useradd -ms /bin/bash hadoop -g hadoop -d /edx/app/hadoop\nRUN echo '%hadoop ALL=(ALL) NOPASSWD:ALL' >> /etc/sudoers\n\n# JAVA\nRUN curl -fSL --header \"Cookie:oraclelicense=accept-securebackup-cookie\" \"$JDK_URL\" -o /var/tmp/$JDK_DIST_FILE \\\n    && tar -xzf /var/tmp/$JDK_DIST_FILE -C $JAVA_HOME --strip-components=1 \\\n    && rm -f /var/tmp/$JDK_DIST_FILE\n\n# HADOOP\nRUN curl -fSL \"$HADOOP_URL\" -o /var/tmp/$HADOOP_DIST_FILE \\\n    && tar -xzf /var/tmp/$HADOOP_DIST_FILE -C $HADOOP_HOME --strip-components=1 \\\n    && sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-8-oracle\\nexport HADOOP_PREFIX=/edx/app/hadoop/hadoop\\nexport HADOOP_HOME=/edx/app/hadoop/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n    && sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/edx/app/hadoop/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh \\\n    && sed -i 's#<configuration>#<configuration><property><name>fs.defaultFS</name><value>hdfs://namenode:8020</value></property>#' $HADOOP_HOME/etc/hadoop/core-site.xml \\\n    && sed 's#<configuration>#<configuration><property><name>mapreduce.framework.name</name><value>yarn</value></property>#' $HADOOP_HOME/etc/hadoop/mapred-site.xml.template > $HADOOP_HOME/etc/hadoop/mapred-site.xml \\\n    && sed -i 's#<configuration>#<configuration><property><name>yarn.resourcemanager.hostname</name><value>resourcemanager</value></property>#' $HADOOP_HOME/etc/hadoop/yarn-site.xml \\\n    && rm -f /var/tmp/$HADOOP_DIST_FILE\n\n# HIVE\nRUN curl -fSL \"$HIVE_URL\" -o /var/tmp/$HIVE_DIST_FILE \\\n    && tar -xzf /var/tmp/$HIVE_DIST_FILE -C $HIVE_HOME --strip-components=1 \\\n    && rm -f /var/tmp/$HIVE_DIST_FILE\nADD docker/build/analytics_pipeline/hive-site.xml.template $HIVE_HOME/conf/hive-site.xml\n\n# SPARK\nRUN curl -fSL \"$SPARK_URL\" -o /var/tmp/$SPARK_DIST_FILE \\\n    && tar -xzf /var/tmp/$SPARK_DIST_FILE -C $SPARK_HOME --strip-components=1 \\\n    && echo 'spark.master  spark://sparkmaster:7077\\nspark.eventLog.enabled  true\\nspark.eventLog.dir  hdfs://namenode:8020/tmp/spark-events\\nspark.history.fs.logDirectory  hdfs://namenode:8020/tmp/spark-events\\nspark.sql.warehouse.dir hdfs://namenode:8020/spark-warehouse' > $SPARK_HOME/conf/spark-defaults.conf \\\n    && rm -f /var/tmp/$SPARK_DIST_FILE\n\n# SQOOP\nRUN curl -fSL \"$SQOOP_URL\" -o /var/tmp/$SQOOP_DIST_FILE \\\n    && curl -fSL \"$SQOOP_MYSQL_CONNECTOR_URL\" -o /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE.tar.gz \\\n    && curl -fSL \"$SQOOP_VERTICA_CONNECTOR_URL\" -o /var/tmp/$SQOOP_VERTICA_CONNECTOR_FILE \\\n    && tar -xzf /var/tmp/$SQOOP_DIST_FILE -C $SQOOP_HOME --strip-components=1 \\\n    && tar -xzf /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE.tar.gz -C /var/tmp/ \\\n    && cp /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE/$SQOOP_MYSQL_CONNECTOR_FILE-bin.jar $SQOOP_LIB \\\n    && cp /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE/$SQOOP_MYSQL_CONNECTOR_FILE-bin.jar $HIVE_HOME/lib/ \\\n    && cp /var/tmp/$SQOOP_VERTICA_CONNECTOR_FILE $SQOOP_LIB \\\n    && rm -rf /var/tmp/$SQOOP_DIST_FILE /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE* /var/tmp/$SQOOP_VERTICA_CONNECTOR_FILE* && rm /var/tmp/$SQOOP_MYSQL_CONNECTOR_FILE.tar.gz\n\nWORKDIR /var/tmp\n# Edx Hadoop Util Library\nRUN git clone https://github.com/edx/edx-analytics-hadoop-util \\\n    && cd /var/tmp/edx-analytics-hadoop-util \\\n    && $JAVA_HOME/bin/javac -cp `/edx/app/hadoop/hadoop/bin/hadoop classpath` org/edx/hadoop/input/ManifestTextInputFormat.java \\\n    && $JAVA_HOME/bin/jar cf /edx/app/hadoop/lib/edx-analytics-hadoop-util.jar org/edx/hadoop/input/ManifestTextInputFormat.class\n\n# configure bootstrap scripts for container\nADD docker/build/analytics_pipeline/bootstrap.sh /etc/bootstrap.sh\nRUN chown hadoop:hadoop /etc/bootstrap.sh \\\n    && chmod 700 /etc/bootstrap.sh \\\n    && chown -R hadoop:hadoop /edx/app/hadoop\n\n# Analytics pipeline\nARG OPENEDX_RELEASE=master\nENV OPENEDX_RELEASE=${OPENEDX_RELEASE}\nRUN git clone https://github.com/edx/edx-analytics-pipeline \\\n    && cd edx-analytics-pipeline \\\n    && git checkout ${OPENEDX_RELEASE} \\\n    && cd .. \\\n    && cp /var/tmp/edx-analytics-pipeline/Makefile /var/tmp/Makefile \\\n    && cp -r /var/tmp/edx-analytics-pipeline/requirements /var/tmp/requirements \\\n    && rm -rf /var/tmp/edx-analytics-pipeline\n\nRUN pip install --no-cache-dir $COMMON_PIP_PACKAGES_PIP $COMMON_PIP_PACKAGES_SETUPTOOLS $COMMON_PIP_PACKAGES_VIRTUALENV \\\n    && virtualenv $ANALYTICS_PIPELINE_VENV/analytics_pipeline/ \\\n    && chown -R hadoop:hadoop $ANALYTICS_PIPELINE_VENV/analytics_pipeline/ \\\n    && echo '[hadoop]\\nversion: cdh4\\ncommand: /edx/app/hadoop/hadoop/bin/hadoop\\nstreaming-jar: /edx/app/hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar' > /etc/luigi/client.cfg\n\nRUN apt-get update && make system-requirements\nADD docker/build/analytics_pipeline/devstack.sh /edx/app/analytics_pipeline/devstack.sh\nRUN chown hadoop:hadoop /edx/app/analytics_pipeline/devstack.sh && chmod a+x /edx/app/analytics_pipeline/devstack.sh\nUSER hadoop\nRUN touch /edx/app/hadoop/.bashrc \\\n    && echo 'export JAVA_HOME=/usr/lib/jvm/java-8-oracle\\nexport HADOOP_HOME=/edx/app/hadoop/hadoop\\nexport HIVE_HOME=/edx/app/hadoop/hive\\nexport SQOOP_HOME=/edx/app/hadoop/sqoop\\nexport SPARK_HOME=/edx/app/hadoop/spark\\nexport PATH=\"/edx/app/analytics_pipeline/venvs/analytics_pipeline/bin:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${HIVE_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${SQOOP_HOME}/bin:$PATH\"' > /edx/app/hadoop/.bashrc \\\n    && . $ANALYTICS_PIPELINE_VENV/analytics_pipeline/bin/activate \\\n    && make test-requirements requirements\n\nRUN sudo chown hadoop:hadoop $COMMON_CFG_DIR/edx-analytics-pipeline/ \\\n    && echo \"{\\\"username\\\": \\\"$COMMON_MYSQL_READ_ONLY_USER\\\", \\\"host\\\": \\\"mysql\\\", \\\"password\\\": \\\"$COMMON_MYSQL_READ_ONLY_PASS\\\", \\\"port\\\": 3306}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/input.json \\\n    && echo \"{\\\"username\\\": \\\"$ANALYTICS_PIPELINE_OUTPUT_DATABASE_USER\\\", \\\"host\\\": \\\"mysql\\\", \\\"password\\\": \\\"$ANALYTICS_PIPELINE_OUTPUT_DATABASE_PASSWORD\\\", \\\"port\\\": 3306}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/output.json \\\n    && echo \"{\\\"username\\\": \\\"dbadmin\\\", \\\"host\\\": \\\"vertica\\\", \\\"password\\\": \\\"\\\", \\\"port\\\": 5433}\" > $COMMON_CFG_DIR/edx-analytics-pipeline/warehouse.json\n\nADD docker/build/analytics_pipeline/acceptance.json $COMMON_CFG_DIR/edx-analytics-pipeline/acceptance.json\nWORKDIR /edx/app/analytics_pipeline/analytics_pipeline\n\nCMD [\"/etc/bootstrap.sh\", \"-d\"]\n"
}
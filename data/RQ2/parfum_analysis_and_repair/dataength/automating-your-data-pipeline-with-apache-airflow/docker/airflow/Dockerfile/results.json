{
  "startTime": 1674250558650,
  "endTime": 1674250559827,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 42,
        "lineEnd": 42,
        "columnStart": 4,
        "columnEnd": 39
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 44,
        "lineEnd": 48,
        "columnStart": 4,
        "columnEnd": 48
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 49,
        "lineEnd": 49,
        "columnStart": 4,
        "columnEnd": 24
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 14,
        "lineEnd": 34,
        "columnStart": 4,
        "columnEnd": 10
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Base Image\nFROM --platform=linux/amd64 spark-base:latest\nLABEL maintainer=\"MarcLamberti\"\n\n# Arguments that can be set with docker build\nARG AIRFLOW_VERSION=2.1.2\nARG AIRFLOW_HOME=/usr/local/airflow\n\n# Export the environment variable AIRFLOW_HOME where airflow will be installed\nENV AIRFLOW_HOME=${AIRFLOW_HOME}\n\n# Install dependencies and tools\nRUN apt-get update -y && \\\n    apt-get upgrade -yqq && \\\n    apt-get install -yqq --no-install-recommends \\\n    python3-dev \\\n    wget \\\n    libczmq-dev \\\n    curl \\\n    libssl-dev \\\n    git \\\n    inetutils-telnet \\\n    bind9utils freetds-dev \\\n    libkrb5-dev \\\n    libsasl2-dev \\\n    libffi-dev libpq-dev \\\n    freetds-bin build-essential \\\n    default-libmysqlclient-dev \\\n    apt-utils \\\n    rsync \\\n    zip \\\n    unzip \\\n    gcc \\\n    vim \\\n    netcat \\\n    && apt-get autoremove -yqq --purge && apt-get clean && rm -rf /var/lib/apt/lists/*;\n\nCOPY ./requirements-python3.7.txt /requirements-python3.7.txt\n\n# Upgrade pip\n# Create airflow user\n# Install apache airflow with subpackages\nRUN pip install --no-cache-dir --upgrade \"pip==20.2.4\" && \\\n    useradd -ms /bin/bash -d ${AIRFLOW_HOME} airflow && \\\n    pip install --no-cache-dir apache-airflow[crypto,celery,postgres,apache.hive,jdbc,mysql,ssh,docker,apache.hdfs,redis,slack,apache.webhdfs,apache.spark,papermill,http]==${AIRFLOW_VERSION} \\\n        jupyter \\\n        great_expectations \\\n        airflow-provider-great-expectations \\\n        --constraint /requirements-python3.7.txt\nRUN pip install --no-cache-dir -U scipy\n\n# Copy the airflow.cfg file (config)\n#COPY ./config/airflow.cfg ${AIRFLOW_HOME}/airflow.cfg\n\n# Set the owner of the files in AIRFLOW_HOME to the user airflow\nRUN chown -R airflow: ${AIRFLOW_HOME}\n\n# Copy the entrypoint.sh from host to container (at path AIRFLOW_HOME)\nCOPY ./start-airflow.sh ./start-airflow.sh\n\n# Set the entrypoint.sh file to be executable\nRUN chmod +x ./start-airflow.sh\n\n# Set the username to use\nUSER airflow\n\n# Create the folder dags inside $AIRFLOW_HOME\nRUN mkdir -p ${AIRFLOW_HOME}/dags\n\n# Expose ports (just to indicate that this container needs to map port)\nEXPOSE 8080\n\n# Execute start-airflow.sh\nCMD [ \"./start-airflow.sh\" ]\n\n"
}
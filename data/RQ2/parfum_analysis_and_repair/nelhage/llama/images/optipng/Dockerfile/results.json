{
  "startTime": 1674248326066,
  "endTime": 1674248326825,
  "originalSmells": [
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 20,
        "lineEnd": 20,
        "columnStart": 8,
        "columnEnd": 50
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 20,
        "lineEnd": 20,
        "columnStart": 8,
        "columnEnd": 50
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# We're going to get the llama runtime from a pre-built\n# container. ghcr.io/nelhage/llama is built out of the `Dockerfile` in the\n# root of the llama repository.\n#\n# We could build our entire image on top of the llama image, but we\n# only need the `llama_runtime` binary, so we're going to copy that\n# binary out, which lets us build our image on an arbitrary base\n# image.\nFROM ghcr.io/nelhage/llama as llama\n\nFROM debian:stable\n\n# These lines make Llama work. We copy the Llama runtime, and we point\n# `ENTRYPOINT` at it.\nCOPY --from=llama /llama_runtime /llama_runtime\nENTRYPOINT [\"/llama_runtime\"]\n\n# Install the packages we need. It's important that we also grab\n# `ca-certificates` so the Llama can find a CA store to talk to S3.\nRUN apt-get update && \\\n        apt-get -y --no-install-recommends install optipng ca-certificates && \\\n        apt-get clean && rm -rf /var/lib/apt/lists/*;\n\n# If we specify a CMD, llama will respect it, in a very similar manner\n# to `docker run`; without the CMD, any command-line to `llama invoke`\n# will be executed directly, using the first argument as the command\n# to run.\n"
}
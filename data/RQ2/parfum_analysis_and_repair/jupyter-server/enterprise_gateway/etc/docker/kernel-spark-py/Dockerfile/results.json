{
  "startTime": 1674253991444,
  "endTime": 1674253992036,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 28,
        "lineEnd": 28,
        "columnStart": 4,
        "columnEnd": 105
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "ARG HUB_ORG\nARG TAG\n\n# Ubuntu 18.04.1 LTS Bionic\nARG BASE_CONTAINER=$HUB_ORG/kernel-py:$TAG\nFROM $BASE_CONTAINER\n\nARG SPARK_VERSION\n\nENV SPARK_VER $SPARK_VERSION\nENV SPARK_HOME /opt/spark\nENV KERNEL_LANGUAGE python\nENV R_LIBS_USER $R_LIBS_USER:${SPARK_HOME}/R/lib\nENV PATH $PATH:$SPARK_HOME/bin\n\nUSER root\n\nRUN apt-get update && \\\n    apt-get install -yq --no-install-recommends \\\n    openjdk-8-jdk \\\n    less \\\n    curl \\\n    libssl-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\nENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64\n\n# Download and install Spark\nRUN curl -f -s https://archive.apache.org/dist/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-hadoop2.7.tgz | \\\n    tar -xz -C /opt && \\\n    ln -s ${SPARK_HOME}-${SPARK_VER}-bin-hadoop2.7 $SPARK_HOME\n\n# Download entrypoint.sh from matching tag\n# Use tini from Anaconda installation\nRUN cd /opt/ && \\\n    wget https://raw.githubusercontent.com/apache/spark/v${SPARK_VER}/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh && \\\n    chmod a+x /opt/entrypoint.sh && \\\n    sed -i 's/tini -s/tini -g/g' /opt/entrypoint.sh && \\\n    ln -sfn /opt/conda/bin/tini /usr/bin/tini\n\nWORKDIR $SPARK_HOME/work-dir\n# Ensure that work-dir is writable by everyone\nRUN chmod 0777 $SPARK_HOME/work-dir\n\nENTRYPOINT [ \"/opt/entrypoint.sh\" ]\n\nUSER jovyan\n"
}
{
  "startTime": 1674251364969,
  "endTime": 1674251365394,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 16,
        "lineEnd": 16,
        "columnStart": 4,
        "columnEnd": 29
      }
    },
    {
      "rule": "mkdirUsrSrcThenRemove",
      "position": {
        "lineStart": 9,
        "lineEnd": 9,
        "columnStart": 4,
        "columnEnd": 25
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM sotera/pyspark-mongo-jupyter:5\n# from https://github.com/jupyter/docker-stacks/blob/master/pyspark-notebook/Dockerfile\n\nENV TERM=xterm\n\n# docker build-time args\nARG SERVICE\nARG MAIN=main.py\n\nRUN mkdir -p /usr/src/app && rm -rf /usr/src/app\nWORKDIR /usr/src/app\n\n# pyspark-mongo-jupyter uses anaconda for py2/3.\n# maybe we should use that instead of pip install\n# here, but will leave for now.\nCOPY $SERVICE/requirements.txt .\nRUN pip install --no-cache-dir --upgrade pip\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY $SERVICE .\n# create consistent top-level filename\nCOPY $SERVICE/$MAIN main.py\n# match project dir structure to satisfy imports\nCOPY util /usr/src/util\n\n# entrypoint is tini\nCMD [\"start.sh\", \"/usr/local/spark/bin/spark-submit\", \"--packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.0.0\", \"main.py\"]\n"
}
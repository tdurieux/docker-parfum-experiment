{
  "startTime": 1674255731536,
  "endTime": 1674255732788,
  "originalSmells": [
    {
      "rule": "npmCacheCleanAfterInstall",
      "position": {
        "lineStart": 15,
        "lineEnd": 15,
        "columnStart": 4,
        "columnEnd": 42
      }
    },
    {
      "rule": "aptGetUpdatePrecedesInstall",
      "position": {
        "lineStart": 38,
        "lineEnd": 39,
        "columnStart": 33,
        "columnEnd": 111
      }
    }
  ],
  "repairedSmells": [
    {
      "rule": "aptGetUpdatePrecedesInstall",
      "position": {
        "lineStart": 38,
        "lineEnd": 39,
        "columnStart": 33,
        "columnEnd": 111
      }
    }
  ],
  "repairedDockerfile": "FROM fluxproject/ros_base_gpu\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    locales bzip2 tree unzip xz-utils curl wget iproute2 sudo \\\n    python-pip python3-pip python-setuptools python3-setuptools \\\n    openjdk-8-jdk-headless nodejs npm nodejs-legacy \\\n    iputils-ping net-tools iproute knot-dnsutils vim \\\n    ffmpeg \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Jupyterhub setting\nRUN mkdir -p /etc/jupyterhub\nCOPY jupyterhub_config.py /etc/jupyterhub/\n\n# Introduce flux user\nRUN npm install -g configurable-http-proxy && npm cache clean --force;\nRUN useradd -u 11111 -m -s /bin/bash flux\nRUN usermod -aG sudo flux\nRUN bash -c \" echo flux:flux | chpasswd \"\n\n# Flux user setting # TODO: link\nCOPY spark-ex-kubernetes.sh /home/flux/\n\nRUN python2 -m pip install --upgrade --user pip && \\\n    python3 -m pip install --upgrade --user pip && \\\n    python3 -m pip install --no-cache-dir --upgrade jupyter jupyterhub jupyterlab && \\\n    python2 -m pip install --no-cache-dir --upgrade pyspark matplotlib pandas tensorflow-gpu keras Pillow && \\\n    python2 -m pip install --no-cache-dir --upgrade --force-reinstall requests imageio moviepy seaborn gmaps && \\\n    python2 -m pip install ipykernel && \\\n    python2 -m ipykernel install && \\\n    python3 -m pip install ipykernel && \\\n    python3 -m ipykernel install\n\nRUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 66F84AE1EB71A8AC108087DCAF677210FF6D3CDA && \\\n    bash -c 'echo \"deb [ arch=amd64 ] http://packages.dataspeedinc.com/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-dataspeed-public.list' && \\\n    apt-get update\n\nRUN bash -c 'echo \"yaml http://packages.dataspeedinc.com/ros/ros-public-'$ROS_DISTRO'.yaml '$ROS_DISTRO'\" > /etc/ros/rosdep/sources.list.d/30-dataspeed-public-'$ROS_DISTRO'.list' && \\\n    rosdep update 2>/dev/null && apt-get install -y --no-install-recommends \\\n      ros-$ROS_DISTRO-dbw-mkz ros-$ROS_DISTRO-mobility-base ros-$ROS_DISTRO-baxter-sdk ros-$ROS_DISTRO-velodyne && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Default to UTF-8\nRUN locale-gen en_US.UTF-8\nENV LANG en_US.UTF-8\nENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64\nENV PATH $PATH:/opt/apache/hadoop/bin\nENV ROSIF_JAR /opt/ros_hadoop/master/lib/rosbaginputformat.jar\n\nRUN mkdir -p /opt/ros_hadoop/latest\nRUN mkdir -p /opt/ros_hadoop/master/dist/\nRUN mkdir -p /opt/apache/\nRUN mkdir -p /opt/ros_spark/dist/\nCOPY . /opt/ros_hadoop/master/\nRUN bash -c \"curl -s https://api.github.com/repos/valtech/ros_hadoop/releases/latest | egrep -io 'https://api.github.com/repos/valtech/ros_hadoop/tarball/[^\\\"]*' | xargs wget --quiet -O /opt/ros_hadoop/latest.tgz\"\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/master/dist/hadoop-3.0.0.tar.gz ] ; then wget --no-check-certificate -O /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz -q https://www.eu.apache.org/dist/hadoop/common/hadoop-3.1.1/hadoop-3.1.1.tar.gz ; fi\"\nRUN tar -xzf /opt/ros_hadoop/latest.tgz -C /opt/ros_hadoop/latest --strip-components=1 && rm /opt/ros_hadoop/latest.tgz\nRUN tar -xzf /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz -C /opt/apache && rm /opt/ros_hadoop/master/dist/hadoop-3.1.1.tar.gz\nRUN ln -s /opt/apache/hadoop-3.1.1 /opt/apache/hadoop\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/latest/lib/rosbaginputformat.jar ] ; then ln -s /opt/ros_hadoop/master/lib/rosbaginputformat.jar /opt/ros_hadoop/latest/lib/rosbaginputformat.jar ; fi\"\n\n## for spark example tests\nRUN bash -c \"if [ ! -f /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz ] ; then wget --quiet -O /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz http://apache.lauf-forum.at/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz ; fi\"\nRUN tar -xzf /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz -C /opt/apache && rm /opt/ros_spark/dist/spark-2.3.1-bin-hadoop2.7.tgz\n\nRUN printf \"<configuration>\\n\\n<property>\\n<name>fs.defaultFS</name>\\n<value>hdfs://localhost:9000</value>\\n</property>\\n</configuration>\" > /opt/apache/hadoop/etc/hadoop/core-site.xml && \\\n    printf \"<configuration>\\n<property>\\n<name>dfs.replication</name>\\n<value>1</value>\\n</property>\\n</configuration>\" > /opt/apache/hadoop/etc/hadoop/hdfs-site.xml && \\\n    bash -c \"/opt/apache/hadoop/bin/hdfs namenode -format 2>/dev/null\" && \\\n    printf \"#! /bin/bash\\n/opt/apache/hadoop/bin/hdfs --daemon stop datanode\\n/opt/apache/hadoop/bin/hdfs --daemon stop namenode\\n/opt/apache/hadoop/bin/hdfs --daemon start namenode\\n/opt/apache/hadoop/bin/hdfs --daemon start datanode\\nexec \\\"\\$@\\\"\\n\" > /start_hadoop.sh && \\\n    chmod a+x /start_hadoop.sh\n\nRUN printf \"#! /bin/bash\\nset -e\\nsource \\\"/opt/ros/$ROS_DISTRO/setup.bash\\\"\\n/start_hadoop.sh\\nexec \\\"\\$@\\\"\\n\" > /ros_hadoop.sh && \\\n    chmod a+x /ros_hadoop.sh\n\nRUN bash -c \"if [ ! -f /opt/ros_hadoop/master/dist/HMB_4.bag ] ; then wget --quiet -O /opt/ros_hadoop/master/dist/HMB_4.bag https://xfiles.valtech.io/f/c494d168522045e3bcc0/?dl=1 ; fi\" && \\\n    java -jar \"$ROSIF_JAR\" -f /opt/ros_hadoop/master/dist/HMB_4.bag\n\nRUN bash -c \"/start_hadoop.sh\" && \\\n    /opt/apache/hadoop/bin/hdfs dfsadmin -safemode wait && \\\n    /opt/apache/hadoop/bin/hdfs dfsadmin -report && \\\n    /opt/apache/hadoop/bin/hdfs dfs -mkdir /user && \\\n    /opt/apache/hadoop/bin/hdfs dfs -mkdir /user/root && \\\n    /opt/apache/hadoop/bin/hdfs dfs -mkdir /user/flux && \\\n    /opt/apache/hadoop/bin/hdfs dfs -put /opt/ros_hadoop/master/dist/HMB_4.bag && \\\n    /opt/apache/hadoop/bin/hdfs --daemon stop datanode && \\\n    /opt/apache/hadoop/bin/hdfs --daemon stop namenode\n\nRUN \\\n  mkdir -p /ope/ros_hadoop/latest/doc && \\\n  chmod -R 777 /opt/ros_hadoop\n\nWORKDIR /opt/ros_hadoop/latest/doc/\nENTRYPOINT [\"/ros_hadoop.sh\"]\n\nCMD [\"jupyterhub\", \"-f\", \"/etc/jupyterhub/jupyterhub_config.py\"]\n"
}
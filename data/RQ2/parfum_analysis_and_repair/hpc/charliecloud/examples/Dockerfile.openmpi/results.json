{
  "startTime": 1674216575043,
  "endTime": 1674216575889,
  "originalSmells": [
    {
      "rule": "configureShouldUseBuildFlag",
      "position": {
        "lineStart": 110,
        "lineEnd": 110,
        "columnStart": 4,
        "columnEnd": 35
      }
    },
    {
      "rule": "configureShouldUseBuildFlag",
      "position": {
        "lineStart": 126,
        "lineEnd": 135,
        "columnStart": 4,
        "columnEnd": 58
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 108,
        "lineEnd": 108,
        "columnStart": 4,
        "columnEnd": 41
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 122,
        "lineEnd": 122,
        "columnStart": 4,
        "columnEnd": 40
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# ch-test-scope: full\nFROM almalinux8\n\n# A key goal of this Dockerfile is to demonstrate best practices for building\n# OpenMPI for use inside a container.\n#\n# This OpenMPI aspires to work close to optimally on clusters with any of the\n# following interconnects:\n#\n#    - Ethernet (TCP/IP)\n#    - InfiniBand (IB)\n#    - Omni-Path (OPA)\n#    - RDMA over Converged Ethernet (RoCE) interconnects\n#\n# with no environment variables, command line arguments, or additional\n# configuration files. Thus, we try to implement decisions at build time.\n#\n# This is a work in progress, and we're very interested in feedback.\n#\n# OpenMPI has numerous ways to communicate messages [1]. The ones relevant to\n# this build and the interconnects they support are:\n#\n#   Module        Eth   IB    OPA   RoCE    note  decision\n#   ------------  ----  ----  ----  ----    ----  --------\n#\n#   ob1 : tcp      Y*    X     X     X      a     include\n#   ob1 : openib   N     Y     Y     Y      b,c   exclude\n#   cm  : psm2     N     N     Y*    N            include\n#       : ucx      Y?    Y*    N     Y?     b,d   include\n#\n#   Y : supported\n#   Y*: best choice for that interconnect\n#   X : supported but sub-optimal\n#\n#   a : No RDMA, so performance will suffer.\n#   b : Uses libibverbs.\n#   c : Will be removed in OpenMPI 5.\n#   d : Uses Mellanox libraries if available in preference to libibverbs.\n#\n# You can check what's available with:\n#\n#   $ ch-run /var/tmp/openmpi -- ompi_info | egrep '(btl|mtl|pml)'\n#\n# The other build decisions are:\n#\n#   1. PMI/PMIx: Include these so that we can use srun or any other PMI[x]\n#      provider, with no matching OpenMPI needed on the host.\n#\n#   2. --disable-pty-support to avoid \"pipe function call failed when\n#      setting up I/O forwarding subsystem\".\n#\n#   3. --enable-mca-no-build=plm-slurm to support launching processes using\n#      the host's srun (i.e., the container OpenMPI needs to talk to the host\n#      Slurm's PMI) but prevent OpenMPI from invoking srun itself from within\n#      the container, where srun is not installed (the error messages from\n#      this are inscrutable).\n#\n# [1]: https://github.com/open-mpi/ompi/blob/master/README\n\n# OS packages needed to build this stuff.\n#\n# Note that libpsm2 is x86-64 only so we skip if missing\nRUN dnf install -y --setopt=install_weak_deps=false \\\n                automake \\\n                file \\\n                flex \\\n                gcc \\\n                gcc-c++ \\\n                gcc-gfortran \\\n                git \\\n                ibacm \\\n                libevent-devel \\\n                libtool \\\n                libibumad \\\n                libibumad-devel \\\n                libibverbs \\\n                libibverbs-devel \\\n                libibverbs-utils \\\n                librdmacm \\\n                librdmacm-devel \\\n                rdma-core \\\n                make \\\n                numactl-devel \\\n                wget \\\n && dnf install -y --setopt=install_weak_deps=false --skip-broken \\\n                libpsm2 \\\n                libpsm2-devel \\\n && dnf clean all\n\nWORKDIR /usr/local/src\n\n# UCX. There is a package but we want control over build options, specifically\n# multithreaded support.\nARG UCX_VERSION=1.11.2\nRUN git clone --branch v${UCX_VERSION} --depth 1 \\\n              https://github.com/openucx/ucx.git \\\n && cd ucx \\\n && ./autogen.sh \\\n && ./contrib/configure-release-mt --prefix=/usr/local \\\n && make -j$(getconf _NPROCESSORS_ONLN) install \\\n && rm -Rf ../ucx*\n\n# PMI2.\n#\n# There isn't a package available with the Slurm PMI2 libraries we need, so\n# build them from Slurm's release.\nARG SLURM_VERSION=19-05-3-2\nRUN wget https://github.com/SchedMD/slurm/archive/slurm-${SLURM_VERSION}.tar.gz \\\n && tar -xf slurm-${SLURM_VERSION}.tar.gz \\\n && cd slurm-slurm-${SLURM_VERSION} \\\n && ./configure --build=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" --prefix=/usr/local \\\n && cd contribs/pmi2 \\\n && make -j$(getconf _NPROCESSORS_ONLN) install \\\n && rm -Rf ../../../slurm* && rm slurm-${SLURM_VERSION}.tar.gz\n\n# OpenMPI.\n#\n# Patch OpenMPI to disable UCX plugin on systems with Intel or Cray HSNs. UCX\n# has inferior performance than PSM2/uGNI but higher priority.\nARG MPI_URL=https://www.open-mpi.org/software/ompi/v3.1/downloads\nARG MPI_VERSION=3.1.6\nRUN wget -nv ${MPI_URL}/openmpi-${MPI_VERSION}.tar.gz \\\n && tar xf openmpi-${MPI_VERSION}.tar.gz && rm openmpi-${MPI_VERSION}.tar.gz\nCOPY dont-init-ucx-on-intel-cray.patch ./openmpi-${MPI_VERSION}\nRUN cd openmpi-${MPI_VERSION} \\\n && patch -p1 dont-init-ucx-on-intel-cray.patch \\\n && CFLAGS=-O3 \\\n    CXXFLAGS=-O3 \\\n    ./configure --build=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" \\\n\n\n                --sysconfdir=/mnt/0 \\\n                --with-slurm \\\n\n                --with-pmix \\\n                --with-ucx \\\n                --disable-pty-support \\\n                --enable-mca-no-build=btl-openib,plm-slurm \\\n && make -j$(getconf _NPROCESSORS_ONLN) install \\\n && rm -Rf ../openmpi-${MPI_VERSION}*\nRUN ldconfig\n\n# OpenMPI expects this program to exist, even if it's not used. Default is\n# \"ssh : rsh\", but that's not installed.\nRUN echo 'plm_rsh_agent = false' >> /mnt/0/openmpi-mca-params.conf\n"
}
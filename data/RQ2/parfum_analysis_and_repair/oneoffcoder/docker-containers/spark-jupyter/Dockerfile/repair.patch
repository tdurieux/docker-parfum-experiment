diff --git a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/oneoffcoder/docker-containers/spark-jupyter/Dockerfile b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/oneoffcoder/docker-containers/spark-jupyter/Dockerfile/repaired.Dockerfile
index 2348d5c..0229107 100644
--- a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/oneoffcoder/docker-containers/spark-jupyter/Dockerfile
+++ b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/oneoffcoder/docker-containers/spark-jupyter/Dockerfile/repaired.Dockerfile
@@ -32,9 +32,9 @@ ENV NOTEBOOK_PASSWORD=""
 # setup ubuntu
 RUN apt-get update -y \
     && apt-get upgrade -y \
-    && apt-get -y install openjdk-8-jdk wget openssh-server sshpass supervisor \
-    && apt-get -y install nano net-tools lynx \
-    && apt-get clean
+    && apt-get -y --no-install-recommends install openjdk-8-jdk wget openssh-server sshpass supervisor \
+    && apt-get -y --no-install-recommends install nano net-tools lynx \
+    && apt-get clean && rm -rf /var/lib/apt/lists/*;
 
 # setup ssh
 RUN ssh-keygen -t rsa -P "" -f /root/.ssh/id_rsa \
@@ -43,7 +43,7 @@ RUN ssh-keygen -t rsa -P "" -f /root/.ssh/id_rsa \
 COPY ubuntu/root/.ssh/config /root/.ssh/config
 
 # setup hadoop
-RUN wget -q http://apache.mirrors.tds.net/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz -O /tmp/hadoop-3.2.1.tar.gz \
+RUN wget -q https://apache.mirrors.tds.net/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz -O /tmp/hadoop-3.2.1.tar.gz \
     && tar -xzf /tmp/hadoop-3.2.1.tar.gz -C /usr/local/ \
     && ln -s /usr/local/hadoop-3.2.1 /usr/local/hadoop \
     && rm -fr /usr/local/hadoop/etc/hadoop/* \
@@ -53,7 +53,7 @@ RUN wget -q http://apache.mirrors.tds.net/hadoop/common/hadoop-3.2.1/hadoop-3.2.
 	&& mkdir /var/hadoop/hadoop-namenode \
 	&& mkdir /var/hadoop/mr-history \
 	&& mkdir /var/hadoop/mr-history/done \
-	&& mkdir /var/hadoop/mr-history/tmp
+	&& mkdir /var/hadoop/mr-history/tmp && rm /tmp/hadoop-3.2.1.tar.gz
 COPY ubuntu/usr/local/hadoop/etc/hadoop/* /usr/local/hadoop/etc/hadoop/
 COPY ubuntu/usr/local/hadoop/extras/* /usr/local/hadoop/extras/
 RUN $HADOOP_HOME/bin/hdfs namenode -format oneoffcoder
@@ -62,7 +62,7 @@ RUN $HADOOP_HOME/bin/hdfs namenode -format oneoffcoder
 RUN wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz -O /tmp/spark-2.4.4-bin-hadoop2.7.tgz \
     && tar -xzf /tmp/spark-2.4.4-bin-hadoop2.7.tgz -C /usr/local/ \
     && ln -s /usr/local/spark-2.4.4-bin-hadoop2.7 /usr/local/spark \
-    && rm /usr/local/spark/conf/*.template
+    && rm /usr/local/spark/conf/*.template && rm /tmp/spark-2.4.4-bin-hadoop2.7.tgz
 COPY ubuntu/usr/local/spark/conf/* /usr/local/spark/conf/
 
 # setup conda
@@ -73,7 +73,7 @@ RUN wget -q https://repo.anaconda.com/archive/Anaconda3-2020.02-Linux-x86_64.sh
     && $CONDA_HOME/bin/conda env update -n base --file /tmp/environment.yml \
     && $CONDA_HOME/bin/conda update -n root conda -y \
     && $CONDA_HOME/bin/conda update --all -y \
-    && $CONDA_HOME/bin/pip install --upgrade pip
+    && $CONDA_HOME/bin/pip install --no-cache-dir --upgrade pip
 
 # setup volumes
 RUN mkdir /root/ipynb
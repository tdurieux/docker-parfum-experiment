{
  "startTime": 1674219159560,
  "endTime": 1674219161012,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 44,
        "lineEnd": 44,
        "columnStart": 7,
        "columnEnd": 25
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 56,
        "lineEnd": 56,
        "columnStart": 7,
        "columnEnd": 55
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 72,
        "lineEnd": 72,
        "columnStart": 7,
        "columnEnd": 65
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 34,
        "lineEnd": 43,
        "columnStart": 7,
        "columnEnd": 19
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 34,
        "lineEnd": 43,
        "columnStart": 7,
        "columnEnd": 19
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM ubuntu:latest\n\nLABEL org=\"One-Off Coder\"\nLABEL author=\"Jee Vang, Ph.D.\"\nLABEL email=\"info@oneoffcoder.com\"\nLABEL website=\"https://www.oneoffcoder.com\"\nLABEL facebook=\"https://www.facebook.com/oneoffcoder\"\nLABEL twitter=\"https://twitter.com/oneoffcoder\"\nLABEL instagram=\"https://www.instagram.com/oneoffcoder/\"\nLABEL youtube=\"https://www.youtube.com/channel/UCCCv8Glpb2dq2mhUj5mcHCQ\"\nLABEL linkedin=\"https://www.linkedin.com/company/one-off-coder\"\n\nENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\nENV HDFS_NAMENODE_USER=root\nENV HDFS_DATANODE_USER=root\nENV HDFS_SECONDARYNAMENODE_USER=root\nENV YARN_RESOURCEMANAGER_USER=root\nENV YARN_NODEMANAGER_USER=root\nENV YARN_PROXYSERVER_USER=root\nENV HADOOP_HOME=/usr/local/hadoop\nENV HADOOP_YARN_HOME=${HADOOP_HOME}\nENV HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\nENV HADOOP_LOG_DIR=${HADOOP_YARN_HOME}/logs\nENV HADOOP_IDENT_STRING=root\nENV HADOOP_MAPRED_IDENT_STRING=root\nENV HADOOP_MAPRED_HOME=${HADOOP_HOME}\nENV SPARK_HOME=/usr/local/spark\nENV CONDA_HOME=/usr/local/conda\nENV PYSPARK_MASTER=yarn\nENV PATH=${CONDA_HOME}/bin:${SPARK_HOME}/bin:${HADOOP_HOME}/bin:${PATH}\n\n# setup ubuntu\nRUN apt-get update -y \\\n    && apt-get upgrade -y \\\n    && DEBIAN_FRONTEND=noninteractive apt-get --no-install-recommends -y install \\\n        openjdk-8-jdk \\\n        wget \\\n        openssh-server \\\n        sshpass \\\n        supervisor \\\n        nano \\\n        net-tools \\\n        lynx \\\n        python3-pip \\\n    && pip install --no-cache-dir psutil \\\n    && apt-get clean \\\n    && ln -s /usr/bin/python3.8 /usr/bin/python && rm -rf /var/lib/apt/lists/*;\n\n# setup ssh\nRUN ssh-keygen -t rsa -P \"\" -f /root/.ssh/id_rsa \\\n    && cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys \\\n    && chmod 0600 /root/.ssh/authorized_keys\nCOPY ubuntu/root/.ssh/config /root/.ssh/config\n\n# install hadoop\nRUN wget -q https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz -O /tmp/hadoop-3.3.1.tar.gz \\\n    && tar -xzf /tmp/hadoop-3.3.1.tar.gz -C /usr/local/ \\\n    && ln -s /usr/local/hadoop-3.3.1 /usr/local/hadoop \\\n    && rm -fr /usr/local/hadoop/etc/hadoop/* \\\n    && mkdir /usr/local/hadoop/extras \\\n    && mkdir /var/hadoop \\\n\t&& mkdir /var/hadoop/hadoop-datanode \\\n\t&& mkdir /var/hadoop/hadoop-namenode \\\n\t&& mkdir /var/hadoop/mr-history \\\n\t&& mkdir /var/hadoop/mr-history/done \\\n\t&& mkdir /var/hadoop/mr-history/tmp && rm /tmp/hadoop-3.3.1.tar.gz\nCOPY ubuntu/usr/local/hadoop/etc/hadoop/* /usr/local/hadoop/etc/hadoop/\nCOPY ubuntu/usr/local/hadoop/extras/* /usr/local/hadoop/extras/\nRUN $HADOOP_HOME/bin/hdfs namenode -format oneoffcoder\n\n# setup spark\nRUN wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz -O /tmp/spark-3.1.2-bin-hadoop3.2.tgz \\\n    && tar -xzf /tmp/spark-3.1.2-bin-hadoop3.2.tgz -C /usr/local/ \\\n    && ln -s /usr/local/spark-3.1.2-bin-hadoop3.2 /usr/local/spark \\\n    && rm /usr/local/spark/conf/*.template && rm /tmp/spark-3.1.2-bin-hadoop3.2.tgz\nCOPY ubuntu/usr/local/spark/conf/* /usr/local/spark/conf/\n\n# clean up\nRUN rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* \\\n    && mkdir /tmp/spark-events\n"
}
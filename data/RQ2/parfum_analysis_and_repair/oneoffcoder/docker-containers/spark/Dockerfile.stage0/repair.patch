diff --git a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/oneoffcoder/docker-containers/spark/Dockerfile.stage0 b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/oneoffcoder/docker-containers/spark/Dockerfile.stage0/repaired.Dockerfile
index a5d0dd5..e9ba680 100644
--- a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/oneoffcoder/docker-containers/spark/Dockerfile.stage0
+++ b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/oneoffcoder/docker-containers/spark/Dockerfile.stage0/repaired.Dockerfile
@@ -32,7 +32,7 @@ ENV PATH=${CONDA_HOME}/bin:${SPARK_HOME}/bin:${HADOOP_HOME}/bin:${PATH}
 # setup ubuntu
 RUN apt-get update -y \
     && apt-get upgrade -y \
-    && DEBIAN_FRONTEND=noninteractive apt-get -y install \
+    && DEBIAN_FRONTEND=noninteractive apt-get --no-install-recommends -y install \
         openjdk-8-jdk \
         wget \
         openssh-server \
@@ -42,9 +42,9 @@ RUN apt-get update -y \
         net-tools \
         lynx \
         python3-pip \
-    && pip install psutil \
+    && pip install --no-cache-dir psutil \
     && apt-get clean \
-    && ln -s /usr/bin/python3.8 /usr/bin/python
+    && ln -s /usr/bin/python3.8 /usr/bin/python && rm -rf /var/lib/apt/lists/*;
 
 # setup ssh
 RUN ssh-keygen -t rsa -P "" -f /root/.ssh/id_rsa \
@@ -63,7 +63,7 @@ RUN wget -q https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar
 	&& mkdir /var/hadoop/hadoop-namenode \
 	&& mkdir /var/hadoop/mr-history \
 	&& mkdir /var/hadoop/mr-history/done \
-	&& mkdir /var/hadoop/mr-history/tmp
+	&& mkdir /var/hadoop/mr-history/tmp && rm /tmp/hadoop-3.3.1.tar.gz
 COPY ubuntu/usr/local/hadoop/etc/hadoop/* /usr/local/hadoop/etc/hadoop/
 COPY ubuntu/usr/local/hadoop/extras/* /usr/local/hadoop/extras/
 RUN $HADOOP_HOME/bin/hdfs namenode -format oneoffcoder
@@ -72,7 +72,7 @@ RUN $HADOOP_HOME/bin/hdfs namenode -format oneoffcoder
 RUN wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz -O /tmp/spark-3.1.2-bin-hadoop3.2.tgz \
     && tar -xzf /tmp/spark-3.1.2-bin-hadoop3.2.tgz -C /usr/local/ \
     && ln -s /usr/local/spark-3.1.2-bin-hadoop3.2 /usr/local/spark \
-    && rm /usr/local/spark/conf/*.template
+    && rm /usr/local/spark/conf/*.template && rm /tmp/spark-3.1.2-bin-hadoop3.2.tgz
 COPY ubuntu/usr/local/spark/conf/* /usr/local/spark/conf/
 
 # clean up
{
  "startTime": 1674249033178,
  "endTime": 1674249034119,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 18,
        "lineEnd": 18,
        "columnStart": 4,
        "columnEnd": 119
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 20,
        "lineEnd": 20,
        "columnStart": 4,
        "columnEnd": 139
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 18,
        "lineEnd": 18,
        "columnStart": 4,
        "columnEnd": 119
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 20,
        "lineEnd": 20,
        "columnStart": 4,
        "columnEnd": 139
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 13,
        "lineEnd": 14,
        "columnStart": 22,
        "columnEnd": 23
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Start with the \"min\" image, and from the \"full\" image, copy only what we need.\n# See https://github.com/triton-inference-server/server/blob/master/docs/compose.md\nFROM nvcr.io/nvidia/tritonserver:21.03-py3 as full\nFROM nvcr.io/nvidia/tritonserver:21.03-py3-min\nCOPY --from=full /opt/tritonserver/bin /opt/tritonserver/bin\nCOPY --from=full /opt/tritonserver/lib /opt/tritonserver/lib\nCOPY --from=full /opt/tritonserver/backends/dali /opt/tritonserver/backends/dali\nCOPY --from=full /opt/tritonserver/backends/python /opt/tritonserver/backends/python\nCOPY --from=full /opt/tritonserver/backends/tensorflow1 /opt/tritonserver/backends/tensorflow1\nCOPY --from=full /usr/lib/x86_64-linux-gnu/libre2.so.5 /usr/lib/x86_64-linux-gnu/libre2.so.5\nCOPY --from=full /usr/lib/x86_64-linux-gnu/libb64.so.0d /usr/lib/x86_64-linux-gnu/libb64.so.0d\nCOPY --from=full /usr/local/lib/python3.8/dist-packages /usr/local/lib/python3.8/dist-packages\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    python3 python3-pip && rm -rf /var/lib/apt/lists/*;\n\n# Download the large models. Github frowns upon such large files, so we store them elsewhere.\nWORKDIR /models/yolov4/1/\nRUN curl -f https://opencv.facetraining.mobiledgex.net/TritonInferenceServer/models/yolov4/1/model.plan --output model.plan\nWORKDIR /models/inception_graphdef/1\nRUN curl -f https://opencv.facetraining.mobiledgex.net/TritonInferenceServer/models/inception_graphdef/1/model.graphdef --output model.graphdef\n\nCOPY ./models /models\nCOPY ./plugins /plugins\n# dali_src not needed to function, but good to have available for reference.\nCOPY ./dali_src /dali_src\n\n# ports for REST, GRPC, and Stats\nEXPOSE 8000/tcp\nEXPOSE 8001/tcp\nEXPOSE 8002/tcp\n\nWORKDIR /opt/tritonserver/bin\n\nENV LD_LIBRARY_PATH /usr/local/cuda/compat/lib\nENV LIBRARY_PATH /usr/local/cuda/lib64/stubs\nENV PATH /opt/tritonserver/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin\nENV LD_PRELOAD=/plugins/libmyplugins.so\nCMD [\"tritonserver\", \"--model-repository=/models\", \"--strict-model-config=false\", \"--grpc-infer-allocation-pool-size=16\", \"--log-verbose\", \"1\"]\n"
}
{
  "startTime": 1674256140301,
  "endTime": 1674256140886,
  "originalSmells": [
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 39,
        "lineEnd": 39,
        "columnStart": 26,
        "columnEnd": 95
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "ARG CUDA_DOCKER_VERSION=10.0-devel-ubuntu20.04\nFROM nvidia/cuda:${CUDA_DOCKER_VERSION}\n\n# Arguments for the build. CUDA_DOCKER_VERSION needs to be repeated because\n# the first usage only applies to the FROM tag.\nARG CUDA_DOCKER_VERSION=10.0-devel-ubuntu20.04\nARG CUDNN_VERSION=7.6.0.64-1+cuda10.0\nARG NCCL_VERSION_OVERRIDE=2.4.7-1+cuda10.0\nARG MPI_KIND=OpenMPI\nARG PYTHON_VERSION=3.6\nARG GPP_VERSION=7\nARG TENSORFLOW_PACKAGE=tensorflow-gpu==1.15.0\nARG KERAS_PACKAGE=keras==2.2.4\nARG PYTORCH_PACKAGE=torch==1.2.0\nARG PYTORCH_LIGHTNING_PACKAGE=pytorch_lightning==0.7.6\nARG TORCHVISION_PACKAGE=torchvision==0.4.0\nARG MXNET_PACKAGE=mxnet-cu100==1.5.0\nARG PYSPARK_PACKAGE=pyspark==2.4.7\n# if SPARK_PACKAGE is set, installs Spark into /spark from the tgz archive\n# if SPARK_PACKAGE is a preview version, installs PySpark from the tgz archive\n# see https://archive.apache.org/dist/spark/ for available packages, version must match PYSPARK_PACKAGE\nARG SPARK_PACKAGE=spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\nARG HOROVOD_BUILD_FLAGS=\"HOROVOD_GPU_OPERATIONS=NCCL\"\nARG HOROVOD_MIXED_INSTALL=0\n\n# to avoid interaction with apt-get\nENV DEBIAN_FRONTEND=noninteractive\n\n# Set default shell to /bin/bash\nSHELL [\"/bin/bash\", \"-euo\", \"pipefail\", \"-c\"]\n\n# Extract ubuntu distribution version and download the corresponding key.\n# This is to fix CI failures caused by the new rotating key mechanism rolled out by Nvidia.\n# Refer to https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212771 for more details.\n#RUN DIST=$(echo ${CUDA_DOCKER_VERSION#*ubuntu} | sed 's/\\.//'); \\\n#    apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu${DIST}/x86_64/3bf863cc.pub && \\\n#    apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu${DIST}/x86_64/7fa2af80.pub\n\n# Prepare to install specific g++ versions\nRUN apt-get update -qq && apt-get install -y --no-install-recommends software-properties-common && rm -rf /var/lib/apt/lists/*;\nRUN add-apt-repository ppa:ubuntu-toolchain-r/test\n\n# Install essential packages.\nRUN CUDNN_MAJOR=$(cut -d '.' -f 1 <<< \"${CUDNN_VERSION}\"); \\\n    apt-get update -qq && apt-get install -y --allow-downgrades --allow-change-held-packages --no-install-recommends \\\n        wget \\\n        ca-certificates \\\n        cmake \\\n        openssh-client \\\n        openssh-server \\\n        git \\\n        build-essential \\\n        g++-${GPP_VERSION} \\\n        moreutils \\\n        libcudnn${CUDNN_MAJOR}=${CUDNN_VERSION} \\\n        libnccl2=${NCCL_VERSION_OVERRIDE} \\\n        libnccl-dev=${NCCL_VERSION_OVERRIDE}\n\n# setup ssh service\nRUN ssh-keygen -f /root/.ssh/id_rsa -q -N ''\nRUN cp -v /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n\n# Install Python.\nRUN apt-get update -qq && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-distutils\nRUN ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python\nRUN ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python${PYTHON_VERSION/%.*/}\nRUN wget --progress=dot:mega https://bootstrap.pypa.io/get-pip.py && python get-pip.py && rm get-pip.py\n\n# pinning pip to 21.0.0 as 22.0.0 cannot fetch pytorch packages from html linl\n# https://github.com/pytorch/pytorch/issues/72045\nRUN pip install --no-cache-dir -U --force pip~=21.0.0 \"setuptools<60.1.0\" requests pytest mock pytest-forked parameterized\n\n# Add launch helper scripts\nRUN echo \"env SPARK_HOME=/spark SPARK_DRIVER_MEM=512m PYSPARK_PYTHON=/usr/bin/python${PYTHON_VERSION} PYSPARK_DRIVER_PYTHON=/usr/bin/python${PYTHON_VERSION} \\\"\\$@\\\"\" > /spark_env.sh\nRUN echo /spark_env.sh pytest -v --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.\\$1.\\${HOROVOD_RANK:-\\${OMPI_COMM_WORLD_RANK:-\\${PMI_RANK}}}.\\$2.xml \\${@:2} > /pytest.sh\nRUN echo /spark_env.sh pytest -v --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.\\$1.standalone.\\$2.xml --forked \\${@:2} > /pytest_standalone.sh\nRUN chmod a+x /spark_env.sh\nRUN chmod a+x /pytest.sh\nRUN chmod a+x /pytest_standalone.sh\n\n# Install Spark stand-alone cluster.\nRUN if [[ -n ${SPARK_PACKAGE} ]]; then \\\n        wget --progress=dot:giga \"https://www.apache.org/dyn/closer.lua/spark/${SPARK_PACKAGE}?action=download\" -O - | tar -xzC /tmp; \\\n        archive=$(basename \"${SPARK_PACKAGE}\") bash -c \"mv -v /tmp/\\${archive/%.tgz/} /spark\"; \\\n    fi\n\n# Install PySpark.\nRUN apt-get update -qq && apt install -y openjdk-8-jdk-headless\nRUN if [[ ${SPARK_PACKAGE} != *\"-preview\"* ]]; then \\\n        pip install --no-cache-dir ${PYSPARK_PACKAGE}; \\\n    else \\\n        apt-get update -qq && apt-get install pandoc; \\\n        pip install --no-cache-dir pypandoc; \\\n        (cd /spark/python && python setup.py sdist && pip install --no-cache-dir dist/pyspark-*.tar.gz && rm dist/pyspark-*); \\\n    fi\n\n# Install Ray.\nRUN pip install --no-cache-dir ray==1.3.0\n\n# Install MPI.\nRUN if [[ ${MPI_KIND} == \"OpenMPI\" ]]; then \\\n        wget --progress=dot:mega -O /tmp/openmpi-3.0.0-bin.tar.gz https://github.com/horovod/horovod/files/1596799/openmpi-3.0.0-bin.tar.gz && \\\n            cd /usr/local && tar -zxf /tmp/openmpi-3.0.0-bin.tar.gz && ldconfig && \\\n            echo \"mpirun -allow-run-as-root -np 2 -H localhost:2 -bind-to none -map-by slot -mca mpi_abort_print_stack 1\" > /mpirun_command; \\\n    elif [[ ${MPI_KIND} == \"MPICH\" ]]; then \\\n        apt-get update -qq && apt-get install -y mpich && \\\n            echo \"mpirun -np 2\" > /mpirun_command; \\\n    fi\n\n# Set default NCCL parameters\nRUN echo NCCL_DEBUG=INFO >> /etc/nccl.conf\n\n# Install mpi4py.\n# This requires SETUPTOOLS_USE_DISTUTILS=stdlib as with setuptools>=60.1.0 installing mpi4py broke\n# https://github.com/mpi4py/mpi4py/issues/157#issuecomment-1001022274\nRUN if [[ ${MPI_KIND} != \"None\" ]]; then \\\n        SETUPTOOLS_USE_DISTUTILS=stdlib pip install --no-cache-dir mpi4py; \\\n    fi\n\n# Install TensorFlow and Keras (releases).\n# Pin scipy!=1.4.0: https://github.com/scipy/scipy/issues/11237\n# Pin protobuf~=3.20 for tensorflow<2.6.5: https://github.com/tensorflow/tensorflow/issues/56077\nRUN if [[ ${TENSORFLOW_PACKAGE} != \"tf-nightly-gpu\" ]]; then \\\n        PROTOBUF_PACKAGE=\"\"; \\\n        if [[ ${TENSORFLOW_PACKAGE} == tensorflow-gpu==1.15.* ]] || \\\n           [[ ${TENSORFLOW_PACKAGE} == tensorflow-gpu==2.[012345].* ]]; then \\\n            PROTOBUF_PACKAGE=\"protobuf~=3.20\"; \\\n        fi; \\\n        pip install --no-cache-dir ${TENSORFLOW_PACKAGE} ${PROTOBUF_PACKAGE}; \\\n        if [[ ${KERAS_PACKAGE} != \"None\" ]]; then \\\n            pip uninstall -y keras; \\\n            pip install --no-cache-dir ${KERAS_PACKAGE} \"scipy!=1.4.0\" \"pandas<1.1.0\"; \\\n        fi; \\\n        mkdir -p ~/.keras; \\\n        ldconfig /usr/local/cuda/targets/x86_64-linux/lib/stubs; \\\n        python -c \"import tensorflow as tf; tf.keras.datasets.mnist.load_data()\"; \\\n        ldconfig; \\\n    fi\n\n# Pin h5py < 3 for tensorflow: https://github.com/tensorflow/tensorflow/issues/44467\nRUN pip install 'h5py<3.0' --force-reinstall\n\n# Install PyTorch (releases).\n# Pin Pillow<7.0 for torchvision < 0.5.0: https://github.com/pytorch/vision/issues/1718\n# Pin Pillow!=8.3.0 for torchvision: https://github.com/pytorch/vision/issues/4146\nRUN if [[ ${PYTORCH_PACKAGE} != \"torch-nightly-cu\"* ]]; then \\\n        pip install --no-cache-dir ${PYTORCH_PACKAGE} ${TORCHVISION_PACKAGE} -f https://download.pytorch.org/whl/${PYTORCH_PACKAGE/*+/}/torch_stable.html; \\\n        if [[ \"${TORCHVISION_PACKAGE/%+*/}\" == torchvision==0.[1234].* ]]; then \\\n            pip install --no-cache-dir \"Pillow<7.0\" --no-deps; \\\n        else \\\n            pip install --no-cache-dir \"Pillow!=8.3.0\" --no-deps; \\\n        fi; \\\n    fi\nRUN pip install ${PYTORCH_LIGHTNING_PACKAGE}\n\n# Install MXNet (releases).\nRUN if [[ ${MXNET_PACKAGE} != \"mxnet-nightly-cu\"* ]]; then \\\n        pip install --no-cache-dir ${MXNET_PACKAGE} ; \\\n    fi\n\n# Prefetch Spark MNIST dataset.\nRUN mkdir -p /work\nRUN mkdir -p /data && wget --progress=dot:mega https://horovod-datasets.s3.amazonaws.com/mnist.bz2 -O /data/mnist.bz2\n\n# Prefetch Spark Rossmann dataset.\nRUN mkdir -p /work\nRUN mkdir -p /data && wget --progress=dot:mega https://horovod-datasets.s3.amazonaws.com/rossmann.tgz -O - | tar -xzC /data\n\n# Prefetch PyTorch datasets.\nRUN wget --progress=dot:mega https://horovod-datasets.s3.amazonaws.com/pytorch_datasets.tgz -O - | tar -xzC /data\n\n### END OF CACHE ###\nCOPY . /horovod\n\n# Install nightly packages here so they do not get cached\n\n# Install TensorFlow and Keras (nightly).\n# Pin scipy!=1.4.0: https://github.com/scipy/scipy/issues/11237\nRUN if [[ ${TENSORFLOW_PACKAGE} == \"tf-nightly-gpu\" ]]; then \\\n        pip install --no-cache-dir ${TENSORFLOW_PACKAGE}; \\\n        if [[ ${KERAS_PACKAGE} != \"None\" ]]; then \\\n            pip uninstall -y keras-nightly; \\\n            pip install --no-cache-dir ${KERAS_PACKAGE} \"scipy!=1.4.0\" \"pandas<1.1.0\"; \\\n        fi; \\\n        mkdir -p ~/.keras; \\\n        ldconfig /usr/local/cuda/targets/x86_64-linux/lib/stubs; \\\n        python -c \"import tensorflow as tf; tf.keras.datasets.mnist.load_data()\"; \\\n        ldconfig; \\\n    fi\n\n# Install PyTorch (nightly).\n# Pin Pillow!=8.3.0 for torchvision: https://github.com/pytorch/vision/issues/4146\nRUN if [[ ${PYTORCH_PACKAGE} == \"torch-nightly-cu\"* ]]; then \\\n        pip install --no-cache-dir --pre torch ${TORCHVISION_PACKAGE} -f https://download.pytorch.org/whl/nightly/${PYTORCH_PACKAGE/#torch-nightly-/}/torch_nightly.html; \\\n        pip install --no-cache-dir \"Pillow!=8.3.0\" --no-deps; \\\n    fi\n\n# Install MXNet (nightly).\nRUN if [[ ${MXNET_PACKAGE} == \"mxnet-nightly-cu\"* ]]; then \\\n        pip install --no-cache-dir --pre ${MXNET_PACKAGE/-nightly/} -f https://dist.mxnet.io/python/${MXNET_PACKAGE/#mxnet-nightly-/}; \\\n    fi\n\n# Install Horovod.\nRUN cd /horovod && \\\n    python setup.py sdist && \\\n    ldconfig /usr/local/cuda/targets/x86_64-linux/lib/stubs && \\\n    bash -c \"${HOROVOD_BUILD_FLAGS} HOROVOD_WITH_TENSORFLOW=1 HOROVOD_WITH_PYTORCH=1 HOROVOD_WITH_MXNET=1 pip install --no-cache-dir -v $(ls /horovod/dist/horovod-*.tar.gz)[spark,ray]\" && \\\n    ldconfig\n\n# Show the effective python package version to easily spot version differences\nRUN pip freeze | sort\n\n# Hack for compatibility of MNIST example with TensorFlow 1.1.0.\nRUN if [[ ${TENSORFLOW_PACKAGE} == \"tensorflow-gpu==1.1.0\" ]]; then \\\n        sed -i \"s/from tensorflow import keras/from tensorflow.contrib import keras/\" /horovod/examples/tensorflow/tensorflow_mnist.py; \\\n    fi\n\n# Hack TensorFlow MNIST example to be smaller.\nRUN sed -i \"s/last_step=20000/last_step=100/\" /horovod/examples/tensorflow/tensorflow_mnist.py\n\n# Hack TensorFlow Eager MNIST example to be smaller.\nRUN sed -i \"s/dataset.take(20000/dataset.take(100/\" /horovod/examples/tensorflow/tensorflow_mnist_eager.py\n\n# Hack TensorFlow 2.0 example to be smaller.\nRUN sed -i \"s/dataset.take(10000/dataset.take(100/\" /horovod/examples/tensorflow2/tensorflow2_mnist.py\n\n# Hack TensorFlow 2.0 Data Service example to be smaller.\nRUN sed -i \"s/ epochs=24/ epochs=4/\" /horovod/examples/tensorflow2/tensorflow2_mnist_data_service_train_fn_*_side_dispatcher.py\n\n# Hack Keras MNIST advanced example to be smaller.\nRUN sed -i \"s/'--epochs', type=int, default=24,/'--epochs', type=int, default=9,/\" /horovod/examples/keras/keras_mnist_advanced.py\n\n# Hack TensorFlow 2.0 Keras MNIST advanced example to be smaller.\nRUN sed -i \"s/epochs=24/epochs=9/\" /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py\n\n# Hack PyTorch MNIST example to be smaller.\nRUN sed -i \"s/'--epochs', type=int, default=10,/'--epochs', type=int, default=2,/\" /horovod/examples/pytorch/pytorch_mnist.py\n\n# Hack Keras Spark Rossmann Run example to be smaller.\nRUN sed -i \"s/x = Dense(1000,/x = Dense(100,/g\" /horovod/examples/spark/keras/keras_spark_rossmann_run.py\nRUN sed -i \"s/x = Dense(500,/x = Dense(50,/g\" /horovod/examples/spark/keras/keras_spark_rossmann_run.py\n\n# Hack Keras Spark Rossmann Estimator example to be smaller.\nRUN sed -i \"s/x = Dense(1000,/x = Dense(100,/g\" /horovod/examples/spark/keras/keras_spark_rossmann_estimator.py\nRUN sed -i \"s/x = Dense(500,/x = Dense(50,/g\" /horovod/examples/spark/keras/keras_spark_rossmann_estimator.py\n\n# Export HOROVOD_MIXED_INSTALL\nENV HOROVOD_MIXED_INSTALL=${HOROVOD_MIXED_INSTALL}\n"
}
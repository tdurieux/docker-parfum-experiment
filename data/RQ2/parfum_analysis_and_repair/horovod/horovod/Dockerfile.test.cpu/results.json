{
  "startTime": 1674249759639,
  "endTime": 1674249761882,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 158,
        "lineEnd": 158,
        "columnStart": 4,
        "columnEnd": 44
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 171,
        "lineEnd": 171,
        "columnStart": 4,
        "columnEnd": 44
      }
    },
    {
      "rule": "aptGetInstallUseY",
      "position": {
        "lineStart": 79,
        "lineEnd": 79,
        "columnStart": 30,
        "columnEnd": 52
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 51,
        "lineEnd": 51,
        "columnStart": 26,
        "columnEnd": 130
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 75,
        "lineEnd": 75,
        "columnStart": 26,
        "columnEnd": 63
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 79,
        "lineEnd": 79,
        "columnStart": 30,
        "columnEnd": 52
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 124,
        "lineEnd": 124,
        "columnStart": 30,
        "columnEnd": 54
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 31,
        "lineEnd": 31,
        "columnStart": 26,
        "columnEnd": 95
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 35,
        "lineEnd": 44,
        "columnStart": 26,
        "columnEnd": 17
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 51,
        "lineEnd": 51,
        "columnStart": 26,
        "columnEnd": 130
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 75,
        "lineEnd": 75,
        "columnStart": 26,
        "columnEnd": 63
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 79,
        "lineEnd": 79,
        "columnStart": 30,
        "columnEnd": 52
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 124,
        "lineEnd": 124,
        "columnStart": 30,
        "columnEnd": 54
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "ARG UBUNTU_VERSION=20.04\nFROM ubuntu:${UBUNTU_VERSION}\n\n# Arguments for the build. UBUNTU_VERSION needs to be repeated because\n# the first usage only applies to the FROM tag.\nARG UBUNTU_VERSION=20.04\nARG MPI_KIND=OpenMPI\nARG PYTHON_VERSION=3.6\nARG GPP_VERSION=7\n# NOTE: keep versions in sync with setup.py extras_require{'dev'}:\nARG TENSORFLOW_PACKAGE=tensorflow-cpu==1.15.0\nARG KERAS_PACKAGE=keras==2.2.4\nARG PYTORCH_PACKAGE=torch==1.2.0+cpu\nARG PYTORCH_LIGHTNING_PACKAGE=pytorch_lightning==0.7.6\nARG TORCHVISION_PACKAGE=torchvision==0.4.0+cpu\nARG MXNET_PACKAGE=mxnet==1.5.0\nARG PYSPARK_PACKAGE=pyspark==2.4.7\n# if SPARK_PACKAGE is set, installs Spark into /spark from the tgz archive\n# if SPARK_PACKAGE is a preview version, installs PySpark from the tgz archive\n# see https://archive.apache.org/dist/spark/ for available packages, version must match PYSPARK_PACKAGE\nARG SPARK_PACKAGE=spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\nARG CCL_PACKAGE=master\nARG HOROVOD_BUILD_FLAGS=\"\"\n\n# to avoid interaction with apt-get\nENV DEBIAN_FRONTEND=noninteractive\n\n# Set default shell to /bin/bash\nSHELL [\"/bin/bash\", \"-euo\", \"pipefail\", \"-c\"]\n\n# Prepare to install specific g++ versions\nRUN apt-get update -qq && apt-get install -y --no-install-recommends software-properties-common && rm -rf /var/lib/apt/lists/*;\nRUN add-apt-repository ppa:ubuntu-toolchain-r/test\n\n# Install essential packages.\nRUN apt-get update -qq && apt-get install -y --no-install-recommends \\\n        wget \\\n        ca-certificates \\\n        cmake \\\n        openssh-client \\\n        openssh-server \\\n        git \\\n        build-essential \\\n        g++-${GPP_VERSION} \\\n        moreutils && rm -rf /var/lib/apt/lists/*;\n\n# setup ssh service\nRUN ssh-keygen -f /root/.ssh/id_rsa -q -N ''\nRUN cp -v /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n\n# Install Python.\nRUN apt-get update -qq && apt-get install --no-install-recommends -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-distutils && rm -rf /var/lib/apt/lists/*;\nRUN ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python\nRUN ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python${PYTHON_VERSION/%.*/}\nRUN wget --progress=dot:mega https://bootstrap.pypa.io/get-pip.py && python get-pip.py && rm get-pip.py\n\n# pinning pip to 21.0.0 as 22.0.0 cannot fetch pytorch packages from html linl\n# https://github.com/pytorch/pytorch/issues/72045\nRUN pip install --no-cache-dir -U --force pip~=21.0.0 setuptools requests pytest mock pytest-forked parameterized\n\n# Add launch helper scripts\nRUN echo \"env SPARK_HOME=/spark SPARK_DRIVER_MEM=512m PYSPARK_PYTHON=/usr/bin/python${PYTHON_VERSION} PYSPARK_DRIVER_PYTHON=/usr/bin/python${PYTHON_VERSION} \\\"\\$@\\\"\" > /spark_env.sh\nRUN echo /spark_env.sh pytest -v --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.\\$1.\\${HOROVOD_RANK:-\\${OMPI_COMM_WORLD_RANK:-\\${PMI_RANK}}}.\\$2.xml \\${@:2} > /pytest.sh\nRUN echo /spark_env.sh pytest -v --capture=no --continue-on-collection-errors --junit-xml=/artifacts/junit.\\$1.standalone.\\$2.xml --forked \\${@:2} > /pytest_standalone.sh\nRUN chmod a+x /spark_env.sh\nRUN chmod a+x /pytest.sh\nRUN chmod a+x /pytest_standalone.sh\n\n# Install Spark stand-alone cluster.\nRUN if [[ -n ${SPARK_PACKAGE} ]]; then \\\n        wget --progress=dot:giga \"https://www.apache.org/dyn/closer.lua/spark/${SPARK_PACKAGE}?action=download\" -O - | tar -xzC /tmp; \\\n        archive=$(basename \"${SPARK_PACKAGE}\") bash -c \"mv -v /tmp/\\${archive/%.tgz/} /spark\"; \\\n    fi\n\n# Install PySpark.\nRUN apt-get update -qq && apt install --no-install-recommends -y openjdk-8-jdk-headless && rm -rf /var/lib/apt/lists/*;\nRUN if [[ ${SPARK_PACKAGE} != *\"-preview\"* ]];then \\\n        pip install --no-cache-dir ${PYSPARK_PACKAGE}; \\\n    else \\\n        apt-get update -qq && apt-get install -y --no-install-recommends pandoc; rm -rf /var/lib/apt/lists/*; \\\n        pip install --no-cache-dir pypandoc; \\\n        (cd /spark/python && python setup.py sdist && pip install --no-cache-dir dist/pyspark-*.tar.gz && rm dist/pyspark-*); \\\n    fi\n\n# Pin cloudpickle to 1.3.0\n# Dill breaks clouldpickle > 1.3.0 when using Spark2\n# https://github.com/cloudpipe/cloudpickle/issues/393\nRUN if [[ ${PYSPARK_PACKAGE} == \"pyspark==2.\"* ]]; then \\\n        pip install --no-cache-dir cloudpickle==1.3.0; \\\n    fi\n\n# Install Ray.\n# Updating to 1.7.0 to pass ray tests\nRUN pip install --no-cache-dir ray==1.7.0\n\n# Install MPI.\nRUN if [[ ${MPI_KIND} == \"OpenMPI\" ]];then \\\n        wget --progress=dot:mega -O /tmp/openmpi-3.0.0-bin.tar.gz https://github.com/horovod/horovod/files/1596799/openmpi-3.0.0-bin.tar.gz && \\\n            cd /usr/local && tar -zxf /tmp/openmpi-3.0.0-bin.tar.gz && ldconfig && \\\n            echo \"mpirun -allow-run-as-root -np 2 -H localhost:2 -bind-to none -map-by slot -mca mpi_abort_print_stack 1\" > /mpirun_command; \\\n    elif [[ ${MPI_KIND} == \"ONECCL\" ]];then \\\n        wget --progress=dot:mega -O /tmp/oneccl.tar.gz https://github.com/oneapi-src/oneCCL/archive/${CCL_PACKAGE}.tar.gz && \\\n            cd /tmp && tar -zxf oneccl.tar.gz && \\\n            mkdir oneCCL-${CCL_PACKAGE}/build && cd oneCCL-${CCL_PACKAGE}/build && cmake .. -DCMAKE_INSTALL_PREFIX=/usr/local/oneccl -DCMAKE_BUILD_TYPE=Release && make -j install && \\\n            rm /tmp/oneccl.tar.gz && rm -Rf /tmp/oneCCL-${CCL_PACKAGE} && \\\n            sed -i 's/if \\[ -z \\\"\\${I_MPI_ROOT}\\\" \\]/if [ -z \\\"${I_MPI_ROOT:-}\\\" ]/g' /usr/local/oneccl/env/setvars.sh && \\\n            sed -i 's/ \\$1/ \\${1:-}/g' /usr/local/oneccl/env/setvars.sh && \\\n            echo \". /usr/local/oneccl/env/setvars.sh\" > /oneccl_env && \\\n            chmod +x /oneccl_env && \\\n            echo \"export CCL_ATL_TRANSPORT=ofi; \\\n                  export HOROVOD_CCL_CACHE=1; \\\n                  echo \\\"\\$(env)\\\"; \\\n                  echo \\\"mpirun is \\$(which mpirun)\\\"; \\\n                  echo \\\"LD_LIBRARY_PATH is \\$(echo \\$LD_LIBRARY_PATH)\\\"; \\\n                  echo \\\"oneCCL links with \\$(ldd /usr/local/oneccl/lib/libccl.so)\\\"; \\\n                  mpirun -np 2 -hosts localhost \\$@\" > /mpirun_command_ofi && \\\n            chmod +x /mpirun_command_ofi && \\\n            cp /mpirun_command_ofi /mpirun_command_mpi && \\\n            sed -i 's/export CCL_ATL_TRANSPORT=ofi;/export CCL_ATL_TRANSPORT=mpi;/g' /mpirun_command_mpi && \\\n            sed -i 's/export HOROVOD_CCL_CACHE=1;/export HOROVOD_CCL_CACHE=0;/g' /mpirun_command_mpi && \\\n            echo \"/mpirun_command_mpi\" > /mpirun_command && \\\n            echo \"-L/usr/local/oneccl/lib -lmpi -I/usr/local/oneccl/include\" > /mpicc_oneccl && \\\n            chmod +x /mpicc_oneccl; \\\n    elif [[ ${MPI_KIND} == \"MPICH\" ]]; then \\\n        apt-get update -qq && apt-get install --no-install-recommends -y mpich && \\\n            echo \"mpirun -np 2\" > /mpirun_command; rm -rf /var/lib/apt/lists/*; \\\n    fi\n\n# Install mpi4py.\n# This requires SETUPTOOLS_USE_DISTUTILS=stdlib as with setuptools>=60.1.0 installing mpi4py broke\n# https://github.com/mpi4py/mpi4py/issues/157#issuecomment-1001022274\nRUN if [[ ${MPI_KIND} != \"None\" ]]; then \\\n        if [[ ${MPI_KIND} == \"ONECCL\" ]]; then \\\n            export I_MPI_ROOT=/usr/local/oneccl; \\\n            export MPICC=/usr/local/oneccl/bin/mpicc; \\\n        fi; \\\n        SETUPTOOLS_USE_DISTUTILS=stdlib pip install --no-cache-dir mpi4py; \\\n    fi\n\n# Install TensorFlow and Keras (releases).\n# Pin scipy!=1.4.0: https://github.com/scipy/scipy/issues/11237\n# Pin protobuf~=3.20 for tensorflow<2.6.5: https://github.com/tensorflow/tensorflow/issues/56077\nRUN if [[ ${TENSORFLOW_PACKAGE} != \"tf-nightly\" ]]; then \\\n        PROTOBUF_PACKAGE=\"\"; \\\n        if [[ ${TENSORFLOW_PACKAGE} == tensorflow*==1.15.* ]] || \\\n           [[ ${TENSORFLOW_PACKAGE} == tensorflow-cpu==2.[012345].* ]]; then \\\n            PROTOBUF_PACKAGE=\"protobuf~=3.20\"; \\\n        fi; \\\n        pip install --no-cache-dir ${TENSORFLOW_PACKAGE} ${PROTOBUF_PACKAGE}; \\\n        if [[ ${KERAS_PACKAGE} != \"None\" ]]; then \\\n            pip uninstall -y keras; \\\n            pip install --no-cache-dir ${KERAS_PACKAGE} \"scipy!=1.4.0\" \"pandas<1.1.0\"; \\\n        fi; \\\n        mkdir -p ~/.keras; \\\n        python -c \"import tensorflow as tf; tf.keras.datasets.mnist.load_data()\"; \\\n    fi\n\n# Pin h5py < 3 for tensorflow: https://github.com/tensorflow/tensorflow/issues/44467\nRUN pip install --no-cache-dir 'h5py<3.0' --force-reinstall\n\n# Install PyTorch (releases).\n# Pin Pillow<7.0 for torchvision < 0.5.0: https://github.com/pytorch/vision/issues/1718\n# Pin Pillow!=8.3.0 for torchvision: https://github.com/pytorch/vision/issues/4146\nRUN if [[ ${PYTORCH_PACKAGE} != \"torch-nightly\" ]]; then \\\n        pip install --no-cache-dir ${PYTORCH_PACKAGE} ${TORCHVISION_PACKAGE} -f https://download.pytorch.org/whl/torch_stable.html; \\\n        if [[ \"${TORCHVISION_PACKAGE/%+*/}\" == torchvision==0.[1234].* ]]; then \\\n            pip install --no-cache-dir \"Pillow<7.0\" --no-deps; \\\n        else \\\n            pip install --no-cache-dir \"Pillow!=8.3.0\" --no-deps; \\\n        fi; \\\n    fi\nRUN pip install --no-cache-dir ${PYTORCH_LIGHTNING_PACKAGE}\n\n\n# Install MXNet (releases).\nRUN if [[ ${MXNET_PACKAGE} != \"mxnet-nightly\" ]]; then \\\n        pip install --no-cache-dir ${MXNET_PACKAGE} ; \\\n    fi\n\n# Prefetch Spark MNIST dataset.\nRUN mkdir -p /work\nRUN mkdir -p /data && wget --progress=dot:mega https://horovod-datasets.s3.amazonaws.com/mnist.bz2 -O /data/mnist.bz2\n\n# Prefetch Spark Rossmann dataset.\nRUN mkdir -p /work\nRUN mkdir -p /data && wget --progress=dot:mega https://horovod-datasets.s3.amazonaws.com/rossmann.tgz -O - | tar -xzC /data\n\n# Prefetch PyTorch datasets.\nRUN wget --progress=dot:mega https://horovod-datasets.s3.amazonaws.com/pytorch_datasets.tgz -O - | tar -xzC /data\n\n### END OF CACHE ###\nCOPY . /horovod\n\n# Install nightly packages here so they do not get cached\n\n# Install TensorFlow and Keras (nightly).\n# Pin scipy!=1.4.0: https://github.com/scipy/scipy/issues/11237\nRUN if [[ ${TENSORFLOW_PACKAGE} == \"tf-nightly\" ]]; then \\\n        pip install --no-cache-dir ${TENSORFLOW_PACKAGE}; \\\n        if [[ ${KERAS_PACKAGE} != \"None\" ]]; then \\\n            pip uninstall -y keras-nightly; \\\n            pip install --no-cache-dir ${KERAS_PACKAGE} \"scipy!=1.4.0\" \"pandas<1.1.0\"; \\\n        fi; \\\n        mkdir -p ~/.keras; \\\n        python -c \"import tensorflow as tf; tf.keras.datasets.mnist.load_data()\"; \\\n    fi\n\n# Install PyTorch (nightly).\n# Pin Pillow!=8.3.0 for torchvision: https://github.com/pytorch/vision/issues/4146\nRUN if [[ ${PYTORCH_PACKAGE} == \"torch-nightly\" ]]; then \\\n        pip install --no-cache-dir --pre torch ${TORCHVISION_PACKAGE} -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html; \\\n        pip install --no-cache-dir \"Pillow!=8.3.0\" --no-deps; \\\n    fi\n\n# Install MXNet (nightly).\nRUN if [[ ${MXNET_PACKAGE} == \"mxnet-nightly\" ]]; then \\\n        pip install --no-cache-dir --pre mxnet -f https://dist.mxnet.io/python/all; \\\n    fi\n\n# Install Horovod.\nRUN if [[ ${MPI_KIND} == \"ONECCL\" ]]; then \\\n      if [ -z \"${LD_LIBRARY_PATH:-}\" ]; then \\\n          export LD_LIBRARY_PATH=\"\"; \\\n      fi; \\\n      if [ -z \"${PYTHONPATH:-}\" ]; then \\\n          export PYTHONPATH=\"\"; \\\n      fi; \\\n      . /usr/local/oneccl/env/setvars.sh; \\\n      export I_MPI_ROOT=/usr/local/oneccl; \\\n      echo \"horovod python setup.py sdist, mpicxx is $(which mpicxx)\"; \\\n    fi; \\\n    cd /horovod && \\\n    python setup.py sdist && \\\n    bash -c \"${HOROVOD_BUILD_FLAGS} HOROVOD_WITH_TENSORFLOW=1 HOROVOD_WITH_PYTORCH=1 HOROVOD_WITH_MXNET=1 pip install --no-cache-dir -v $(ls /horovod/dist/horovod-*.tar.gz)[spark,ray]\"\n\n# Show the effective python package version to easily spot version differences\nRUN pip freeze | sort\n\n# Hack for compatibility of MNIST example with TensorFlow 1.1.0.\nRUN if [[ ${TENSORFLOW_PACKAGE} == \"tensorflow==1.1.0\" ]]; then \\\n        sed -i \"s/from tensorflow import keras/from tensorflow.contrib import keras/\" /horovod/examples/tensorflow/tensorflow_mnist.py; \\\n    fi\n\n# Hack TensorFlow MNIST example to be smaller.\nRUN sed -i \"s/last_step=20000/last_step=100/\" /horovod/examples/tensorflow/tensorflow_mnist.py\n\n# Hack TensorFlow Eager MNIST example to be smaller.\nRUN sed -i \"s/dataset.take(20000/dataset.take(100/\" /horovod/examples/tensorflow/tensorflow_mnist_eager.py\n\n# Hack TensorFlow 2.0 example to be smaller.\nRUN sed -i \"s/dataset.take(10000/dataset.take(100/\" /horovod/examples/tensorflow2/tensorflow2_mnist.py\n\n# Hack TensorFlow 2.0 Data Service example to be smaller.\nRUN sed -i \"s/ epochs=24/ epochs=4/\" /horovod/examples/tensorflow2/tensorflow2_mnist_data_service_train_fn_*_side_dispatcher.py\n\n# Hack Keras MNIST advanced example to be smaller.\nRUN sed -i \"s/'--epochs', type=int, default=24,/'--epochs', type=int, default=9,/\" /horovod/examples/keras/keras_mnist_advanced.py\nRUN sed -i \"s/model.add(Conv2D(32, kernel_size=(3, 3),/model.add(Conv2D(1, kernel_size=(3, 3),/\" /horovod/examples/keras/keras_mnist_advanced.py\nRUN sed -i \"s/model.add(Conv2D(64, (3, 3), activation='relu'))//\" /horovod/examples/keras/keras_mnist_advanced.py\n\n# Hack TensorFlow 2.0 Keras MNIST advanced example to be smaller.\nRUN sed -i \"s/epochs=24/epochs=9/\" /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py\nRUN sed -i \"s/tf.keras.layers.Conv2D(32, \\\\[3, 3\\\\],/tf.keras.layers.Conv2D(1, [3, 3],/\" /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py\nRUN sed -i -E \"s/\\s+tf.keras.layers.Conv2D\\(64, \\\\[3, 3\\\\], activation='relu'\\),//\" /horovod/examples/tensorflow2/tensorflow2_keras_mnist.py\n\n# Hack PyTorch MNIST example to be smaller.\nRUN sed -i \"s/'--epochs', type=int, default=10,/'--epochs', type=int, default=2,/\" /horovod/examples/pytorch/pytorch_mnist.py\nRUN sed -i \"s/self.fc1 = nn.Linear(320, 50)/self.fc1 = nn.Linear(784, 50)/\" /horovod/examples/pytorch/pytorch_mnist.py\nRUN sed -i \"s/x = F.relu(F.max_pool2d(self.conv1(x), 2))//\" /horovod/examples/pytorch/pytorch_mnist.py\nRUN sed -i \"s/x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))//\" /horovod/examples/pytorch/pytorch_mnist.py\nRUN sed -i \"s/x = x.view(-1, 320)/x = x.view(-1, 784)/\" /horovod/examples/pytorch/pytorch_mnist.py\n\n# Hack Keras Spark Rossmann Run example to be smaller.\nRUN sed -i \"s/x = Dense(1000,/x = Dense(100,/g\" /horovod/examples/spark/keras/keras_spark_rossmann_run.py\nRUN sed -i \"s/x = Dense(500,/x = Dense(50,/g\" /horovod/examples/spark/keras/keras_spark_rossmann_run.py\n\n# Hack Keras Spark Rossmann Estimator example to be smaller.\nRUN sed -i \"s/x = Dense(1000,/x = Dense(100,/g\" /horovod/examples/spark/keras/keras_spark_rossmann_estimator.py\nRUN sed -i \"s/x = Dense(500,/x = Dense(50,/g\" /horovod/examples/spark/keras/keras_spark_rossmann_estimator.py\n"
}
{
  "startTime": 1674218840608,
  "endTime": 1674218841366,
  "originalSmells": [
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 36,
        "lineEnd": 36,
        "columnStart": 10,
        "columnEnd": 129
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 44,
        "lineEnd": 44,
        "columnStart": 7,
        "columnEnd": 34
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n# limitations under the License.\n\nARG HADOOP_VERSION=2.8.4\nARG HIVE_VERSION=2.3.3\nFROM apachehudi/hudi-hadoop_${HADOOP_VERSION}-hive_${HIVE_VERSION}\n\nENV ENABLE_INIT_DAEMON true\nENV INIT_DAEMON_BASE_URI http://identifier/init-daemon\nENV INIT_DAEMON_STEP spark_master_init\n\nARG SPARK_VERSION=2.4.4\nARG SPARK_HADOOP_VERSION=2.7\n\nENV SPARK_VERSION ${SPARK_VERSION}\nENV HADOOP_VERSION ${SPARK_HADOOP_VERSION}\n\nCOPY wait-for-step.sh /\nCOPY execute-step.sh /\nCOPY finish-step.sh /\n\nRUN echo \"Installing Spark-version (${SPARK_VERSION})\" \\\n      && wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n      && tar -xvzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n      && mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark \\\n      && rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n      && cd /\n\n# Install python3 to enable and use pyspark shell\nRUN apt-get update \\\n    && apt-get -yq --no-install-recommends install python3 \\\n    && ln -sf $(which python3) /usr/bin/python \\\n    && rm -rf /var/lib/apt/lists/*\n\n#Give permission to execute scripts\nRUN chmod +x /wait-for-step.sh && chmod +x /execute-step.sh && chmod +x /finish-step.sh\n\n# Fix the value of PYTHONHASHSEED\n# Note: this is needed when you use Python 3.3 or greater\nENV PYTHONHASHSEED 1\n\nENV SPARK_HOME /opt/spark\nENV SPARK_INSTALL ${SPARK_HOME}\nENV SPARK_CONF_DIR ${SPARK_HOME}/conf\nENV PATH $SPARK_INSTALL/bin:$PATH\n\nENV SPARK_DRIVER_PORT 5001\nENV SPARK_UI_PORT 5002\nENV SPARK_BLOCKMGR_PORT 5003\n\nEXPOSE $SPARK_DRIVER_PORT $SPARK_UI_PORT $SPARK_BLOCKMGR_PORT\n\n# Without this spark-shell fails - Download if it is not already there in $SPARK_INSTALL\nRUN wget -nc -q -O \"${SPARK_INSTALL}/jars/jersey-bundle-1.19.4.jar\" \"https://repo1.maven.org/maven2/com/sun/jersey/jersey-bundle/1.19.4/jersey-bundle-1.19.4.jar\"\n\n"
}
{
  "startTime": 1674255859841,
  "endTime": 1674255860622,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 10,
        "lineEnd": 10,
        "columnStart": 27,
        "columnEnd": 133
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM openjdk:11-jdk-slim\nLABEL maintainer=\"Luis Belloch <docker@luisbelloch.es>\"\n\nENV DEBIAN_FRONTEND=noninteractive\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends python3-software-properties python3-numpy curl && \\\n    rm -rf /var/lib/apt/lists/*\n\nARG SPARK_VERSION=3.1.2\nENV SPARK_HOME=/opt/spark\nRUN mkdir -p /opt/spark && curl -f -s https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.2.tgz | tar -xz -C \"${SPARK_HOME}\" --strip-components=1\nENV PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH\n\nRUN cp \"${SPARK_HOME}/conf/log4j.properties.template\" \"${SPARK_HOME}/conf/log4j.properties\" && \\\n    sed -ibak 's/rootCategory=INFO/rootCategory=ERROR/g' \"${SPARK_HOME}/conf/log4j.properties\"\n\nENV SPARK_NO_DAEMONIZE=true\nENV PYSPARK_PYTHON=python3\nEXPOSE 4040 7077 8080\n\nCMD [\"pyspark\"]\n\n"
}
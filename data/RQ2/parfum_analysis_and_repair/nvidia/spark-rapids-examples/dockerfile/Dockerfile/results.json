{
  "startTime": 1674250490217,
  "endTime": 1674250490999,
  "originalSmells": [
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 21,
        "lineEnd": 21,
        "columnStart": 22,
        "columnEnd": 92
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nFROM nvidia/cuda:11.0-devel-ubuntu18.04\nARG spark_uid=185\n\n# Install java dependencies\nRUN apt-get update && apt-get install -y --no-install-recommends openjdk-8-jdk openjdk-8-jre && rm -rf /var/lib/apt/lists/*;\nENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64\nENV PATH $PATH:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n\n# Before building the docker image, first build and make a Spark distribution following\n# the instructions in http://spark.apache.org/docs/latest/building-spark.html.\n# If this docker file is being used in the context of building your images from a Spark\n# distribution, the docker build command should be invoked from the top level directory\n# of the Spark distribution. E.g.:\n# docker build -t spark:latest -f kubernetes/dockerfiles/spark/Dockerfile .\n\nRUN set -ex && \\\n    ln -s /lib /lib64 && \\\n    mkdir -p /opt/spark && \\\n    mkdir -p /opt/spark/examples && \\\n    mkdir -p /opt/spark/work-dir && \\\n    touch /opt/spark/RELEASE && \\\n    rm /bin/sh && \\\n    ln -sv /bin/bash /bin/sh && \\\n    echo \"auth required pam_wheel.so use_uid\" >> /etc/pam.d/su && \\\n    chgrp root /etc/passwd && chmod ug+rw /etc/passwd\n\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update && apt-get install -y --no-install-recommends apt-utils \\\n && apt-get install -y --no-install-recommends python libgomp1 \\\n && rm -rf /var/lib/apt/lists/*\n\nCOPY jars /opt/spark/jars\nCOPY bin /opt/spark/bin\nCOPY sbin /opt/spark/sbin\nCOPY kubernetes/dockerfiles/spark/entrypoint.sh /opt/\nCOPY examples /opt/spark/examples\nCOPY kubernetes/tests /opt/spark/tests\nCOPY data /opt/spark/data\n\nENV SPARK_HOME /opt/spark\n\nWORKDIR /opt/spark/work-dir\nRUN chmod g+w /opt/spark/work-dir\n\nENV TINI_VERSION v0.18.0\nADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /sbin/tini\nRUN chmod +rx /sbin/tini\n\nENTRYPOINT [ \"/opt/entrypoint.sh\" ]\n\n# Specify the User that the actual main process will run as\nUSER ${spark_uid}\n\n"
}
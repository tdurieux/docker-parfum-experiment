{
  "startTime": 1674251726442,
  "endTime": 1674251727090,
  "originalSmells": [
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 30,
        "lineEnd": 30,
        "columnStart": 8,
        "columnEnd": 144
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nARG FROM_IMAGE_NAME=nvcr.io/nvidia/tensorflow:20.10-tf1-py3\n\nFROM ${FROM_IMAGE_NAME}\n\nUSER root\n\n# Spark dependencies\nENV APACHE_SPARK_VERSION 3.1.3\nENV HADOOP_VERSION 2.7\n\nRUN apt-get -y update && \\\n    apt-get install --no-install-recommends -y openjdk-8-jre-headless ca-certificates-java && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN cd /tmp && \\\n        wget -q https://archive.apache.org/dist/spark/spark-${APACHE_SPARK_VERSION}/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \\\n        echo \"34b2c6c5698c254181a020f9e0e5d29b3edeb3b99cd7f51105760b29681461e6e1bb0490c09d8b2005b5836f22d74251961939cad010ffc0d8de656d633b976e *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\" | sha512sum -c - && \\\n        tar xzf spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /usr/local --owner root --group root --no-same-owner && \\\n        rm spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\nRUN cd /usr/local && ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark\n\n# Spark config\nENV SPARK_HOME /usr/local/spark\nENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip\nENV SPARK_OPTS --driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info\n\nCOPY requirements* ./\nRUN pip install --no-cache-dir -r requirements.txt\nRUN pip install --no-cache-dir --no-deps -r requirements-no-deps.txt\n\nWORKDIR  /wd\nCOPY . .\n"
}
{
  "startTime": 1674255339398,
  "endTime": 1674255340340,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 21,
        "lineEnd": 21,
        "columnStart": 5,
        "columnEnd": 62
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM singularities/hadoop:2.8\nMAINTAINER Singularities\n\n# Version\nENV SPARK_VERSION=2.2.1\n\n# Set home\nENV SPARK_HOME=/usr/local/spark-$SPARK_VERSION\n\n# Install dependencies\nRUN apt-get update \\\n  && DEBIAN_FRONTEND=noninteractive apt-get install \\\n    -yq --no-install-recommends  \\\n      python python3 \\\n  && apt-get clean \\\n\t&& rm -rf /var/lib/apt/lists/*\n\n# Install Spark\nRUN mkdir -p \"${SPARK_HOME}\" \\\n  && export ARCHIVE=spark-$SPARK_VERSION-bin-without-hadoop.tgz \\\n  && export DOWNLOAD_PATH=apache/spark/spark-$SPARK_VERSION/$ARCHIVE \\\n  && curl -f -sSL https://mirrors.ocf.berkeley.edu/$DOWNLOAD_PATH | \\\n    tar -xz -C $SPARK_HOME --strip-components 1 \\\n  && rm -rf $ARCHIVE\nCOPY spark-env.sh $SPARK_HOME/conf/spark-env.sh\nENV PATH=$PATH:$SPARK_HOME/bin\n\n# Ports\nEXPOSE 6066 7077 8080 8081\n\n# Copy start script\nCOPY start-spark /opt/util/bin/start-spark\n\n# Fix environment for other users\nRUN echo \"export SPARK_HOME=$SPARK_HOME\" >> /etc/bash.bashrc \\\n  && echo 'export PATH=$PATH:$SPARK_HOME/bin'>> /etc/bash.bashrc\n\n# Add deprecated commands\nRUN echo '#!/usr/bin/env bash' > /usr/bin/master \\\n  && echo 'start-spark master' >> /usr/bin/master \\\n  && chmod +x /usr/bin/master \\\n  && echo '#!/usr/bin/env bash' > /usr/bin/worker \\\n  && echo 'start-spark worker $1' >> /usr/bin/worker \\\n  && chmod +x /usr/bin/worker\n"
}
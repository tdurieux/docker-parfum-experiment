{
  "startTime": 1674250250086,
  "endTime": 1674250251161,
  "originalSmells": [
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 2,
        "lineEnd": 2,
        "columnStart": 22,
        "columnEnd": 66
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 2,
        "lineEnd": 2,
        "columnStart": 22,
        "columnEnd": 66
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM cowait/task\n\nRUN apt-get update && apt-get install --no-install-recommends -y wget default-jre-headless && rm -rf /var/lib/apt/lists/*;\n\n# hopefully always true on debian stretch\nENV JAVA_HOME=/usr/lib/jvm/default-java\n\n# install spark\nENV SPARK_VERSION=3.0.1\nENV HADOOP_VERSION=3.2\nENV PY4J_VERSION=0.10.9\n\nRUN wget \\\n        -O spark.tgz https://www-eu.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n        --progress=dot:giga && \\\n    tar -xzf spark.tgz -C / && \\\n    rm spark.tgz\n\nENV SPARK_HOME=/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}\nENV PATH=$SPARK_HOME/python:$PATH:$SPARK_HOME/bin\nENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-${PY4J_VERSION}-src.zip:$PYTHONPATH\nCOPY spark-defaults.conf /spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}/conf\n"
}
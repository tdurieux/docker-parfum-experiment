{
  "startTime": 1674255982256,
  "endTime": 1674255983681,
  "originalSmells": [
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 26,
        "lineEnd": 26,
        "columnStart": 8,
        "columnEnd": 162
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "########## modify from Jupyter docker ##########\n# https://github.com/jupyter/docker-stacks/blob/master/pyspark-notebook/Dockerfile\n\n# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\nARG BASE_CONTAINER=jupyter/scipy-notebook\nFROM $BASE_CONTAINER\n\nLABEL maintainer=\"Jupyter Project <jupyter@googlegroups.com>\"\n\nUSER root\n\n# Spark dependencies\nENV APACHE_SPARK_VERSION 2.4.0\nENV HADOOP_VERSION 2.7\n\nRUN apt-get -y update && \\\n    apt-get install --no-install-recommends -y openjdk-8-jre-headless ca-certificates-java && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\n\n#RUN \\\n#    apt-get install -y postgresql\n\nRUN cd /tmp && \\\n        wget -q https://mirrors.ukfast.co.uk/sites/ftp.apache.org/spark/spark-${APACHE_SPARK_VERSION}/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \\\n        echo \"5F4184E0FE7E5C8AE67F5E6BC5DEEE881051CC712E9FF8AEDDF3529724C00E402C94BB75561DD9517A372F06C1FCB78DC7AE65DCBD4C156B3BA4D8E267EC2936 *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\" | sha512sum -c - && \\\n        tar xzf spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /usr/local --owner root --group root --no-same-owner && \\\n        rm spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\nRUN cd /usr/local && ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark\n\n\n\n# Install pip and library\n# How to install pip on Ubuntu\n# https://www.saltycrane.com/blog/2010/02/how-install-pip-ubuntu/\n\n#RUN \\\n#    apt-get install python-pip python-dev build-essential\n#RUN \\\n#    pip install --upgrade pip && \\\n#    pip install --upgrade virtualenv \\\n#    pip install psycopg2 numpy pandas scikit-learn scipy pyyaml\n\n\n# Mesos dependencies\n# Install from the Xenial Mesosphere repository since there does not (yet)\n# exist a Bionic repository and the dependencies seem to be compatible for now.\n#COPY mesos.key /tmp/\n#RUN apt-get -y update && \\\n#    apt-get install --no-install-recommends -y gnupg && \\\n#    apt-key add /tmp/mesos.key && \\\n#    echo \"deb http://repos.mesosphere.io/ubuntu xenial main\" > /etc/apt/sources.list.d/mesosphere.list && \\\n#    apt-get -y update && \\\n#    apt-get --no-install-recommends -y install mesos=1.2\\* && \\\n#    apt-get purge --auto-remove -y gnupg && \\\n#    apt-get clean && \\\n#    rm -rf /var/lib/apt/lists/*\n\n# Spark and Mesos config\nENV SPARK_HOME /usr/local/spark\nENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip\n#ENV MESOS_NATIVE_LIBRARY /usr/local/lib/libmesos.so\nENV SPARK_OPTS --driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info\n\nUSER $NB_UID\n\n# Install pyarrow\nRUN conda install --quiet -y 'pyarrow' && \\\n    conda clean -tipsy && \\\n    fix-permissions $CONDA_DIR && \\\n    fix-permissions /home/$NB_USER\n\n#############  run a test posgre query  ################\n\nENTRYPOINT [\"/bin/bash\", \"/setup_posgre_local.sh\"]\n"
}
{
  "startTime": 1674252678992,
  "endTime": 1674252680022,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 13,
        "lineEnd": 13,
        "columnStart": 2,
        "columnEnd": 120
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 13,
        "lineEnd": 13,
        "columnStart": 2,
        "columnEnd": 120
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 7,
        "lineEnd": 7,
        "columnStart": 2,
        "columnEnd": 67
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM openjdk:8-jre\n\nENV HADOOP_VERSION 2.9.2\nENV HADOOP_HOME /opt/hadoop\nENV HADOOP_CONF_DIR /opt/hadoop/etc/hadoop\n\nRUN apt-get update && \\\n  apt-get install --no-install-recommends -yf libpostgresql-jdbc-java procps openssh-server && \\\n  apt-get clean && \\\n  rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*\n\n# get + unpack hadoop to /opt/hadoop\nRUN mkdir ${HADOOP_HOME} && \\\n  curl -f https://apache.cs.utah.edu/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -o hadoop.tar.gz && \\\n  tar -xf hadoop.tar.gz -C ${HADOOP_HOME} --strip-components=1 && \\\n  rm hadoop.tar.gz\n\n# set up ssh for passwordless auth so the hive image can talk to the hadoop image\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\nCOPY ssh-config /root/.ssh/config\nRUN service ssh start\n# Do I need this?\n# RUN sed -i 's/^UsePAM yes/UsePAM no/g'\n\n# copy our local configs over\nCOPY conf/* ${HADOOP_CONF_DIR}/\nRUN sed -i \"s#^export JAVA_HOME=.*#export JAVA_HOME=${JAVA_HOME}#g\" ${HADOOP_CONF_DIR}/hadoop-env.sh\n\nENV PATH ${PATH}:${HADOOP_HOME}/bin\n\n# helper script that starts sshd and runs an hdfs command (requires the passwordless auth + ssh + hdfs working)\n# TODO: I don't think I need this if I don't have a cluster\nCOPY run_hdfs.sh /usr/local/bin\nRUN chmod +x /usr/local/bin/run_hdfs.sh\n\nCOPY run_yarn.sh /usr/local/bin\nRUN chmod +x /usr/local/bin/run_yarn.sh\n\n# our config sets hadoop.tmp.dir to /hadoop, and these values derive from that.\nRUN mkdir -p /hadoop/dfs/data /hadoop/dfs/name /hadoop/dfs/namesecondary && \\\n    hdfs namenode -format\nVOLUME /hadoop\n\n# 9000 - metadata\n# 50010 - data\n# 50020 - IPC\n# 22 - ssh\nEXPOSE 9000 50010 50020 22\n\n# no idea - yarn stuff?\nEXPOSE 8030 8031 8032 8040\n\nCMD [\"hdfs\"]\n"
}
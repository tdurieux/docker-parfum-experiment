{
  "startTime": 1674250037005,
  "endTime": 1674250038161,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 10,
        "lineEnd": 10,
        "columnStart": 4,
        "columnEnd": 91
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 10,
        "lineEnd": 10,
        "columnStart": 4,
        "columnEnd": 91
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 6,
        "lineEnd": 7,
        "columnStart": 4,
        "columnEnd": 6
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 6,
        "lineEnd": 7,
        "columnStart": 4,
        "columnEnd": 6
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM spirals/hadoop-benchmark:hadoop-benchmark\nMAINTAINER Bo ZHANG <bo.zhang@inria.fr>\n\n# prerequisite\nRUN DEBIAN_FRONTEND=noninteractive \\\n    apt-get update -yqq && \\\n    apt-get install --no-install-recommends -yqq \\\n\t\t\t\tbc && rm -rf /var/lib/apt/lists/*;\n\n# download and install Spark\nRUN curl -f https://www.eu.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-without-hadoop.tgz | tar -xz -C /usr/local/ && \\\n    ln -s /usr/local/spark-2.1.0-bin-without-hadoop /usr/local/spark\n\n# copy configuration files\nADD spark-conf/* /usr/local/spark/conf/\nADD hadoop-conf/* /usr/local/hadoop/etc/hadoop/\n\n# add spark start helpers\nADD *start.sh /\n\n# set basic envs\nENV SPARK_HOME /usr/local/spark\nENV PATH=$PATH:$SPARK_HOME/bin\n\nENTRYPOINT [ \"/entrypoint.sh\" ]\n\n# spark ports\nEXPOSE 7077 7337 8080 8081 17337\n# yarn port\nEXPOSE 8025"
}
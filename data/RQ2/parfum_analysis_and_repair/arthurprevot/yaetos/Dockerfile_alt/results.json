{
  "startTime": 1674246579085,
  "endTime": 1674246579633,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 15,
        "lineEnd": 15,
        "columnStart": 4,
        "columnEnd": 36
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 14,
        "lineEnd": 14,
        "columnStart": 22,
        "columnEnd": 44
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 16,
        "lineEnd": 16,
        "columnStart": 4,
        "columnEnd": 25
      }
    },
    {
      "rule": "aptGetUpdatePrecedesInstall",
      "position": {
        "lineStart": 16,
        "lineEnd": 16,
        "columnStart": 4,
        "columnEnd": 25
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 14,
        "lineEnd": 14,
        "columnStart": 22,
        "columnEnd": 44
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 16,
        "lineEnd": 16,
        "columnStart": 4,
        "columnEnd": 25
      }
    }
  ],
  "repairedSmells": [
    {
      "rule": "aptGetUpdatePrecedesInstall",
      "position": {
        "lineStart": 16,
        "lineEnd": 16,
        "columnStart": 4,
        "columnEnd": 49
      }
    }
  ],
  "repairedDockerfile": "FROM docker.io/bitnami/spark:3.1.2\n# docker.io/bitnami/spark:3.1.2 -> spark 3.1.2, python 3.6.15, scala 2.12.10, https://github.com/bitnami/bitnami-docker-spark, https://hub.docker.com/r/bitnami/spark\n# docker.io/bitnami/spark:2.4.5 -> spark 2.4.5, python 3.6.10, scala 2.11.12, https://github.com/bitnami/bitnami-docker-spark, https://hub.docker.com/r/bitnami/spark\n# FROM bde2020/spark-master:2.4.5-hadoop2.7  # https://github.com/big-data-europe/docker-spark, https://hub.docker.com/r/bde2020/spark-master. Failed installing python libs below. Seems to be missing basic compilers.\n# FROM jupyter/pyspark-notebook # spark 3.0.0. scala 2.12, Python 3.8.5. https://github.com/jupyter/docker-stacks, https://hub.docker.com/r/jupyter/pyspark-notebook. Was used successfully. Now leads to problems with sparks newly added jars that rely on scala 2.11.\n# FROM jupyter/pyspark-notebook:2fd856878b83  # to try later to get setup when spark got to 2.4.5 (https://github.com/jupyter/docker-stacks/commit/45d51e3b42b83748fe64997b0e39473aeee10377)\n# FROM arthurpr/pyspark_aws_etl:latest\n# FROM arthurpr/pyspark_aws_etl:oracle # also available to skip oracle install steps below.\n# TODO: build spark image from vanilla ubuntu (or other), see https://github.com/masroorhasan/docker-pyspark\nUSER root\n\n# Pip installs. Using local copy to tmp dir to allow checkpointing this step (no re-installs as long as requirements.txt doesn't change)\nCOPY yaetos/scripts/requirements_alt.txt /tmp/requirements.txt\nWORKDIR /tmp/\nRUN apt-get update && apt-get install --no-install-recommends -y git && rm -rf /var/lib/apt/lists/*;\nRUN pip3 install --no-cache-dir -r requirements.txt\nRUN apt install --no-install-recommends nodejs -y && rm -rf /var/lib/apt/lists/*;\n\nWORKDIR /mnt/yaetos\n\nENV YAETOS_FRAMEWORK_HOME /mnt/yaetos/\nENV PYTHONPATH $YAETOS_FRAMEWORK_HOME:$PYTHONPATH\n# ENV SPARK_HOME /usr/local/spark # already set in base docker image\nENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/build:$PYTHONPATH\n\nENV YAETOS_JOBS_HOME /mnt/yaetos_jobs/\nENV PYTHONPATH $YAETOS_JOBS_HOME:$PYTHONPATH\n\n# Expose ports for monitoring.\n# SparkContext web UI on 4040 -- only available for the duration of the application.\n# Spark masterâ€™s web UI on 8080.\n# Spark worker web UI on 8081.\n# Jupyter web UI on 8888.\nEXPOSE 4040 8080 8081 8888\n\n# CMD [\"/bin/bash\"] # commented so the command can be sent by docker run\n\n# Usage: docker run -it -p 4040:4040 -p 8080:8080 -p 8081:8081 -v ~/code/yaetos:/mnt/yaetos -v ~/.aws:/root/.aws -h spark <image_id>\n# or update launch_env.sh and execute it.\n"
}
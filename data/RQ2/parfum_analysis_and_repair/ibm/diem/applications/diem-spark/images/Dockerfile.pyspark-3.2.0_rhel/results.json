{
  "startTime": 1674251739000,
  "endTime": 1674251739653,
  "originalSmells": [
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 16,
        "lineEnd": 16,
        "columnStart": 4,
        "columnEnd": 60
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# v3.2.0_rhel_2022-01-4\nFROM quay.io/diem/base-spark:3.2.0_rhel as base_spark\n\nFROM registry.access.redhat.com/ubi8/python-39 as base\n\nWORKDIR /\n\nUSER root\n\nARG img=spark-3.2.0-bin-custom-spark\nARG install_dir=/tmp/artifacts\nARG V_ENV=/opt/venv\n\nCOPY --from=base_spark /tmp ${install_dir}/\n\nRUN mkdir -p ${install_dir}/tar\nRUN tar -xzf /tmp/artifacts/${img}.tgz -C ${install_dir}/tar && rm /tmp/artifacts/${img}.tgz\nRUN mv ${install_dir}/tar/${img}/ /opt/spark\nRUN mv /opt/spark/kubernetes/dockerfiles/spark/entrypoint.sh /opt/\nRUN mv /opt/spark/kubernetes/dockerfiles/spark/decom.sh /opt/\nRUN dnf clean all && [ ! -d /var/cache/dnf ] || rm -rf /var/cache/dnf\nRUN [ ! -d ${install_dir} ] || rm -rf ${install_dir}\n\nWORKDIR /opt/spark/work-dir\nRUN chmod g+w /opt/spark/work-dir && chmod a+x /opt/decom.sh\n\nFROM registry.access.redhat.com/ubi8/python-39\n\nUSER root\n\nCOPY --from=base /opt /opt\n\n#WORKDIR /\n\nRUN dnf -y upgrade && dnf install -y java-11-openjdk-headless.x86_64 && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    dnf clean all && [ ! -d /var/cache/dnf ] || rm -rf /var/cache/dnf\n\nADD https://github.com/krallin/tini/releases/download/v0.18.0/tini /bin/tini\nRUN chmod +x /bin/tini\n\nARG BUILD_DATE\nARG V_ENV=/opt/venv\n\nLABEL maintainer=guy_huinen@be.ibm.com \\\n    org.label-schema.description=\"Diem Nodepy Application\" \\\n    org.label-schema.name=\"nodepy\" \\\n    org.label-schema.version=$BUILD_VERSION \\\n    org.label-schema.build-date=$BUILD_DATE\n\n# Set 'spark' module defined environment variables\nENV \\\n    PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/spark/bin\" \\\n    SPARK_HOME=\"/opt/spark\" \\\n    SPARK_INSTALL=\"/opt/spark\" \\\n    LANG=en_US.UTF-8 \\\n    NODE_ENV=production \\\n    V_ENV=/opt/venv \\\n    PYTHONPATH=\"/usr/bin/python3:/opt/venv/lib/python3.9/site-packages\"\n# Custom scripts from 'spark' module\nENV PATH=\"${V_ENV}/bin:$PATH\"\n\nCOPY jars/* /opt/spark/jars/\nCOPY requirements-3.2.0.txt /opt/spark/\n\nRUN python3 -m pip install --upgrade pip\nRUN python3 -m venv ${V_ENV}\nRUN python3 -m pip install  --prefer-binary -r /opt/spark/requirements-3.2.0.txt\n\nENTRYPOINT [ \"/opt/entrypoint.sh\" ]\n\nWORKDIR /opt/spark/work-dir\nRUN chmod -R 775 ."
}
{
  "startTime": 1674252712287,
  "endTime": 1674252713295,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 23,
        "lineEnd": 23,
        "columnStart": 4,
        "columnEnd": 105
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 16,
        "lineEnd": 18,
        "columnStart": 22,
        "columnEnd": 14
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 16,
        "lineEnd": 18,
        "columnStart": 22,
        "columnEnd": 14
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "ARG HUB_ORG\nARG TAG\n\nARG BASE_CONTAINER=$HUB_ORG/kernel-r:$TAG\nFROM $BASE_CONTAINER\n\nARG SPARK_VERSION\n\nUSER root\n\nENV SPARK_VER $SPARK_VERSION\nENV SPARK_HOME /opt/spark\nENV KERNEL_LANGUAGE=R\nENV R_LIBS_USER $R_LIBS_USER:${R_HOME}/library:${SPARK_HOME}/R/lib\nENV PATH $PATH:$SPARK_HOME/bin\n\nRUN apt-get update && apt-get install --no-install-recommends -y \\\n    openjdk-8-jdk \\\n    libssl-dev && rm -rf /var/lib/apt/lists/*;\n\nENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64\n\n# Download and install Spark\nRUN curl -f -s https://archive.apache.org/dist/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-hadoop2.7.tgz | \\\n    tar -xz -C /opt && \\\n    ln -s ${SPARK_HOME}-${SPARK_VER}-bin-hadoop2.7 $SPARK_HOME\n\n# Download entrypoint.sh from matching tag\n# Use tini from Anaconda installation\nRUN cd /opt/ && \\\n    wget https://raw.githubusercontent.com/apache/spark/v${SPARK_VER}/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh && \\\n    chmod a+x /opt/entrypoint.sh && \\\n    sed -i 's/tini -s/tini -g/g' /opt/entrypoint.sh && \\\n    ln -sfn /opt/conda/bin/tini /usr/bin/tini\n\nWORKDIR $SPARK_HOME/work-dir\n# Ensure that work-dir is writable by everyone\nRUN chmod 0777 $SPARK_HOME/work-dir\n\nENTRYPOINT [ \"/opt/entrypoint.sh\" ]\n\nUSER jovyan\n"
}
{
  "startTime": 1674252288044,
  "endTime": 1674252288924,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 38,
        "lineEnd": 38,
        "columnStart": 4,
        "columnEnd": 155
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 6,
        "lineEnd": 7,
        "columnStart": 5,
        "columnEnd": 18
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 10,
        "lineEnd": 25,
        "columnStart": 4,
        "columnEnd": 8
      }
    },
    {
      "rule": "aptGetUpdatePrecedesInstall",
      "position": {
        "lineStart": 10,
        "lineEnd": 25,
        "columnStart": 4,
        "columnEnd": 8
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 6,
        "lineEnd": 7,
        "columnStart": 5,
        "columnEnd": 18
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 10,
        "lineEnd": 25,
        "columnStart": 4,
        "columnEnd": 8
      }
    }
  ],
  "repairedSmells": [
    {
      "rule": "aptGetUpdatePrecedesInstall",
      "position": {
        "lineStart": 10,
        "lineEnd": 25,
        "columnStart": 4,
        "columnEnd": 8
      }
    }
  ],
  "repairedDockerfile": "FROM marketplace.gcr.io/google/debian11\n\nENV C2D_RELEASE 3.3.0\n\n# Add Java 11\nRUN apt-get update \\\n  && apt-get install --no-install-recommends -y \\\n    openjdk-11-jdk && rm -rf /var/lib/apt/lists/*;\n\n# Add Dependencies for PySpark and Spark\nRUN apt-get install --no-install-recommends -y \\\n    ca-certificates \\\n    curl \\\n    software-properties-common \\\n    ssh \\\n    net-tools \\\n    python3 \\\n    python3-pip \\\n    python3-numpy \\\n    python3-matplotlib \\\n    python3-scipy \\\n    python3-pandas \\\n    python3-sympy \\\n    supervisor \\\n    vim \\\n    wget && rm -rf /var/lib/apt/lists/*;\n\nRUN update-alternatives --install \"/usr/bin/python\" \"python\" \"$(which python3)\" 1\n\n# Fix the value of PYTHONHASHSEED\n# Note: this is needed when you use Python 3.3 or greater\nENV SPARK_SHA256=7fbfb59b46fcbf0f7b3584c923d43fe6d0b405ab050d5ad43f659ead8521b4e4\nENV SPARK_VERSION=3.3.0\nENV SCALA_VERSION=2.13\nENV SPARK_HOME=/opt/spark\nENV PYTHONHASHSEED=1\n\n# Download and uncompress spark from the apache archive\nRUN curl -f -L -o apache-spark.tgz \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3-scala${SCALA_VERSION}.tgz\" \\\n      && echo \"${SPARK_SHA256}  apache-spark.tgz\" | sha256sum -c --strict --quiet \\\n      && mkdir -p /opt/spark \\\n      && tar -xf apache-spark.tgz -C /opt/spark --strip-components=1 \\\n      && rm -f apache-spark.tgz\n\nWORKDIR /opt/spark\n\nENV SPARK_MASTER_PORT=7077\nENV SPARK_MASTER_WEBUI_PORT=8080\nENV SPARK_LOG_DIR=/opt/spark/logs\nENV SPARK_MASTER_LOG=/opt/spark/logs/spark-master.out\nENV SPARK_WORKER_LOG=/opt/spark/logs/spark-worker.out\nENV SPARK_WORKER_WEBUI_PORT=8080\nENV SPARK_WORKER_PORT=7000\nENV SPARK_MASTER=\"spark://spark-master:7077\"\nENV SPARK_WORKLOAD=\"master\"\n\nEXPOSE 8080 7077 6066\n\nRUN mkdir -p $SPARK_LOG_DIR \\\n      && touch $SPARK_MASTER_LOG \\\n      && touch $SPARK_WORKER_LOG \\\n      && ln -sf /dev/stdout $SPARK_MASTER_LOG \\\n      && ln -sf /dev/stdout $SPARK_WORKER_LOG\n\nCOPY start-spark.sh /\nCOPY metrics.properties /opt/spark/conf/metrics.properties\nCOPY test_job.py /opt/spark-apps/test_job.py\nCOPY supervisor.conf /etc/supervisor/conf.d/supervisor.conf\n\nWORKDIR /opt/spark\n\nENTRYPOINT [ \"/start-spark.sh\" ]\n"
}
{
  "startTime": 1674217979671,
  "endTime": 1674217980498,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 23,
        "lineEnd": 23,
        "columnStart": 4,
        "columnEnd": 107
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 27,
        "lineEnd": 27,
        "columnStart": 4,
        "columnEnd": 111
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 32,
        "lineEnd": 32,
        "columnStart": 4,
        "columnEnd": 90
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 19,
        "lineEnd": 19,
        "columnStart": 4,
        "columnEnd": 75
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 19,
        "lineEnd": 19,
        "columnStart": 4,
        "columnEnd": 75
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# tini is not available in 18\nARG base_image=ubuntu:20.04\n\nFROM $base_image\n\n# TODO(pclay): support java 11\nARG java_version=8\nENV JAVA_HOME=/usr/lib/jvm/java-$java_version-openjdk-amd64\n\nARG hadoop_version=3.3.1\n# Spark picks this up for executor classpath:\n# https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh#L56\nENV HADOOP_HOME=/opt/pkb/hadoop\nENV HADOOP_OPTIONAL_TOOLS=hadoop-aws\n\nARG spark_version=3.1.2\nENV SPARK_HOME=/opt/pkb/spark\n\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y curl openjdk-$java_version-jre-headless python3 tini && rm -rf /var/lib/apt/lists/*;\n\n# Install hadoop to make s3 connector work\nRUN mkdir -p $HADOOP_HOME\nRUN curl -f -L https://downloads.apache.org/hadoop/common/hadoop-$hadoop_version/hadoop-$hadoop_version.tar.gz \\\n    | tar -C $HADOOP_HOME --strip-components=1 -xzf -\n\nRUN mkdir -p $SPARK_HOME\nRUN curl -f -L https://downloads.apache.org/spark/spark-$spark_version/spark-$spark_version-bin-without-hadoop.tgz \\\n    | tar -C $SPARK_HOME --strip-components=1 -xzf -\n\n# Install GCS connector\nRUN cd $HADOOP_HOME/share/hadoop/common/lib && \\\n    curl -f -O https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\n\n# Configure Spark\nRUN echo \"spark.master=k8s://https://kubernetes\" \\\n    > $SPARK_HOME/conf/spark-defaults.conf\n\n# Used by spark-submit for driver classpath\nRUN echo SPARK_DIST_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath` \\\n    > $SPARK_HOME/conf/spark-env.sh\n\n# Set up entrypoint\nRUN cp $SPARK_HOME/kubernetes/dockerfiles/spark/entrypoint.sh /entrypoint.sh\nRUN chmod 755 /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\n"
}
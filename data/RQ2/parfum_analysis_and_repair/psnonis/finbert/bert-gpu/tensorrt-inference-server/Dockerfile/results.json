{
  "startTime": 1674252251679,
  "endTime": 1674252253361,
  "originalSmells": [
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 95,
        "lineEnd": 95,
        "columnStart": 4,
        "columnEnd": 90
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 97,
        "lineEnd": 97,
        "columnStart": 4,
        "columnEnd": 38
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 77,
        "lineEnd": 77,
        "columnStart": 22,
        "columnEnd": 68
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 137,
        "lineEnd": 142,
        "columnStart": 4,
        "columnEnd": 19
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 296,
        "lineEnd": 297,
        "columnStart": 4,
        "columnEnd": 29
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Copyright (c) 2018-2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#\n# Multistage build.\n#\n\nARG BASE_IMAGE=nvcr.io/nvidia/tensorrtserver:19.06-py3\nARG PYTORCH_IMAGE=nvcr.io/nvidia/pytorch:19.06-py3\nARG TENSORFLOW_IMAGE=nvcr.io/nvidia/tensorflow:19.06-py3\n\n############################################################################\n## Caffe2 stage: Use PyTorch container to get Caffe2 backend\n############################################################################\nFROM ${PYTORCH_IMAGE} AS trtserver_caffe2\n\n# We cannot just pull libraries from the PyTorch container... we need\n# to:\n#   - copy over netdef_backend_c2 interface so it can build with other\n#     C2 sources\n\n# Copy netdef_backend_c2 into Caffe2 core so it builds into the\n# libcaffe2 library. We want netdef_backend_c2 to build against the\n# Caffe2 protobuf since it interfaces with that code.\nCOPY src/backends/caffe2/netdef_backend_c2.* \\\n     /opt/pytorch/pytorch/caffe2/core/\n\n# Build same as in pytorch container... except for the NO_DISTRIBUTED\n# line where we turn off features not needed for trtserver\n# This will build both the caffe2 libraries needed by the Caffe2 NetDef backend\n# and the LibTorch library needed by the PyTorch backend.\nWORKDIR /opt/pytorch\nRUN pip uninstall -y torch\nRUN cd pytorch && \\\n    TORCH_CUDA_ARCH_LIST=\"5.2 6.0 6.1 7.0 7.5+PTX\" \\\n      CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" \\\n      NCCL_INCLUDE_DIR=\"/usr/include/\" \\\n      NCCL_LIB_DIR=\"/usr/lib/\" \\\n      NO_DISTRIBUTED=1 NO_TEST=1 NO_MIOPEN=1 USE_MKLDNN=0 USE_OPENCV=OFF USE_LEVELDB=OFF \\\n      python setup.py install && python setup.py clean\n\n############################################################################\n## Onnx Runtime stage: Build Onnx Runtime on CUDA 10, CUDNN 7\n############################################################################\nFROM ${BASE_IMAGE} AS trtserver_onnx\n\n# Currently the prebuilt Onnx Runtime library is built on CUDA 9, thus it\n# needs to be built from source\n\n# Onnx Runtime release version\nARG ONNX_RUNTIME_VERSION=0.4.0\n\n# Get release version of Onnx Runtime\nWORKDIR /workspace\nRUN apt-get update && apt-get install -y --no-install-recommends git && rm -rf /var/lib/apt/lists/*;\nRUN git clone -b rel-${ONNX_RUNTIME_VERSION} --recursive https://github.com/Microsoft/onnxruntime\n\nENV PATH=\"/opt/cmake/bin:${PATH}\"\nARG SCRIPT_DIR=/workspace/onnxruntime/tools/ci_build/github/linux/docker/scripts\nRUN ${SCRIPT_DIR}/install_ubuntu.sh && ${SCRIPT_DIR}/install_deps.sh\n\n# Allow configure to pick up GDK and CuDNN where it expects it.\n# (Note: $CUDNN_VERSION is defined by NVidia's base image)\nRUN _CUDNN_VERSION=$(echo $CUDNN_VERSION | cut -d. -f1-2) && \\\n    mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/include && \\\n    ln -s /usr/include/cudnn.h /usr/local/cudnn-$_CUDNN_VERSION/cuda/include/cudnn.h && \\\n    mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64 && \\\n    ln -s /etc/alternatives/libcudnn_so /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64/libcudnn.so\n\n# Build and Install LLVM\nARG LLVM_VERSION=6.0.1\nRUN cd /tmp && \\\n    wget --no-verbose https://releases.llvm.org/$LLVM_VERSION/llvm-$LLVM_VERSION.src.tar.xz && \\\n    xz -d llvm-$LLVM_VERSION.src.tar.xz && \\\n    tar xvf llvm-$LLVM_VERSION.src.tar && \\\n    cd llvm-$LLVM_VERSION.src && \\\n    mkdir -p build && \\\n    cd build && \\\n    cmake .. -DCMAKE_BUILD_TYPE=Release && \\\n    cmake --build . -- -j$(nproc) && \\\n    cmake -DCMAKE_INSTALL_PREFIX=/usr/local/llvm-$LLVM_VERSION -DBUILD_TYPE=Release -P cmake_install.cmake && \\\n    cd /tmp && \\\n    rm -rf llvm* && rm llvm-$LLVM_VERSION.src.tar\n\nENV LD_LIBRARY_PATH /usr/local/openblas/lib:$LD_LIBRARY_PATH\n\n# Build files will be in /workspace/build\nARG COMMON_BUILD_ARGS=\"--skip_submodule_sync --parallel --build_shared_lib --use_openmp\"\nRUN mkdir -p /workspace/build\nRUN python3 /workspace/onnxruntime/tools/ci_build/build.py --build_dir /workspace/build \\\n            --config Release $COMMON_BUILD_ARGS \\\n            --use_cuda \\\n            --cuda_home /usr/local/cuda \\\n            --cudnn_home /usr/local/cudnn-$(echo $CUDNN_VERSION | cut -d. -f1-2)/cuda \\\n            --update \\\n            --build\n\n############################################################################\n## Build stage: Build inference server based on TensorFlow container\n############################################################################\nFROM ${TENSORFLOW_IMAGE} AS trtserver_build\n\nARG TRTIS_VERSION=1.3.0dev\nARG TRTIS_CONTAINER_VERSION=19.06dev\nARG PYVER=3.5\n\n# The TFServing release branch must match the TF release used by\n# TENSORFLOW_IMAGE\nARG TFS_BRANCH=r1.12\n\n# libcurl and libopencv are needed to build some testing\n# applications. libgoogle-glog0v5 is needed by caffe2 libraries.\n# libopencv is needed by image preprocessing custom backend\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n            automake \\\n            libgoogle-glog0v5 \\\n            libopencv-dev \\\n            libopencv-core-dev \\\n            libtool && rm -rf /var/lib/apt/lists/*;\n\n# Use the PYVER version of python\nRUN rm -f /usr/bin/python && \\\n    rm -f /usr/bin/python`echo $PYVER | cut -c1-1` && \\\n    ln -s /usr/bin/python$PYVER /usr/bin/python && \\\n    ln -s /usr/bin/python$PYVER /usr/bin/python`echo $PYVER | cut -c1-1`\n\n# Caffe2 library requirements...\nCOPY --from=trtserver_caffe2 \\\n     /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_detectron_ops_gpu.so \\\n     /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 \\\n     /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so \\\n     /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 \\\n     /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so \\\n     /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 \\\n     /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so \\\n     /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 \\\n     /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so \\\n     /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_avx2.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_core.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_def.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_gnu_thread.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_intel_lp64.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_rt.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_vml_def.so /opt/tensorrtserver/lib/\n\n# LibTorch library\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so \\\n      /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/. \\\n      /usr/local/include/.\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/include/. \\\n      /usr/local/include/.\n\n# Onnx Runtime library\nARG ONNX_RUNTIME_VERSION=0.4.0\nCOPY --from=trtserver_onnx /workspace/onnxruntime/include/onnxruntime /usr/local/include/\nCOPY --from=trtserver_onnx /workspace/build/Release/libonnxruntime.so.${ONNX_RUNTIME_VERSION} /opt/tensorrtserver/lib/\nRUN ln -s /opt/tensorrtserver/lib/libonnxruntime.so.${ONNX_RUNTIME_VERSION} /opt/tensorrtserver/lib/libonnxruntime.so\n\n# Copy entire repo into container even though some is not needed for\n# build itself... because we want to be able to copyright check on\n# files that aren't directly needed for build.\nWORKDIR /workspace\nRUN rm -fr *\nCOPY . .\n\n# Pull the TFS release that matches the version of TF being used.\nRUN git clone --single-branch -b ${TFS_BRANCH} https://github.com/tensorflow/serving.git\n\n# Modify the TF model loader to allow us to set the default GPU\nRUN sha1sum -c tools/patch/tensorflow/checksums && \\\n    patch -i tools/patch/tensorflow/cc/saved_model/loader.cc \\\n          /opt/tensorflow/tensorflow-source/tensorflow/cc/saved_model/loader.cc\n\n# TFS modifications. Use a checksum to detect if the TFS file has\n# changed... if it has need to verify our patch is still valid and\n# update the patch/checksum as necessary.\nRUN sha1sum -c tools/patch/tfs/checksums && \\\n    patch -i tools/patch/tfs/model_servers/server_core.cc \\\n          /workspace/serving/tensorflow_serving/model_servers/server_core.cc && \\\n    patch -i tools/patch/tfs/sources/storage_path/file_system_storage_path_source.cc \\\n          /workspace/serving/tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc && \\\n    patch -i tools/patch/tfs/sources/storage_path/file_system_storage_path_source.h \\\n          /workspace/serving/tensorflow_serving/sources/storage_path/file_system_storage_path_source.h && \\\n    patch -i tools/patch/tfs/sources/storage_path/file_system_storage_path_source.proto \\\n          /workspace/serving/tensorflow_serving/sources/storage_path/file_system_storage_path_source.proto && \\\n    patch -i tools/patch/tfs/util/retrier.cc \\\n          /workspace/serving/tensorflow_serving/util/retrier.cc && \\\n    patch -i tools/patch/tfs/util/BUILD \\\n          /workspace/serving/tensorflow_serving/util/BUILD && \\\n    patch -i tools/patch/tfs/workspace.bzl \\\n          /workspace/serving/tensorflow_serving/workspace.bzl\n\nENV TF_NEED_GCP 1\nENV TF_NEED_S3 1\n\n# Build the server and any testing artifacts\nRUN (cd /opt/tensorflow && ./nvbuild.sh --python$PYVER --configonly) && \\\n    mv .bazelrc .bazelrc.orig && \\\n    cat .bazelrc.orig /opt/tensorflow/tensorflow-source/.tf_configure.bazelrc > .bazelrc && \\\n    bazel build -c opt \\\n          src/servers/trtserver \\\n          src/custom/... \\\n          src/test/... && \\\n    (cd /opt/tensorrtserver && ln -s /workspace/qa qa) && \\\n    mkdir -p /opt/tensorrtserver/include && \\\n    cp bazel-out/k8-opt/genfiles/src/core/api.pb.h /opt/tensorrtserver/include/. && \\\n    cp bazel-out/k8-opt/genfiles/src/core/model_config.pb.h /opt/tensorrtserver/include/. && \\\n    cp bazel-out/k8-opt/genfiles/src/core/request_status.pb.h /opt/tensorrtserver/include/. && \\\n    cp bazel-out/k8-opt/genfiles/src/core/server_status.pb.h /opt/tensorrtserver/include/. && \\\n    mkdir -p /opt/tensorrtserver/bin && \\\n    cp bazel-bin/src/servers/trtserver /opt/tensorrtserver/bin/. && \\\n    cp bazel-bin/src/test/caffe2plan /opt/tensorrtserver/bin/. && \\\n    mkdir -p /opt/tensorrtserver/lib && \\\n    cp bazel-bin/src/core/libtrtserver.so /opt/tensorrtserver/lib/. && \\\n    mkdir -p /opt/tensorrtserver/custom && \\\n    cp bazel-bin/src/custom/addsub/libaddsub.so /opt/tensorrtserver/custom/. && \\\n    cp bazel-bin/src/custom/identity/libidentity.so /opt/tensorrtserver/custom/. && \\\n    cp bazel-bin/src/custom/image_preprocess/libimagepreprocess.so /opt/tensorrtserver/custom/. && \\\n    cp bazel-bin/src/custom/param/libparam.so /opt/tensorrtserver/custom/. && \\\n    cp bazel-bin/src/custom/sequence/libsequence.so /opt/tensorrtserver/custom/. && \\\n    bazel clean --expunge && \\\n    rm -rf /root/.cache/bazel && \\\n    rm -rf /tmp/*\n\nENV TENSORRT_SERVER_VERSION ${TRTIS_VERSION}\nENV NVIDIA_TENSORRT_SERVER_VERSION ${TRTIS_CONTAINER_VERSION}\n\nENV LD_LIBRARY_PATH /opt/tensorrtserver/lib:${LD_LIBRARY_PATH}\nENV PATH /opt/tensorrtserver/bin:${PATH}\nENV PYVER ${PYVER}\n\nCOPY nvidia_entrypoint.sh /opt/tensorrtserver\nENTRYPOINT [\"/opt/tensorrtserver/nvidia_entrypoint.sh\"]\n\n############################################################################\n##  Production stage: Create container with just inference server executable\n############################################################################\nFROM ${BASE_IMAGE}\n\nARG TRTIS_VERSION=1.3.0dev\nARG TRTIS_CONTAINER_VERSION=19.06dev\nARG PYVER=3.5\n\nENV TENSORRT_SERVER_VERSION ${TRTIS_VERSION}\nENV NVIDIA_TENSORRT_SERVER_VERSION ${TRTIS_CONTAINER_VERSION}\nLABEL com.nvidia.tensorrtserver.version=\"${TENSORRT_SERVER_VERSION}\"\n\nENV LD_LIBRARY_PATH /opt/tensorrtserver/lib:${LD_LIBRARY_PATH}\nENV PATH /opt/tensorrtserver/bin:${PATH}\nENV PYVER ${PYVER}\n\nENV TF_ADJUST_HUE_FUSED         1\nENV TF_ADJUST_SATURATION_FUSED  1\nENV TF_ENABLE_WINOGRAD_NONFUSED 1\nENV TF_AUTOTUNE_THRESHOLD       2\n\n# Create a user that can be used to run the tensorrt-server as\n# non-root. Make sure that this user to given ID 1000.\nENV TENSORRT_SERVER_USER=tensorrt-server\nRUN id -u $TENSORRT_SERVER_USER > /dev/null 2>&1 || \\\n    useradd $TENSORRT_SERVER_USER && \\\n    [ `id -u $TENSORRT_SERVER_USER` -eq 1000 ] && \\\n    [ `id -g $TENSORRT_SERVER_USER` -eq 1000 ]\n\n# libgoogle-glog0v5 is needed by caffe2 libraries.\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n            libgoogle-glog0v5 && rm -rf /var/lib/apt/lists/*;\n\nWORKDIR /opt/tensorrtserver\nRUN rm -fr /opt/tensorrtserver/*\nCOPY LICENSE .\nCOPY --from=trtserver_build /workspace/serving/LICENSE LICENSE.tfserving\nCOPY --from=trtserver_build /opt/tensorflow/tensorflow-source/LICENSE LICENSE.tensorflow\nCOPY --from=trtserver_caffe2 /opt/pytorch/pytorch/LICENSE LICENSE.pytorch\nCOPY --from=trtserver_build /opt/tensorrtserver/bin/trtserver bin/\nCOPY --from=trtserver_build /opt/tensorrtserver/lib lib\nCOPY --from=trtserver_build /opt/tensorrtserver/include include\n\nRUN chmod ugo-w+rx /opt/tensorrtserver/lib/*.so\n\n# Extra defensive wiring for CUDA Compat lib\nRUN ln -sf ${_CUDA_COMPAT_PATH}/lib.real ${_CUDA_COMPAT_PATH}/lib \\\n && echo ${_CUDA_COMPAT_PATH}/lib > /etc/ld.so.conf.d/00-cuda-compat.conf \\\n && ldconfig \\\n && rm -f ${_CUDA_COMPAT_PATH}/lib\n\nCOPY nvidia_entrypoint.sh /opt/tensorrtserver\nENTRYPOINT [\"/opt/tensorrtserver/nvidia_entrypoint.sh\"]\n\nARG NVIDIA_BUILD_ID\nENV NVIDIA_BUILD_ID ${NVIDIA_BUILD_ID:-<unknown>}\nLABEL com.nvidia.build.id=\"${NVIDIA_BUILD_ID}\"\nARG NVIDIA_BUILD_REF\nLABEL com.nvidia.build.ref=\"${NVIDIA_BUILD_REF}\"\n"
}
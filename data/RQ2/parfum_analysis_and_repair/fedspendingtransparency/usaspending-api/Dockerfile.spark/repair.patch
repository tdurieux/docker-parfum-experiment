diff --git a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/fedspendingtransparency/usaspending-api/Dockerfile.spark b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/fedspendingtransparency/usaspending-api/Dockerfile.spark/repaired.Dockerfile
index 0ed6785..a48c6fa 100644
--- a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/fedspendingtransparency/usaspending-api/Dockerfile.spark
+++ b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/fedspendingtransparency/usaspending-api/Dockerfile.spark/repaired.Dockerfile
@@ -11,15 +11,15 @@ ARG PROJECT_LOG_DIR=/logs
 
 RUN yum -y update && yum clean all
 # sqlite-devel added as prerequisite for coverage python lib, used by pytest-cov plugin
-RUN yum -y install wget gcc openssl-devel bzip2-devel libffi libffi-devel zlib-devel sqlite-devel
+RUN yum -y install wget gcc openssl-devel bzip2-devel libffi libffi-devel zlib-devel sqlite-devel && rm -rf /var/cache/yum
 RUN yum -y groupinstall "Development Tools"
 
 # Building Python 3.x
 WORKDIR /usr/src
 RUN wget --quiet https://www.python.org/ftp/python/${PYTHON_VERSION}/Python-${PYTHON_VERSION}.tgz \
-    && tar xzf Python-${PYTHON_VERSION}.tgz
+    && tar xzf Python-${PYTHON_VERSION}.tgz && rm Python-${PYTHON_VERSION}.tgz
 WORKDIR /usr/src/Python-${PYTHON_VERSION}
-RUN ./configure --enable-optimizations \
+RUN ./configure --build="$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)" --enable-optimizations \
     && make altinstall \
     && ln -sf /usr/local/bin/python`echo ${PYTHON_VERSION} | awk -F. '{short_version=$1 FS $2; print short_version}'` /usr/bin/python3 \
     && echo "Installed $(python3 --version)"
@@ -27,7 +27,7 @@ RUN ./configure --enable-optimizations \
 ENV PYTHONUNBUFFERED=1
 
 # Install Java 1.8.x
-RUN yum -y install java-1.8.0-openjdk
+RUN yum -y install java-1.8.0-openjdk && rm -rf /var/cache/yum
 ENV JAVA_HOME=/usr/lib/jvm/jre
 
 # Install Hadoop and Spark
@@ -38,7 +38,7 @@ RUN wget --quiet https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_V
     && wget --quiet https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz \
     && tar xzf spark-${SPARK_VERSION}-bin-without-hadoop.tgz \
     && ln -sfn /usr/local/spark-${SPARK_VERSION}-bin-without-hadoop /usr/local/spark \
-    && echo "Installed $(/usr/local/hadoop/bin/hadoop version)"
+    && echo "Installed $(/usr/local/hadoop/bin/hadoop version)" && rm hadoop-${HADOOP_VERSION}.tar.gz
 ENV HADOOP_HOME=/usr/local/hadoop
 ENV SPARK_HOME=/usr/local/spark
 # Cannot set ENV var = command-result, [i.e. doing: ENV SPARK_DIST_CLASSPATH=$(${HADOOP_HOME}/bin/hadoop classpath)], so interpolating the hadoop classpath the long way
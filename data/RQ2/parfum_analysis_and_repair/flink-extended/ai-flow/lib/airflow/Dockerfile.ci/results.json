{
  "startTime": 1674249743079,
  "endTime": 1674249744840,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 172,
        "lineEnd": 172,
        "columnStart": 4,
        "columnEnd": 96
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 206,
        "lineEnd": 206,
        "columnStart": 4,
        "columnEnd": 101
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 211,
        "lineEnd": 211,
        "columnStart": 7,
        "columnEnd": 123
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 215,
        "lineEnd": 215,
        "columnStart": 7,
        "columnEnd": 120
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 219,
        "lineEnd": 219,
        "columnStart": 7,
        "columnEnd": 114
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 371,
        "lineEnd": 371,
        "columnStart": 7,
        "columnEnd": 36
      }
    },
    {
      "rule": "yarnCacheCleanAfterInstall",
      "position": {
        "lineStart": 299,
        "lineEnd": 299,
        "columnStart": 4,
        "columnEnd": 63
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 273,
        "lineEnd": 273,
        "columnStart": 4,
        "columnEnd": 47
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 280,
        "lineEnd": 282,
        "columnStart": 8,
        "columnEnd": 62
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 323,
        "lineEnd": 323,
        "columnStart": 12,
        "columnEnd": 85
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 325,
        "lineEnd": 325,
        "columnStart": 12,
        "columnEnd": 94
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 335,
        "lineEnd": 335,
        "columnStart": 12,
        "columnEnd": 71
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 378,
        "lineEnd": 378,
        "columnStart": 8,
        "columnEnd": 46
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 207,
        "lineEnd": 207,
        "columnStart": 7,
        "columnEnd": 37
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 212,
        "lineEnd": 212,
        "columnStart": 7,
        "columnEnd": 77
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 216,
        "lineEnd": 216,
        "columnStart": 7,
        "columnEnd": 75
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 220,
        "lineEnd": 220,
        "columnStart": 7,
        "columnEnd": 71
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# WARNING: THIS DOCKERFILE IS NOT INTENDED FOR PRODUCTION USE OR DEPLOYMENT.\n#\nARG PYTHON_BASE_IMAGE=\"python:3.6-slim-buster\"\nFROM ${PYTHON_BASE_IMAGE} as main\n\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\n\nARG PYTHON_BASE_IMAGE=\"python:3.6-slim-buster\"\nENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE}\n\nARG AIRFLOW_VERSION=\"2.0.0.dev0\"\nENV AIRFLOW_VERSION=$AIRFLOW_VERSION\n\nARG PYTHON_MAJOR_MINOR_VERSION=\"3.6\"\nENV PYTHON_MAJOR_MINOR_VERSION=${PYTHON_MAJOR_MINOR_VERSION}\n\nARG PIP_VERSION=20.2.4\nENV PIP_VERSION=${PIP_VERSION}\n\n# Print versions\nRUN echo \"Base image: ${PYTHON_BASE_IMAGE}\"\nRUN echo \"Airflow version: ${AIRFLOW_VERSION}\"\n\n# Make sure noninteractive debian install is used and language variables set\nENV DEBIAN_FRONTEND=noninteractive LANGUAGE=C.UTF-8 LANG=C.UTF-8 LC_ALL=C.UTF-8 \\\n    LC_CTYPE=C.UTF-8 LC_MESSAGES=C.UTF-8\n\n# By increasing this number we can do force build of all dependencies\nARG DEPENDENCIES_EPOCH_NUMBER=\"5\"\n# Increase the value below to force reinstalling of all dependencies\nENV DEPENDENCIES_EPOCH_NUMBER=${DEPENDENCIES_EPOCH_NUMBER}\n\n# Install curl and gnupg2 - needed to download nodejs in the next step\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n           curl \\\n           gnupg2 \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nARG ADDITIONAL_DEV_APT_DEPS=\"\"\nENV ADDITIONAL_DEV_APT_DEPS=${ADDITIONAL_DEV_APT_DEPS}\n\nARG DEV_APT_COMMAND=\"\\\n    curl --fail --location https://deb.nodesource.com/setup_10.x | bash - \\\n    && curl https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add - > /dev/null \\\n    && echo 'deb https://dl.yarnpkg.com/debian/ stable main' > /etc/apt/sources.list.d/yarn.list\"\nENV DEV_APT_COMMAND=${DEV_APT_COMMAND}\n\nARG ADDITIONAL_DEV_APT_COMMAND=\"\"\nENV ADDITIONAL_DEV_APT_COMMAND=${ADDITIONAL_DEV_APT_COMMAND}\n\nARG ADDITIONAL_DEV_ENV_VARS=\"\"\n\n# Install basic and additional apt dependencies\nRUN mkdir -pv /usr/share/man/man1 \\\n    && mkdir -pv /usr/share/man/man7 \\\n    && export ${ADDITIONAL_DEV_ENV_VARS?} \\\n    && bash -o pipefail -e -u -x -c \"${DEV_APT_COMMAND}\" \\\n    && bash -o pipefail -e -u -x -c \"${ADDITIONAL_DEV_APT_COMMAND}\" \\\n    && apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n           apt-utils \\\n           build-essential \\\n           dirmngr \\\n           dumb-init \\\n           freetds-bin \\\n           freetds-dev \\\n           git \\\n           graphviz \\\n           gosu \\\n           libffi-dev \\\n           libkrb5-dev \\\n           libpq-dev \\\n           libsasl2-2 \\\n           libsasl2-dev \\\n           libsasl2-modules \\\n           libssl-dev \\\n           libenchant-dev \\\n           locales  \\\n           netcat \\\n           nodejs \\\n           rsync \\\n           sasl2-bin \\\n           sudo \\\n           unixodbc \\\n           unixodbc-dev \\\n           yarn \\\n           ${ADDITIONAL_DEV_APT_DEPS} \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nCOPY scripts/docker /scripts/docker\n# fix permission issue in Azure DevOps when running the script\nRUN chmod a+x /scripts/docker/install_mysql.sh\nRUN ./scripts/docker/install_mysql.sh dev\n\nRUN adduser airflow \\\n    && echo \"airflow ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/airflow \\\n    && chmod 0440 /etc/sudoers.d/airflow\n\n# The latest buster images do not have libpython 2.7 installed and it is needed\n# To run virtualenv tests with python 2\nARG RUNTIME_APT_DEPS=\"\\\n      gnupg \\\n      libgcc-8-dev \\\n      apt-transport-https \\\n      bash-completion \\\n      ca-certificates \\\n      software-properties-common \\\n      krb5-user \\\n      ldap-utils \\\n      less \\\n      libpython2.7-stdlib \\\n      lsb-release \\\n      net-tools \\\n      openssh-client \\\n      openssh-server \\\n      postgresql-client \\\n      sqlite3 \\\n      tmux \\\n      unzip \\\n      vim \\\n      xxd\"\nENV RUNTIME_APT_DEP=${RUNTIME_APT_DEPS}\n\nARG ADDITIONAL_RUNTIME_APT_DEPS=\"\"\nENV ADDITIONAL_RUNTIME_APT_DEPS=${ADDITIONAL_RUNTIME_APT_DEPS}\n\nARG RUNTIME_APT_COMMAND=\"\"\nENV RUNTIME_APT_COMMAND=${RUNTIME_APT_COMMAND}\n\nARG ADDITIONAL_RUNTIME_APT_COMMAND=\"\"\nENV ADDITIONAL_RUNTIME_APT_COMMAND=${ADDITIONAL_RUNTIME_APT_COMMAND}\n\nARG ADDITIONAL_RUNTIME_ENV_VARS=\"\"\n\n# Note missing man directories on debian-buster\n# https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=863199\nRUN mkdir -pv /usr/share/man/man1 \\\n    && mkdir -pv /usr/share/man/man7 \\\n    && export ${ADDITIONAL_RUNTIME_ENV_VARS?} \\\n    && bash -o pipefail -e -u -x -c \"${RUNTIME_APT_COMMAND}\" \\\n    && bash -o pipefail -e -u -x -c \"${ADDITIONAL_RUNTIME_APT_COMMAND}\" \\\n    && apt-get update \\\n    && apt-get install --no-install-recommends -y \\\n      ${RUNTIME_APT_DEPS} \\\n      ${ADDITIONAL_RUNTIME_APT_DEPS} \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nARG DOCKER_CLI_VERSION=19.03.9\nENV DOCKER_CLI_VERSION=${DOCKER_CLI_VERSION}\n\nRUN curl -f https://download.docker.com/linux/static/stable/x86_64/docker-${DOCKER_CLI_VERSION}.tgz \\\n    | tar -C /usr/bin --strip-components=1 -xvzf - docker/docker\n\n# Setup PIP\n# By default PIP install run without cache to make image smaller\nARG PIP_NO_CACHE_DIR=\"true\"\nENV PIP_NO_CACHE_DIR=${PIP_NO_CACHE_DIR}\nRUN echo \"Pip no cache dir: ${PIP_NO_CACHE_DIR}\"\n\nARG HOME=/root\nENV HOME=${HOME}\n\nARG AIRFLOW_HOME=/root/airflow\nENV AIRFLOW_HOME=${AIRFLOW_HOME}\n\nARG AIRFLOW_SOURCES=/opt/airflow\nENV AIRFLOW_SOURCES=${AIRFLOW_SOURCES}\n\nWORKDIR ${AIRFLOW_SOURCES}\n\nRUN mkdir -pv ${AIRFLOW_HOME} && \\\n    mkdir -pv ${AIRFLOW_HOME}/dags && \\\n    mkdir -pv ${AIRFLOW_HOME}/logs\n\n# Increase the value here to force reinstalling Apache Airflow pip dependencies\nARG PIP_DEPENDENCIES_EPOCH_NUMBER=\"3\"\nENV PIP_DEPENDENCIES_EPOCH_NUMBER=${PIP_DEPENDENCIES_EPOCH_NUMBER}\n\n# Install BATS and its dependencies for \"in container\" tests\nARG BATS_VERSION=\"0.4.0\"\nARG BATS_SUPPORT_VERSION=\"0.3.0\"\nARG BATS_ASSERT_VERSION=\"2.0.0\"\nARG BATS_FILE_VERSION=\"0.2.0\"\n\nRUN curl -f -sSL https://github.com/bats-core/bats-core/archive/v${BATS_VERSION}.tar.gz -o /tmp/bats.tgz \\\n    && tar -zxf /tmp/bats.tgz -C /tmp \\\n    && /bin/bash /tmp/bats-core-${BATS_VERSION}/install.sh /opt/bats && rm -rf && rm /tmp/bats.tgz\n\nRUN mkdir -p /opt/bats/lib/bats-support \\\n    && curl -f -sSL https://github.com/bats-core/bats-support/archive/v${BATS_SUPPORT_VERSION}.tar.gz -o /tmp/bats-support.tgz \\\n    && tar -zxf /tmp/bats-support.tgz -C /opt/bats/lib/bats-support --strip 1 && rm -rf /tmp/* && rm /tmp/bats-support.tgz\n\nRUN mkdir -p /opt/bats/lib/bats-assert \\\n    && curl -f -sSL https://github.com/bats-core/bats-assert/archive/v${BATS_ASSERT_VERSION}.tar.gz -o /tmp/bats-assert.tgz \\\n    && tar -zxf /tmp/bats-assert.tgz -C /opt/bats/lib/bats-assert --strip 1 && rm -rf /tmp/* && rm /tmp/bats-assert.tgz\n\nRUN mkdir -p /opt/bats/lib/bats-file \\\n    && curl -f -sSL https://github.com/bats-core/bats-file/archive/v${BATS_FILE_VERSION}.tar.gz -o /tmp/bats-file.tgz \\\n    && tar -zxf /tmp/bats-file.tgz -C /opt/bats/lib/bats-file --strip 1 && rm -rf /tmp/* && rm /tmp/bats-file.tgz\n\nRUN echo \"export PATH=/opt/bats/bin:${PATH}\" >> /root/.bashrc\n\n# Additional scripts for managing BATS addons\nCOPY scripts/docker/load.bash /opt/bats/lib/\nRUN chmod a+x /opt/bats/lib/load.bash\n\n\n# Optimizing installation of Cassandra driver\n# Speeds up building the image - cassandra driver without CYTHON saves around 10 minutes\nARG CASS_DRIVER_NO_CYTHON=\"1\"\n# Build cassandra driver on multiple CPUs\nARG CASS_DRIVER_BUILD_CONCURRENCY=\"8\"\n\nENV CASS_DRIVER_BUILD_CONCURRENCY=${CASS_DRIVER_BUILD_CONCURRENCY}\nENV CASS_DRIVER_NO_CYTHON=${CASS_DRIVER_NO_CYTHON}\n\nARG AIRFLOW_REPO=apache/airflow\nENV AIRFLOW_REPO=${AIRFLOW_REPO}\n\nARG AIRFLOW_BRANCH=master\nENV AIRFLOW_BRANCH=${AIRFLOW_BRANCH}\n\n# Airflow Extras installed\nARG AIRFLOW_EXTRAS=\"all\"\nARG ADDITIONAL_AIRFLOW_EXTRAS=\"\"\nENV AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS}${ADDITIONAL_AIRFLOW_EXTRAS:+,}${ADDITIONAL_AIRFLOW_EXTRAS}\n\nRUN echo \"Installing with extras: ${AIRFLOW_EXTRAS}.\"\n\nARG AIRFLOW_CONSTRAINTS_REFERENCE=\"constraints-master\"\nARG AIRFLOW_CONSTRAINTS_LOCATION=\"https://raw.githubusercontent.com/apache/airflow/${AIRFLOW_CONSTRAINTS_REFERENCE}/constraints-${PYTHON_MAJOR_MINOR_VERSION}.txt\"\nENV AIRFLOW_CONSTRAINTS_LOCATION=${AIRFLOW_CONSTRAINTS_LOCATION}\n\n# By changing the CI build epoch we can force reinstalling Airflow from the current master\n# It can also be overwritten manually by setting the AIRFLOW_CI_BUILD_EPOCH environment variable.\nARG AIRFLOW_CI_BUILD_EPOCH=\"3\"\nENV AIRFLOW_CI_BUILD_EPOCH=${AIRFLOW_CI_BUILD_EPOCH}\n\nARG AIRFLOW_PRE_CACHED_PIP_PACKAGES=\"true\"\nENV AIRFLOW_PRE_CACHED_PIP_PACKAGES=${AIRFLOW_PRE_CACHED_PIP_PACKAGES}\n\n# By default in the image, we are installing all providers when installing from sources\nARG INSTALL_PROVIDERS_FROM_SOURCES=\"true\"\nENV INSTALL_PROVIDERS_FROM_SOURCES=${INSTALL_PROVIDERS_FROM_SOURCES}\n\nARG INSTALL_FROM_DOCKER_CONTEXT_FILES=\"\"\nENV INSTALL_FROM_DOCKER_CONTEXT_FILES=${INSTALL_FROM_DOCKER_CONTEXT_FILES}\n\nARG INSTALL_FROM_PYPI=\"true\"\nENV INSTALL_FROM_PYPI=${INSTALL_FROM_PYPI}\n\nRUN pip install --no-cache-dir --upgrade \"pip==${PIP_VERSION}\"\n\n# In case of CI builds we want to pre-install master version of airflow dependencies so that\n# We do not have to always reinstall it from the scratch.\n# This can be reinstalled from latest master by increasing PIP_DEPENDENCIES_EPOCH_NUMBER.\n# And is automatically reinstalled from the scratch every time patch release of python gets released\nRUN if [[ ${AIRFLOW_PRE_CACHED_PIP_PACKAGES} == \"true\" ]]; then \\\n        pip install --no-cache-dir \\\n            \"https://github.com/${AIRFLOW_REPO}/archive/${AIRFLOW_BRANCH}.tar.gz#egg=apache-airflow[${AIRFLOW_EXTRAS}]\" \\\n                --constraint \"${AIRFLOW_CONSTRAINTS_LOCATION}\" \\\n                && pip uninstall --yes apache-airflow; \\\n    fi\n\n# Generate random hex dump file so that we can determine whether it's faster to rebuild the image\n# using current cache (when our dump is the same as the remote onb) or better to pull\n# the new image (when it is different)\nRUN head -c 30 /dev/urandom | xxd -ps >/build-cache-hash\n\n# Link dumb-init for backwards compatibility (so that older images also work)\nRUN ln -sf /usr/bin/dumb-init /usr/local/bin/dumb-init\n\n# Install NPM dependencies here. The NPM dependencies don't change that often and we already have pip\n# installed dependencies in case of CI optimised build, so it is ok to install NPM deps here\n# Rather than after setup.py is added.\nCOPY airflow/www/yarn.lock airflow/www/package.json ${AIRFLOW_SOURCES}/airflow/www/\n\nRUN yarn --cwd airflow/www install --frozen-lockfile --no-cache && yarn cache clean;\n\n# Note! We are copying everything with airflow:airflow user:group even if we use root to run the scripts\n# This is fine as root user will be able to use those dirs anyway.\n\n# Airflow sources change frequently but dependency configuration won't change that often\n# We copy setup.py and other files needed to perform setup of dependencies\n# So in case setup.py changes we can install latest dependencies required.\nCOPY setup.py ${AIRFLOW_SOURCES}/setup.py\nCOPY setup.cfg ${AIRFLOW_SOURCES}/setup.cfg\n\nCOPY airflow/__init__.py ${AIRFLOW_SOURCES}/airflow/__init__.py\n\nARG UPGRADE_TO_LATEST_CONSTRAINTS=\"false\"\nENV UPGRADE_TO_LATEST_CONSTRAINTS=${UPGRADE_TO_LATEST_CONSTRAINTS}\n\n# The goal of this line is to install the dependencies from the most current setup.py from sources\n# This will be usually incremental small set of packages in CI optimized build, so it will be very fast\n# In non-CI optimized build this will install all dependencies before installing sources.\n# Usually we will install versions based on the dependencies in setup.py and upgraded only if needed.\n# But in cron job we will install latest versions matching setup.py to see if there is no breaking change\n# and push the constraints if everything is successful\nRUN if [[ ${INSTALL_FROM_PYPI} == \"true\" ]]; then \\\n        if [[ \"${UPGRADE_TO_LATEST_CONSTRAINTS}\" != \"false\" ]]; then \\\n            pip install --no-cache-dir -e \".[${AIRFLOW_EXTRAS}]\" --upgrade --upgrade-strategy eager; \\\n        else \\\n            pip install --no-cache-dir -e \".[${AIRFLOW_EXTRAS}]\" --upgrade --upgrade-strategy only-if-needed; \\\n        fi; else \\\n            pip install --no-cache-dir -e \".[${AIRFLOW_EXTRAS}]\" --upgrade --upgrade-strategy only-if-needed; \\\n        fi \\\n    fi\n\n# If wheel files are found in /docker-context-files during installation\n# they are also installed additionally to whatever is installed from Airflow.\nCOPY docker-context-files/ /docker-context-files/\n\nRUN if [[ ${INSTALL_FROM_DOCKER_CONTEXT_FILES} != \"true\" ]]; then \\\n        if ls /docker-context-files/*.{whl,tar.gz} 1> /dev/null 2>&1; then \\\n            pip install --no-cache-dir --no-deps /docker-context-files/*.{whl,tar.gz}; \\\n        fi; \\\n    fi\n\n# Copy all the www/ files we need to compile assets. Done as two separate COPY\n# commands so as otherwise it copies the _contents_ of static/ in to www/\nCOPY airflow/www/webpack.config.js ${AIRFLOW_SOURCES}/airflow/www/\nCOPY airflow/www/static ${AIRFLOW_SOURCES}/airflow/www/static/\n\n# Package JS/css for production\nRUN yarn --cwd airflow/www run prod\n\nCOPY scripts/in_container/entrypoint_ci.sh /entrypoint\nRUN chmod a+x /entrypoint\n\n# We can copy everything here. The Context is filtered by dockerignore. This makes sure we are not\n# copying over stuff that is accidentally generated or that we do not need (such as egg-info)\n# if you want to add something that is missing and you expect to see it in the image you can\n# add it with ! in .dockerignore next to the airflow, test etc. directories there\nCOPY . ${AIRFLOW_SOURCES}/\n\n# Install autocomplete for airflow\nRUN if command -v airflow; then \\\n        register-python-argcomplete airflow >> ~/.bashrc ; \\\n    fi\n\n# Install autocomplete for Kubectl\nRUN echo \"source /etc/bash_completion\" >> ~/.bashrc\n\nWORKDIR ${AIRFLOW_SOURCES}\n\n# Install Helm\nARG HELM_VERSION=\"v3.2.4\"\n\nRUN SYSTEM=$(uname -s | tr '[:upper:]' '[:lower:]') \\\n    && HELM_URL=\"https://get.helm.sh/helm-${HELM_VERSION}-${SYSTEM}-amd64.tar.gz\" \\\n    && curl -f --location \"${HELM_URL}\" | tar -xvz -O \"${SYSTEM}\"-amd64/helm > /usr/local/bin/helm \\\n    && chmod +x /usr/local/bin/helm\n\n# Additional python deps to install\nARG ADDITIONAL_PYTHON_DEPS=\"\"\n\nRUN if [[ -n \"${ADDITIONAL_PYTHON_DEPS}\" ]]; then \\\n        pip install --no-cache-dir ${ADDITIONAL_PYTHON_DEPS}; \\\n    fi\n\nWORKDIR ${AIRFLOW_SOURCES}\n\nENV PATH=\"${HOME}:${PATH}\"\n\n# Needed to stop Gunicorn from crashing when /tmp is now mounted from host\nENV GUNICORN_CMD_ARGS=\"--worker-tmp-dir /dev/shm/\"\n\nARG BUILD_ID\nENV BUILD_ID=${BUILD_ID}\nARG COMMIT_SHA\nENV COMMIT_SHA=${COMMIT_SHA}\n\nLABEL org.apache.airflow.distro=\"debian\" \\\n  org.apache.airflow.distro.version=\"buster\" \\\n  org.apache.airflow.module=\"airflow\" \\\n  org.apache.airflow.component=\"airflow\" \\\n  org.apache.airflow.image=\"airflow-ci\" \\\n  org.apache.airflow.version=\"${AIRFLOW_VERSION}\" \\\n  org.apache.airflow.uid=\"0\" \\\n  org.apache.airflow.gid=\"0\" \\\n  org.apache.airflow.buildId=${BUILD_ID} \\\n  org.apache.airflow.commitSha=${COMMIT_SHA}\n\nEXPOSE 8080\n\nENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/entrypoint\"]\n"
}
{
  "startTime": 1674220100710,
  "endTime": 1674220102467,
  "originalSmells": [
    {
      "rule": "yarnCacheCleanAfterInstall",
      "position": {
        "lineStart": 271,
        "lineEnd": 271,
        "columnStart": 8,
        "columnEnd": 69
      }
    },
    {
      "rule": "yarnCacheCleanAfterInstall",
      "position": {
        "lineStart": 272,
        "lineEnd": 272,
        "columnStart": 8,
        "columnEnd": 41
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 173,
        "lineEnd": 173,
        "columnStart": 4,
        "columnEnd": 47
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 186,
        "lineEnd": 188,
        "columnStart": 7,
        "columnEnd": 56
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 250,
        "lineEnd": 251,
        "columnStart": 8,
        "columnEnd": 59
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 254,
        "lineEnd": 254,
        "columnStart": 8,
        "columnEnd": 100
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 258,
        "lineEnd": 258,
        "columnStart": 12,
        "columnEnd": 78
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 430,
        "lineEnd": 430,
        "columnStart": 4,
        "columnEnd": 47
      }
    }
  ],
  "repairedSmells": [
    {
      "rule": "yarnCacheCleanAfterInstall",
      "position": {
        "lineStart": 271,
        "lineEnd": 271,
        "columnStart": 70,
        "columnEnd": 87
      }
    },
    {
      "rule": "yarnCacheCleanAfterInstall",
      "position": {
        "lineStart": 272,
        "lineEnd": 272,
        "columnStart": 8,
        "columnEnd": 41
      }
    }
  ],
  "repairedDockerfile": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# THIS DOCKERFILE IS INTENDED FOR PRODUCTION USE AND DEPLOYMENT.\n# NOTE! IT IS ALPHA-QUALITY FOR NOW - WE ARE IN A PROCESS OF TESTING IT\n#\n#\n# This is a multi-segmented image. It actually contains two images:\n#\n# airflow-build-image  - there all airflow dependencies can be installed (and\n#                        built - for those dependencies that require\n#                        build essentials). Airflow is installed there with\n#                        --user switch so that all the dependencies are\n#                        installed to ${HOME}/.local\n#\n# main                 - this is the actual production image that is much\n#                        smaller because it does not contain all the build\n#                        essentials. Instead the ${HOME}/.local folder\n#                        is copied from the build-image - this way we have\n#                        only result of installation and we do not need\n#                        all the build essentials. This makes the image\n#                        much smaller.\n#\nARG AIRFLOW_VERSION=\"2.0.0.dev0\"\nARG AIRFLOW_EXTRAS=\"async,amazon,celery,cncf.kubernetes,docker,dask,elasticsearch,ftp,grpc,hashicorp,http,google,microsoft.azure,mysql,postgres,redis,sendgrid,sftp,slack,ssh,statsd,virtualenv\"\nARG ADDITIONAL_AIRFLOW_EXTRAS=\"\"\nARG ADDITIONAL_PYTHON_DEPS=\"\"\n\nARG AIRFLOW_HOME=/opt/airflow\nARG AIRFLOW_UID=\"50000\"\nARG AIRFLOW_GID=\"50000\"\n\nARG CASS_DRIVER_BUILD_CONCURRENCY=\"8\"\n\nARG PYTHON_BASE_IMAGE=\"python:3.6-slim-buster\"\nARG PYTHON_MAJOR_MINOR_VERSION=\"3.6\"\n\nARG PIP_VERSION=20.2.4\n\n##############################################################################################\n# This is the build image where we build all dependencies\n##############################################################################################\nFROM ${PYTHON_BASE_IMAGE} as airflow-build-image\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\n\nARG PYTHON_BASE_IMAGE\nENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE}\n\nARG PYTHON_MAJOR_MINOR_VERSION\nENV PYTHON_MAJOR_MINOR_VERSION=${PYTHON_MAJOR_MINOR_VERSION}\n\nARG PIP_VERSION\nENV PIP_VERSION=${PIP_VERSION}\n\n# Make sure noninteractive debian install is used and language variables set\nENV DEBIAN_FRONTEND=noninteractive LANGUAGE=C.UTF-8 LANG=C.UTF-8 LC_ALL=C.UTF-8 \\\n    LC_CTYPE=C.UTF-8 LC_MESSAGES=C.UTF-8\n\n# Install curl and gnupg2 - needed for many other installation steps\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n           curl \\\n           gnupg2 \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nARG DEV_APT_DEPS=\"\\\n     apt-transport-https \\\n     apt-utils \\\n     build-essential \\\n     ca-certificates \\\n     gnupg \\\n     dirmngr \\\n     freetds-bin \\\n     freetds-dev \\\n     gosu \\\n     krb5-user \\\n     ldap-utils \\\n     libffi-dev \\\n     libkrb5-dev \\\n     libpq-dev \\\n     libsasl2-2 \\\n     libsasl2-dev \\\n     libsasl2-modules \\\n     libssl-dev \\\n     locales  \\\n     lsb-release \\\n     nodejs \\\n     openssh-client \\\n     postgresql-client \\\n     python-selinux \\\n     sasl2-bin \\\n     software-properties-common \\\n     sqlite3 \\\n     sudo \\\n     unixodbc \\\n     unixodbc-dev \\\n     yarn\"\nENV DEV_APT_DEPS=${DEV_APT_DEPS}\n\nARG ADDITIONAL_DEV_APT_DEPS=\"\"\nENV ADDITIONAL_DEV_APT_DEPS=${ADDITIONAL_DEV_APT_DEPS}\n\nARG DEV_APT_COMMAND=\"\\\n    curl --fail --location https://deb.nodesource.com/setup_10.x | bash - \\\n    && curl https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add - > /dev/null \\\n    && echo 'deb https://dl.yarnpkg.com/debian/ stable main' > /etc/apt/sources.list.d/yarn.list\"\nENV DEV_APT_COMMAND=${DEV_APT_COMMAND}\n\nARG ADDITIONAL_DEV_APT_COMMAND=\"echo\"\nENV ADDITIONAL_DEV_APT_COMMAND=${ADDITIONAL_DEV_APT_COMMAND}\n\nARG ADDITIONAL_DEV_ENV_VARS=\"\"\n\n# Note missing man directories on debian-buster\n# https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=863199\n# Install basic and additional apt dependencies\nRUN mkdir -pv /usr/share/man/man1 \\\n    && mkdir -pv /usr/share/man/man7 \\\n    && export ${ADDITIONAL_DEV_ENV_VARS?} \\\n    && bash -o pipefail -e -u -x -c \"${DEV_APT_COMMAND}\" \\\n    && bash -o pipefail -e -u -x -c \"${ADDITIONAL_DEV_APT_COMMAND}\" \\\n    && apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n           ${DEV_APT_DEPS} \\\n           ${ADDITIONAL_DEV_APT_DEPS} \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nARG INSTALL_MYSQL_CLIENT=\"true\"\nENV INSTALL_MYSQL_CLIENT=${INSTALL_MYSQL_CLIENT}\n\nCOPY scripts/docker /scripts/docker\nCOPY docker-context-files /docker-context-files\n# fix permission issue in Azure DevOps when running the script\nRUN chmod a+x /scripts/docker/install_mysql.sh\nRUN ./scripts/docker/install_mysql.sh dev\n\nARG AIRFLOW_REPO=apache/airflow\nENV AIRFLOW_REPO=${AIRFLOW_REPO}\n\nARG AIRFLOW_BRANCH=master\nENV AIRFLOW_BRANCH=${AIRFLOW_BRANCH}\n\nARG AIRFLOW_EXTRAS\nARG ADDITIONAL_AIRFLOW_EXTRAS=\"\"\nENV AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS}${ADDITIONAL_AIRFLOW_EXTRAS:+,}${ADDITIONAL_AIRFLOW_EXTRAS}\n\nARG AIRFLOW_CONSTRAINTS_REFERENCE=\"constraints-master\"\nARG AIRFLOW_CONSTRAINTS_LOCATION=\"https://raw.githubusercontent.com/apache/airflow/${AIRFLOW_CONSTRAINTS_REFERENCE}/constraints-${PYTHON_MAJOR_MINOR_VERSION}.txt\"\nENV AIRFLOW_CONSTRAINTS_LOCATION=${AIRFLOW_CONSTRAINTS_LOCATION}\n\nENV PATH=${PATH}:/root/.local/bin\nRUN mkdir -p /root/.local/bin\n\nRUN if [[ -f /docker-context-files/.pypirc ]]; then \\\n        cp /docker-context-files/.pypirc /root/.pypirc; \\\n    fi\n\nRUN pip install --no-cache-dir --upgrade \"pip==${PIP_VERSION}\"\n\n# By default we do not use pre-cached packages, but in CI/Breeze environment we override this to speed up\n# builds in case setup.py/setup.cfg changed. This is pure optimisation of CI/Breeze builds.\nARG AIRFLOW_PRE_CACHED_PIP_PACKAGES=\"false\"\nENV AIRFLOW_PRE_CACHED_PIP_PACKAGES=${AIRFLOW_PRE_CACHED_PIP_PACKAGES}\n\n# In case of Production build image segment we want to pre-install master version of airflow\n# dependencies from GitHub so that we do not have to always reinstall it from the scratch.\nRUN if [[ ${AIRFLOW_PRE_CACHED_PIP_PACKAGES} == \"true\" ]]; then \\\n       if [[ ${INSTALL_MYSQL_CLIENT} != \"true\" ]]; then \\\n          AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS/mysql,}; \\\n       fi; \\\n       pip install --no-cache-dir --user \\\n          \"https://github.com/${AIRFLOW_REPO}/archive/${AIRFLOW_BRANCH}.tar.gz#egg=apache-airflow[${AIRFLOW_EXTRAS}]\" \\\n          --constraint \"${AIRFLOW_CONSTRAINTS_LOCATION}\" \\\n          && pip uninstall --yes apache-airflow; \\\n    fi\n\n# By default we install latest airflow from PyPI so we do not need to copy sources of Airflow\n# but in case of breeze/CI builds we use latest sources and we override those\n# those SOURCES_FROM/TO with \".\" and \"/opt/airflow\" respectively\nARG AIRFLOW_SOURCES_FROM=\"empty\"\nENV AIRFLOW_SOURCES_FROM=${AIRFLOW_SOURCES_FROM}\n\nARG AIRFLOW_SOURCES_TO=\"/empty\"\nENV AIRFLOW_SOURCES_TO=${AIRFLOW_SOURCES_TO}\n\nCOPY ${AIRFLOW_SOURCES_FROM} ${AIRFLOW_SOURCES_TO}\n\nARG CASS_DRIVER_BUILD_CONCURRENCY\nENV CASS_DRIVER_BUILD_CONCURRENCY=${CASS_DRIVER_BUILD_CONCURRENCY}\n\n# This is airflow version that is put in the label of the image build\nARG AIRFLOW_VERSION\nENV AIRFLOW_VERSION=${AIRFLOW_VERSION}\n\nARG ADDITIONAL_PYTHON_DEPS=\"\"\nENV ADDITIONAL_PYTHON_DEPS=${ADDITIONAL_PYTHON_DEPS}\n\n# Determines the way airflow is installed. By default we install airflow from PyPI `apache-airflow` package\n# But it also can be `.` from local installation or GitHub URL pointing to specific branch or tag\n# Of Airflow. Note That for local source installation you need to have local sources of\n# Airflow checked out together with the Dockerfile and AIRFLOW_SOURCES_FROM and AIRFLOW_SOURCES_TO\n# set to \".\" and \"/opt/airflow\" respectively.\nARG AIRFLOW_INSTALLATION_METHOD=\"apache-airflow\"\nENV AIRFLOW_INSTALLATION_METHOD=${AIRFLOW_INSTALLATION_METHOD}\n\n# By default latest released version of airflow is installed (when empty) but this value can be overriden\n# and we can install specific version of airflow this way.\nARG AIRFLOW_INSTALL_VERSION=\"\"\nENV AIRFLOW_INSTALL_VERSION=${AIRFLOW_INSTALL_VERSION}\n\n# We can seet this value to true in case we want to install .whl .tar.gz packages placed in the\n# docker-context-files folder. This can be done for both - additional packages you want to install\n# and for airflow as well (you have to set INSTALL_FROM_PYPI to false in this case)\nARG INSTALL_FROM_DOCKER_CONTEXT_FILES=\"\"\nENV INSTALL_FROM_DOCKER_CONTEXT_FILES=${INSTALL_FROM_DOCKER_CONTEXT_FILES}\n\n# By default we install latest airflow from PyPI. You can set it to false if you want to install\n# Airflow from the .whl or .tar.gz packages placed in `docker-context-files` folder.\nARG INSTALL_FROM_PYPI=\"true\"\nENV INSTALL_FROM_PYPI=${INSTALL_FROM_PYPI}\n\n# By default we install providers from PyPI but in case of Breze build we want to install providers\n# from local sources without the neeed of preparing provider packages upfront. This value is\n# automatically overridden by Breeze scripts.\nARG INSTALL_PROVIDERS_FROM_SOURCES=\"false\"\nENV INSTALL_PROVIDERS_FROM_SOURCES=${INSTALL_PROVIDERS_FROM_SOURCES}\n\nWORKDIR /opt/airflow\n\n# remove mysql from extras if client is not installed\nRUN if [[ ${INSTALL_MYSQL_CLIENT} != \"true\" ]]; then \\\n        AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS/mysql,}; \\\n    fi; \\\n    if [[ ${INSTALL_FROM_PYPI} == \"true\" ]]; then \\\n        pip install --no-cache-dir --user \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_INSTALL_VERSION}\" \\\n            --constraint \"${AIRFLOW_CONSTRAINTS_LOCATION}\"; \\\n    fi; \\\n    if [[ -n \"${ADDITIONAL_PYTHON_DEPS}\" ]]; then \\\n        pip install --no-cache-dir --user ${ADDITIONAL_PYTHON_DEPS} --constraint \"${AIRFLOW_CONSTRAINTS_LOCATION}\"; \\\n    fi; \\\n    if [[ ${INSTALL_FROM_DOCKER_CONTEXT_FILES} == \"true\" ]]; then \\\n        if ls /docker-context-files/*.{whl,tar.gz} 1> /dev/null 2>&1; then \\\n            pip install --no-cache-dir --user --no-deps /docker-context-files/*.{whl,tar.gz}; \\\n        fi; \\\n    fi; \\\n    find /root/.local/ -name '*.pyc' -print0 | xargs -0 rm -r || true ; \\\n    find /root/.local/ -type d -name '__pycache__' -print0 | xargs -0 rm -r || true\n\nRUN AIRFLOW_SITE_PACKAGE=\"/root/.local/lib/python${PYTHON_MAJOR_MINOR_VERSION}/site-packages/airflow\"; \\\n    if [[ -f \"${AIRFLOW_SITE_PACKAGE}/www_rbac/package.json\" ]]; then \\\n        WWW_DIR=\"${AIRFLOW_SITE_PACKAGE}/www_rbac\"; \\\n    elif [[ -f \"${AIRFLOW_SITE_PACKAGE}/www/package.json\" ]]; then \\\n        WWW_DIR=\"${AIRFLOW_SITE_PACKAGE}/www\"; \\\n    fi; \\\n    if [[ ${WWW_DIR:=} != \"\" ]]; then \\\n        yarn --cwd \"${WWW_DIR}\" install --frozen-lockfile --no-cache; yarn cache clean; \\\n        yarn --cwd \"${WWW_DIR}\" run prod; \\\n        rm -rf \"${WWW_DIR}/node_modules\"; \\\n        rm -vf \"${WWW_DIR}\"/{package.json,yarn.lock,.eslintignore,.eslintrc,.stylelintignore,.stylelintrc,compile_assets.sh,webpack.config.js} ; \\\n    fi\n\n# make sure that all directories and files in .local are also group accessible\nRUN find /root/.local -executable -print0 | xargs --null chmod g+x && \\\n    find /root/.local -print0 | xargs --null chmod g+rw\n\n\nARG BUILD_ID\nENV BUILD_ID=${BUILD_ID}\nARG COMMIT_SHA\nENV COMMIT_SHA=${COMMIT_SHA}\n\n\nLABEL org.apache.airflow.distro=\"debian\" \\\n  org.apache.airflow.distro.version=\"buster\" \\\n  org.apache.airflow.module=\"airflow\" \\\n  org.apache.airflow.component=\"airflow\" \\\n  org.apache.airflow.image=\"airflow-build-image\" \\\n  org.apache.airflow.version=\"${AIRFLOW_VERSION}\" \\\n  org.apache.airflow.buildImage.buildId=${BUILD_ID} \\\n  org.apache.airflow.buildImage.commitSha=${COMMIT_SHA}\n\n##############################################################################################\n# This is the actual Airflow image - much smaller than the build one. We copy\n# installed Airflow and all it's dependencies from the build image to make it smaller.\n##############################################################################################\nFROM ${PYTHON_BASE_IMAGE} as main\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\n\nARG AIRFLOW_UID\nARG AIRFLOW_GID\n\nLABEL org.apache.airflow.distro=\"debian\" \\\n  org.apache.airflow.distro.version=\"buster\" \\\n  org.apache.airflow.module=\"airflow\" \\\n  org.apache.airflow.component=\"airflow\" \\\n  org.apache.airflow.image=\"airflow\" \\\n  org.apache.airflow.uid=\"${AIRFLOW_UID}\" \\\n  org.apache.airflow.gid=\"${AIRFLOW_GID}\"\n\nARG PYTHON_BASE_IMAGE\nENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE}\n\nARG AIRFLOW_VERSION\nENV AIRFLOW_VERSION=${AIRFLOW_VERSION}\n\n# Make sure noninteractive debian install is used and language variables set\nENV DEBIAN_FRONTEND=noninteractive LANGUAGE=C.UTF-8 LANG=C.UTF-8 LC_ALL=C.UTF-8 \\\n    LC_CTYPE=C.UTF-8 LC_MESSAGES=C.UTF-8\n\nARG PIP_VERSION\nENV PIP_VERSION=${PIP_VERSION}\n\n# Install curl and gnupg2 - needed for many other installation steps\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n           curl \\\n           gnupg2 \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nARG RUNTIME_APT_DEPS=\"\\\n       apt-transport-https \\\n       apt-utils \\\n       ca-certificates \\\n       curl \\\n       dumb-init \\\n       freetds-bin \\\n       gnupg \\\n       gosu \\\n       krb5-user \\\n       ldap-utils \\\n       libffi6 \\\n       libsasl2-2 \\\n       libsasl2-modules \\\n       libssl1.1 \\\n       locales  \\\n       lsb-release \\\n       netcat \\\n       openssh-client \\\n       postgresql-client \\\n       rsync \\\n       sasl2-bin \\\n       sqlite3 \\\n       sudo \\\n       unixodbc\"\nENV RUNTIME_APT_DEPS=${RUNTIME_APT_DEPS}\n\nARG ADDITIONAL_RUNTIME_APT_DEPS=\"\"\nENV ADDITIONAL_RUNTIME_APT_DEPS=${ADDITIONAL_RUNTIME_APT_DEPS}\n\nARG RUNTIME_APT_COMMAND=\"echo\"\nENV RUNTIME_APT_COMMAND=${RUNTIME_APT_COMMAND}\n\nARG ADDITIONAL_RUNTIME_APT_COMMAND=\"\"\nENV ADDITIONAL_RUNTIME_APT_COMMAND=${ADDITIONAL_RUNTIME_APT_COMMAND}\n\nARG ADDITIONAL_RUNTIME_ENV_VARS=\"\"\n\n# Note missing man directories on debian-buster\n# https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=863199\n# Install basic and additional apt dependencies\nRUN mkdir -pv /usr/share/man/man1 \\\n    && mkdir -pv /usr/share/man/man7 \\\n    && export ${ADDITIONAL_RUNTIME_ENV_VARS?} \\\n    && bash -o pipefail -e -u -x -c \"${RUNTIME_APT_COMMAND}\" \\\n    && bash -o pipefail -e -u -x -c \"${ADDITIONAL_RUNTIME_APT_COMMAND}\" \\\n    && apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n           ${RUNTIME_APT_DEPS} \\\n           ${ADDITIONAL_RUNTIME_APT_DEPS} \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nARG INSTALL_MYSQL_CLIENT=\"true\"\nENV INSTALL_MYSQL_CLIENT=${INSTALL_MYSQL_CLIENT}\n\nCOPY scripts/docker /scripts/docker\n# fix permission issue in Azure DevOps when running the script\nRUN chmod a+x /scripts/docker/install_mysql.sh\nRUN ./scripts/docker/install_mysql.sh prod\n\nENV AIRFLOW_UID=${AIRFLOW_UID}\nENV AIRFLOW_GID=${AIRFLOW_GID}\n\nENV AIRFLOW__CORE__LOAD_EXAMPLES=\"false\"\n\nARG AIRFLOW_USER_HOME_DIR=/home/airflow\nENV AIRFLOW_USER_HOME_DIR=${AIRFLOW_USER_HOME_DIR}\n\nRUN addgroup --gid \"${AIRFLOW_GID}\" \"airflow\" && \\\n    adduser --quiet \"airflow\" --uid \"${AIRFLOW_UID}\" \\\n        --gid \"${AIRFLOW_GID}\" \\\n        --home \"${AIRFLOW_USER_HOME_DIR}\"\n\nARG AIRFLOW_HOME\nENV AIRFLOW_HOME=${AIRFLOW_HOME}\n\n# Make Airflow files belong to the root group and are accessible. This is to accomodate the guidelines from\n# OpenShift https://docs.openshift.com/enterprise/3.0/creating_images/guidelines.html\nRUN mkdir -pv \"${AIRFLOW_HOME}\"; \\\n    mkdir -pv \"${AIRFLOW_HOME}/dags\"; \\\n    mkdir -pv \"${AIRFLOW_HOME}/logs\"; \\\n    chown -R \"airflow:root\" \"${AIRFLOW_USER_HOME_DIR}\" \"${AIRFLOW_HOME}\"; \\\n    find \"${AIRFLOW_HOME}\" -executable -print0 | xargs --null chmod g+x && \\\n        find \"${AIRFLOW_HOME}\" -print0 | xargs --null chmod g+rw\n\nCOPY --chown=airflow:root --from=airflow-build-image /root/.local \"${AIRFLOW_USER_HOME_DIR}/.local\"\n\nCOPY --chown=airflow:root scripts/in_container/prod/entrypoint_prod.sh /entrypoint\nCOPY --chown=airflow:root scripts/in_container/prod/clean-logs.sh /clean-logs\nRUN chmod a+x /entrypoint /clean-logs\n\nRUN pip install --no-cache-dir --upgrade \"pip==${PIP_VERSION}\"\n\n# Make /etc/passwd root-group-writeable so that user can be dynamically added by OpenShift\n# See https://github.com/apache/airflow/issues/9248\nRUN chmod g=u /etc/passwd\n\nENV PATH=\"${AIRFLOW_USER_HOME_DIR}/.local/bin:${PATH}\"\nENV GUNICORN_CMD_ARGS=\"--worker-tmp-dir /dev/shm\"\n\nWORKDIR ${AIRFLOW_HOME}\n\nEXPOSE 8080\n\nUSER ${AIRFLOW_UID}\n\nARG BUILD_ID\nENV BUILD_ID=${BUILD_ID}\nARG COMMIT_SHA\nENV COMMIT_SHA=${COMMIT_SHA}\n\nLABEL org.apache.airflow.distro=\"debian\" \\\n  org.apache.airflow.distro.version=\"buster\" \\\n  org.apache.airflow.module=\"airflow\" \\\n  org.apache.airflow.component=\"airflow\" \\\n  org.apache.airflow.image=\"airflow\" \\\n  org.apache.airflow.version=\"${AIRFLOW_VERSION}\" \\\n  org.apache.airflow.uid=\"${AIRFLOW_UID}\" \\\n  org.apache.airflow.gid=\"${AIRFLOW_GID}\" \\\n  org.apache.airflow.mainImage.buildId=${BUILD_ID} \\\n  org.apache.airflow.mainImage.commitSha=${COMMIT_SHA}\n\nENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/entrypoint\"]\nCMD [\"--help\"]\n"
}
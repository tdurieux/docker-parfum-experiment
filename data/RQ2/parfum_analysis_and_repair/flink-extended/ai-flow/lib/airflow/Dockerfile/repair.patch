diff --git a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/flink-extended/ai-flow/lib/airflow/Dockerfile b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/flink-extended/ai-flow/lib/airflow/Dockerfile/repaired.Dockerfile
index 968c657..dbc5af3 100644
--- a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/flink-extended/ai-flow/lib/airflow/Dockerfile
+++ b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/flink-extended/ai-flow/lib/airflow/Dockerfile/repaired.Dockerfile
@@ -171,7 +171,7 @@ RUN if [[ -f /docker-context-files/.pypirc ]]; then \
         cp /docker-context-files/.pypirc /root/.pypirc; \
     fi
 
-RUN pip install --upgrade "pip==${PIP_VERSION}"
+RUN pip install --no-cache-dir --upgrade "pip==${PIP_VERSION}"
 
 # By default we do not use pre-cached packages, but in CI/Breeze environment we override this to speed up
 # builds in case setup.py/setup.cfg changed. This is pure optimisation of CI/Breeze builds.
@@ -184,7 +184,7 @@ RUN if [[ ${AIRFLOW_PRE_CACHED_PIP_PACKAGES} == "true" ]]; then \
        if [[ ${INSTALL_MYSQL_CLIENT} != "true" ]]; then \
           AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS/mysql,}; \
        fi; \
-       pip install --user \
+       pip install --no-cache-dir --user \
           "https://github.com/${AIRFLOW_REPO}/archive/${AIRFLOW_BRANCH}.tar.gz#egg=apache-airflow[${AIRFLOW_EXTRAS}]" \
           --constraint "${AIRFLOW_CONSTRAINTS_LOCATION}" \
           && pip uninstall --yes apache-airflow; \
@@ -248,16 +248,16 @@ RUN if [[ ${INSTALL_MYSQL_CLIENT} != "true" ]]; then \
         AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS/mysql,}; \
     fi; \
     if [[ ${INSTALL_FROM_PYPI} == "true" ]]; then \
-        pip install --user "${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_INSTALL_VERSION}" \
+        pip install --no-cache-dir --user "${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_INSTALL_VERSION}" \
             --constraint "${AIRFLOW_CONSTRAINTS_LOCATION}"; \
     fi; \
     if [[ -n "${ADDITIONAL_PYTHON_DEPS}" ]]; then \
-        pip install --user ${ADDITIONAL_PYTHON_DEPS} --constraint "${AIRFLOW_CONSTRAINTS_LOCATION}"; \
+        pip install --no-cache-dir --user ${ADDITIONAL_PYTHON_DEPS} --constraint "${AIRFLOW_CONSTRAINTS_LOCATION}"; \
     fi; \
     if [[ ${INSTALL_FROM_DOCKER_CONTEXT_FILES} == "true" ]]; then \
         if ls /docker-context-files/*.{whl,tar.gz} 1> /dev/null 2>&1; then \
-            pip install --user --no-deps /docker-context-files/*.{whl,tar.gz}; \
-        fi ; \
+            pip install --no-cache-dir --user --no-deps /docker-context-files/*.{whl,tar.gz}; \
+        fi; \
     fi; \
     find /root/.local/ -name '*.pyc' -print0 | xargs -0 rm -r || true ; \
     find /root/.local/ -type d -name '__pycache__' -print0 | xargs -0 rm -r || true
@@ -269,10 +269,10 @@ RUN AIRFLOW_SITE_PACKAGE="/root/.local/lib/python${PYTHON_MAJOR_MINOR_VERSION}/s
         WWW_DIR="${AIRFLOW_SITE_PACKAGE}/www"; \
     fi; \
     if [[ ${WWW_DIR:=} != "" ]]; then \
-        yarn --cwd "${WWW_DIR}" install --frozen-lockfile --no-cache; \
+        yarn --cwd "${WWW_DIR}" install --frozen-lockfile --no-cache; yarn cache clean; \
         yarn --cwd "${WWW_DIR}" run prod; \
         rm -rf "${WWW_DIR}/node_modules"; \
-        rm -vf "${WWW_DIR}"/{package.json,yarn.lock,.eslintignore,.eslintrc,.stylelintignore,.stylelintrc,compile_assets.sh,webpack.config.js} ;\
+        rm -vf "${WWW_DIR}"/{package.json,yarn.lock,.eslintignore,.eslintrc,.stylelintignore,.stylelintrc,compile_assets.sh,webpack.config.js} ; \
     fi
 
 # make sure that all directories and files in .local are also group accessible
@@ -428,7 +428,7 @@ COPY --chown=airflow:root scripts/in_container/prod/entrypoint_prod.sh /entrypoi
 COPY --chown=airflow:root scripts/in_container/prod/clean-logs.sh /clean-logs
 RUN chmod a+x /entrypoint /clean-logs
 
-RUN pip install --upgrade "pip==${PIP_VERSION}"
+RUN pip install --no-cache-dir --upgrade "pip==${PIP_VERSION}"
 
 # Make /etc/passwd root-group-writeable so that user can be dynamically added by OpenShift
 # See https://github.com/apache/airflow/issues/9248
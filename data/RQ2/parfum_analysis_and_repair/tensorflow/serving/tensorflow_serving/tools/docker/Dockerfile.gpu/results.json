{
  "startTime": 1674220108408,
  "endTime": 1674220109076,
  "originalSmells": [
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 33,
        "lineEnd": 50,
        "columnStart": 22,
        "columnEnd": 13
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nARG TF_SERVING_VERSION=latest\nARG TF_SERVING_BUILD_IMAGE=tensorflow/serving:${TF_SERVING_VERSION}-devel-gpu\n\nFROM ${TF_SERVING_BUILD_IMAGE} as build_image\nFROM nvidia/cuda:11.2.1-base-ubuntu18.04\n\nARG TF_SERVING_VERSION_GIT_BRANCH=master\nARG TF_SERVING_VERSION_GIT_COMMIT=head\n\nLABEL maintainer=\"gvasudevan@google.com\"\nLABEL tensorflow_serving_github_branchtag=${TF_SERVING_VERSION_GIT_BRANCH}\nLABEL tensorflow_serving_github_commit=${TF_SERVING_VERSION_GIT_COMMIT}\n\nENV CUDNN_VERSION=8.1.0.77\nENV TF_TENSORRT_VERSION=7.2.2\nENV CUDA=11.2\nENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\n\nRUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/3bf863cc.pub && \\\n    apt-get update && apt-get install -y --no-install-recommends \\\n        ca-certificates \\\n        cuda-command-line-tools-11-2 \\\n        cuda-nvrtc-11-2 \\\n        libcublas-11-2 \\\n        libcublas-dev-11-2 \\\n        libcufft-11-2 \\\n        libcurand-11-2 \\\n        libcusolver-11-2 \\\n        libcusparse-11-2 \\\n        libcudnn8=${CUDNN_VERSION}-1+cuda${CUDA} \\\n        libgomp1 \\\n        build-essential \\\n        curl \\\n        libfreetype6-dev \\\n        pkg-config \\\n        software-properties-common \\\n        unzip && rm -rf /var/lib/apt/lists/*;\n\n# We don't install libnvinfer-dev since we don't need to build against TensorRT\nRUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub && \\\n    echo \"deb https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\"  > /etc/apt/sources.list.d/tensorRT.list && \\\n    apt-get update && \\\n    apt-get install -y --no-install-recommends libnvinfer7=${TF_TENSORRT_VERSION}-1+cuda11.0 \\\n      libnvinfer-plugin7=${TF_TENSORRT_VERSION}-1+cuda11.0 \\\n    && apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*;\n\n# Install TF Serving GPU pkg\nCOPY --from=build_image /usr/local/bin/tensorflow_model_server /usr/bin/tensorflow_model_server\n\n# Expose ports\n# gRPC\nEXPOSE 8500\n\n# REST\nEXPOSE 8501\n\n# Set where models should be stored in the container\nENV MODEL_BASE_PATH=/models\nRUN mkdir -p ${MODEL_BASE_PATH}\n\n# The only required piece is the model name in order to differentiate endpoints\nENV MODEL_NAME=model\n\n# Create a script that runs the model server so we can use environment variables\n# while also passing in arguments from the docker command line\nRUN echo '#!/bin/bash \\n\\n\\\ntensorflow_model_server --port=8500 --rest_api_port=8501 \\\n--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n\"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n&& chmod +x /usr/bin/tf_serving_entrypoint.sh\n\nENTRYPOINT [\"/usr/bin/tf_serving_entrypoint.sh\"]\n\n"
}
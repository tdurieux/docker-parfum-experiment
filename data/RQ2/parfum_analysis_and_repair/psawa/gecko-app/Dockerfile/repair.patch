diff --git a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/psawa/gecko-app/Dockerfile b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/psawa/gecko-app/Dockerfile/repaired.Dockerfile
index ce69bc8..a41027a 100644
--- a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/psawa/gecko-app/Dockerfile
+++ b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/psawa/gecko-app/Dockerfile/repaired.Dockerfile
@@ -5,13 +5,13 @@ WORKDIR /app
 # only copy requirements.txt first to leverage Docker cache
 COPY requirements.txt .
 
-RUN pip install -r requirements.txt
+RUN pip install --no-cache-dir -r requirements.txt
 
 COPY . .
 
-# Downloading the fine tuned models 
-RUN mkdir -p /application/models/gector/data/model_files && cd /application/models/gector/data/model_files && curl -O https://grammarly-nlp-data-public.s3.amazonaws.com/gector/xlnet_0_gector.th
-RUN mkdir -p /application/models/sentence_reorder cd /application/models/sentence_reorder && curl -O http://tts.speech.cs.cmu.edu/sentence_order/nips_bert.tar && tar -xf nips_bert.tar && rm nips_bert.tar && mv nips_bert/ model/
+# Downloading the fine tuned models
+RUN mkdir -p /application/models/gector/data/model_files && cd /application/models/gector/data/model_files && curl -f -O https://grammarly-nlp-data-public.s3.amazonaws.com/gector/xlnet_0_gector.th
+RUN mkdir -p /application/models/sentence_reorder cd /application/models/sentence_reorder && curl -f -O http://tts.speech.cs.cmu.edu/sentence_order/nips_bert.tar && tar -xf nips_bert.tar && rm nips_bert.tar && mv nips_bert/ model/
 
 # Instantiating the models once to trigger the download of pretrained models
 RUN python -c "from application.models.gector import model; model = model.load_model(vocab_path = 'application/models/gector/data/output_vocabulary',model_paths = ['application/models/gector/data/model_files/xlnet_0_gector.th'],model_name = 'xlnet')"
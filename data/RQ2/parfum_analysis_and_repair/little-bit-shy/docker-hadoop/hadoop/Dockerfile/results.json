{
  "startTime": 1674217770087,
  "endTime": 1674217771290,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 100,
        "lineEnd": 100,
        "columnStart": 4,
        "columnEnd": 22
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 101,
        "lineEnd": 101,
        "columnStart": 7,
        "columnEnd": 23
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 102,
        "lineEnd": 102,
        "columnStart": 7,
        "columnEnd": 25
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 103,
        "lineEnd": 103,
        "columnStart": 7,
        "columnEnd": 30
      }
    },
    {
      "rule": "yumInstallRmVarCacheYum",
      "position": {
        "lineStart": 33,
        "lineEnd": 37,
        "columnStart": 4,
        "columnEnd": 23
      }
    },
    {
      "rule": "yumInstallRmVarCacheYum",
      "position": {
        "lineStart": 53,
        "lineEnd": 56,
        "columnStart": 4,
        "columnEnd": 19
      }
    },
    {
      "rule": "yumInstallRmVarCacheYum",
      "position": {
        "lineStart": 89,
        "lineEnd": 89,
        "columnStart": 4,
        "columnEnd": 31
      }
    },
    {
      "rule": "yumInstallRmVarCacheYum",
      "position": {
        "lineStart": 90,
        "lineEnd": 90,
        "columnStart": 7,
        "columnEnd": 32
      }
    },
    {
      "rule": "yumInstallRmVarCacheYum",
      "position": {
        "lineStart": 93,
        "lineEnd": 97,
        "columnStart": 4,
        "columnEnd": 20
      }
    },
    {
      "rule": "yumInstallRmVarCacheYum",
      "position": {
        "lineStart": 105,
        "lineEnd": 108,
        "columnStart": 4,
        "columnEnd": 21
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM centos:7\n\nENV HADOOP_FILE=hadoop.tar.gz\nENV HIVE_FILE=hive.tar.gz\nENV SQOOP_FILE=sqoop.tar.gz\nENV KAFKA_FILE=kafka.tgz\nENV HBASE_FILE=hbase.tar.gz\nENV SPARK_FILE=spark.tgz\nENV SCALA_FILE=scala.tgz\nENV KYLIN_FILE=kylin.tar.gz\n\nENV SSH_PORT = 16022\nENV HADOOP_HOME /usr/local/hadoop\nENV HIVE_HOME /usr/local/hive\nENV PYHIVE_HOME /usr/local/pyhive\nENV KAFKA_HOME /usr/local/kafka\nENV HBASE_HOME /usr/local/hbase\nENV SPARK_HOME /usr/local/spark\nENV SCALA_HOME /usr/local/scala\nENV KYLIN_HOME /usr/local/kylin\nENV HADOOP_MAPRED_HOME $HADOOP_HOME\nENV SQOOP_HOME /usr/local/sqoop\nENV HADOOP_COMMON_HOME $HADOOP_HOME\nENV HADOOP_HDFS_HOME $HADOOP_HOME\nENV YARN_HOME $HADOOP_HOME\nENV HADOOP_COMMON_LIB_NATIVE_DIR $HADOOP_HOME/lib/native\nENV PATH $PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$KAFKA_HOME/bin:$HBASE_HOME/bin:$SPARK_HOME/bin:$SCALA_HOME/bin:$KYLIN_HOME/bin\nENV HADOOP_INSTALL $HADOOP_HOME\nENV HADOOP_CLASSPATH $HADOOP_HOME/lib/*\nENV HADOOP_OPTS \"-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_COMMON_LIB_NATIVE_DIR\"\nENV HIVE_CONF_DIR $HIVE_HOME/conf\nENV HADOOP_CONF_DIR $HADOOP_HOME/etc/hadoop\n\nRUN yum install -y \\\n    wget \\\n    net-tools \\\n    sudo \\\n    java-1.8.0-openjdk* && rm -rf /var/cache/yum\n\n# hadoop安装\nCOPY ./tar/${HADOOP_FILE} /tmp/${HADOOP_FILE}\nRUN cd /tmp \\\n    && mkdir ${HADOOP_HOME} \\\n    && tar -xzf /tmp/${HADOOP_FILE} -C ${HADOOP_HOME} --strip-components=1 \\\n    && rm -f /tmp/${HADOOP_FILE}\n\nWORKDIR ${HADOOP_HOME}\n\nRUN useradd hadoop \\\n    && echo \"hadoop    ALL=(ALL:ALL) NOPASSWD: ALL\" >> /etc/sudoers \\\n    && chown hadoop:hadoop -R ${HADOOP_HOME}\n\n# ssl安装（免密码登陆）\nRUN yum install -y \\\n    which \\\n    openssh-server \\\n    openssh-clients && rm -rf /var/cache/yum\n\nRUN mkdir /home/hadoop/.ssh \\\n    && chmod 700 /home/hadoop/.ssh\nCOPY ./ssh/id_rsa /home/hadoop/.ssh/id_rsa\nCOPY ./ssh/authorized_keys /home/hadoop/.ssh/authorized_keys\nRUN chmod 600 /home/hadoop/.ssh/id_rsa\nRUN chmod 600 /home/hadoop/.ssh/authorized_keys\nRUN chown hadoop:hadoop /home/hadoop/.ssh -R\n\nRUN echo \"Port ${SSH_PORT}\" >> /etc/ssh/sshd_config\n\nRUN  ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key -N \"\"\nRUN  ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key -N \"\"\nRUN  ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key -N \"\"\nCOPY ./entrypoint.sh /tmp/entrypoint.sh\nRUN chmod 777 /tmp/entrypoint.sh\n\n# hive安装\nCOPY ./tar/${HIVE_FILE} /tmp/${HIVE_FILE}\nRUN cd /tmp \\\n    && mkdir ${HIVE_HOME} \\\n    && tar -xzf /tmp/${HIVE_FILE} -C ${HIVE_HOME} --strip-components=1 \\\n    && rm -rf ${HIVE_HOME}/lib/log4j-slf4j-impl-2.6.2.jar \\\n    && rm -f /tmp/${HIVE_FILE}\n\n# sqoop安装\nCOPY ./tar/${SQOOP_FILE} /tmp/${SQOOP_FILE}\nRUN cd /tmp \\\n    && mkdir ${SQOOP_HOME} \\\n    && tar -xzf /tmp/${SQOOP_FILE} -C ${SQOOP_HOME} --strip-components=1 \\\n    && rm -f /tmp/${SQOOP_FILE}\n\nRUN yum install -y epel-release \\\n    && yum install -y python-pip \\\n    && yum clean all && rm -rf /var/cache/yum\n\nRUN yum install -y \\\n    gcc \\\n    gcc-c++ \\\n    python-devel \\\n    cyrus-sasl-devel && rm -rf /var/cache/yum\n\n# pyhive安装\nRUN pip install --no-cache-dir pyhive \\\n    && pip install --no-cache-dir sasl \\\n    && pip install --no-cache-dir thrift \\\n    && pip install --no-cache-dir thrift-sasl\n\nRUN yum install -y \\\n    cyrus-sasl-plain \\\n    cyrus-sasl-devel \\\n    cyrus-sasl-gssapi && rm -rf /var/cache/yum\n\nRUN chown hadoop:hadoop -R ${HIVE_HOME} \\\n    && chown hadoop:hadoop -R ${SQOOP_HOME} \\\n    && mkdir ${PYHIVE_HOME} \\\n    && chown hadoop:hadoop -R ${PYHIVE_HOME}\n\n# kafka安装\nCOPY ./tar/${KAFKA_FILE} /tmp/${KAFKA_FILE}\nRUN cd /tmp \\\n    && mkdir ${KAFKA_HOME} \\\n    && tar -xzf /tmp/${KAFKA_FILE} -C ${KAFKA_HOME} --strip-components=1 \\\n    && rm -f /tmp/${KAFKA_FILE} \\\n    && chown hadoop:hadoop -R ${KAFKA_HOME}\n\n# hbase安装\nCOPY ./tar/${HBASE_FILE} /tmp/${HBASE_FILE}\nRUN cd /tmp \\\n    && mkdir ${HBASE_HOME} \\\n    && tar -xzf /tmp/${HBASE_FILE} -C ${HBASE_HOME} --strip-components=1 \\\n    && rm -f /tmp/${HBASE_FILE} \\\n    && chown hadoop:hadoop -R ${HBASE_HOME}\n\n# spark安装\nCOPY ./tar/${SPARK_FILE} /tmp/${SPARK_FILE}\nRUN cd /tmp \\\n    && mkdir ${SPARK_HOME} \\\n    && tar -xzf /tmp/${SPARK_FILE} -C ${SPARK_HOME} --strip-components=1 \\\n    && rm -f /tmp/${SPARK_FILE} \\\n    && chown hadoop:hadoop -R ${SPARK_HOME}\n\n# scala安装\nCOPY ./tar/${SCALA_FILE} /tmp/${SCALA_FILE}\nRUN cd /tmp \\\n    && mkdir ${SCALA_HOME} \\\n    && tar -xzf /tmp/${SCALA_FILE} -C ${SCALA_HOME} --strip-components=1 \\\n    && rm -f /tmp/${SCALA_FILE}\n\n# kylin安装\nCOPY ./tar/${KYLIN_FILE} /tmp/${KYLIN_FILE}\nRUN cd /tmp \\\n    && mkdir ${KYLIN_HOME} \\\n    && tar -xzf /tmp/${KYLIN_FILE} -C ${KYLIN_HOME} --strip-components=1 \\\n    && rm -f /tmp/${KYLIN_FILE} \\\n    && chown hadoop:hadoop -R ${KYLIN_HOME}\n\nUSER hadoop\n\nENTRYPOINT  /tmp/entrypoint.sh"
}
{
  "startTime": 1674250194093,
  "endTime": 1674250195081,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 32,
        "lineEnd": 32,
        "columnStart": 5,
        "columnEnd": 183
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 43,
        "lineEnd": 43,
        "columnStart": 4,
        "columnEnd": 87
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 32,
        "lineEnd": 32,
        "columnStart": 5,
        "columnEnd": 183
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 43,
        "lineEnd": 43,
        "columnStart": 4,
        "columnEnd": 87
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 21,
        "lineEnd": 21,
        "columnStart": 4,
        "columnEnd": 72
      }
    },
    {
      "rule": "aptGetUpdatePrecedesInstall",
      "position": {
        "lineStart": 21,
        "lineEnd": 21,
        "columnStart": 4,
        "columnEnd": 72
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 21,
        "lineEnd": 21,
        "columnStart": 4,
        "columnEnd": 72
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "#\n# This image is modified version of sequenceiq/hadoop-docker\n#   * sequenceiq/hadoop-docker <https://github.com/sequenceiq/hadoop-docker>\n#\n# The modifications are\n#   * Use local hadoop package\n#   * Change template files to indicate docker master node\n#   * Modify bootstrap script\n#\n# Author: Kai Sasaki\n# Date:   2015 Sep,13\n#\n# Creates multi node hadoop cluster on Docker\n\nFROM sequenceiq/pam:ubuntu-14.04\nMAINTAINER lewuathe\n\nUSER root\n\n# install dev tools\nRUN apt-get update && apt-get install --no-install-recommends -y curl tar sudo openssh-server openssh-client rsync && rm -rf /var/lib/apt/lists/*;\n\n# passwordless ssh\nRUN rm -f /etc/ssh/ssh_host_dsa_key /etc/ssh/ssh_host_rsa_key /root/.ssh/id_rsa\nRUN ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n\n# java\nRUN mkdir -p /usr/java/default && \\\n     curl -f -Ls 'https://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz' -H 'Cookie: oraclelicense=accept-securebackup-cookie' | \\\n     tar --strip-components=1 -xz -C /usr/java/default/\n\n# ADD jdk-8u112-linux-x64.tar.gz /usr/java\n# RUN sudo ln -s /usr/java/jdk1.8.0_112/ /usr/java/default\n\nENV JAVA_HOME /usr/java/default\nENV PATH $PATH:$JAVA_HOME/bin\n\n# download native support\nRUN mkdir -p /tmp/native\nRUN curl -f -Ls https://dl.bintray.com/sequenceiq/sequenceiq-bin/hadoop-native-64-2.7.0.tar | tar -x -C /tmp/native\n\nENV HADOOP_VERSION=3.2.0-SNAPSHOT\nADD hadoop-${HADOOP_VERSION}.tar.gz /usr/local/\nWORKDIR /usr/local\nRUN ln -s /usr/local/hadoop-${HADOOP_VERSION} /usr/local/hadoop\n\nENV HADOOP_HOME /usr/local/hadoop\nENV HADOOP_COMMON_HOME /usr/local/hadoop\nENV HADOOP_HDFS_HOME /usr/local/hadoop\nENV HADOOP_MAPRED_HOME /usr/local/hadoop\nENV HADOOP_YARN_HOME /usr/local/hadoop\nENV HADOOP_CONF_DIR /usr/local/hadoop/etc/hadoop\nENV YARN_CONF_DIR /usr/local/hadoop/etc/hadoop\nENV HADOOP_LOG_DIR /var/log/hadoop\n\nRUN mkdir /var/log/hadoop\n\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/java/default\\nexport H=/usr/local/hadoop\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n#RUN . $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n\nRUN mkdir $HADOOP_HOME/input\nRUN cp $HADOOP_HOME/etc/hadoop/*.xml $HADOOP_HOME/input\n\nADD core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml\nADD hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml\nADD mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml\nADD yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml\nADD log4j.properties $HADOOP_HOME/etc/hadoop/log4j.properties\n\nRUN $HADOOP_HOME/bin/hdfs namenode -format\n\n# fixing the libhadoop.so like a boss\nRUN rm -rf /usr/local/hadoop/lib/native\nRUN mv /tmp/native /usr/local/hadoop/lib\n\nADD ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\n\n# workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n\n# fix the 254 error code\nRUN sed  -i \"/^[^#]*UsePAM/ s/.*/#&/\"  /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\n\nRUN service ssh start\n\n# Hdfs ports\nEXPOSE 9000 50010 50020 50070 50075 50090\n# See https://issues.apache.org/jira/browse/HDFS-9427\nEXPOSE 9871 9870 9820 9869 9868 9867 9866 9865 9864\n# Mapred ports\nEXPOSE 19888\n#Yarn ports\nEXPOSE 8030 8031 8032 8033 8040 8042 8088 8188\n#Other ports\nEXPOSE 49707 2122\n"
}
{
  "startTime": 1674216706709,
  "endTime": 1674216707491,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 9,
        "lineEnd": 9,
        "columnStart": 4,
        "columnEnd": 68
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM tiangolo/uvicorn-gunicorn:python3.8\nLABEL maintainer=\"Omar Espejel <espejelomar@gmail.com>\"\n\n# Add any system dependency here\n# RUN apt-get update -y && apt-get install libXXX -y\n\nCOPY ./requirements.txt /app\n# This enables better docker caching so adding new requirements doesn't\n# retrigger reinstalling the whole pytorch.\nRUN pip install --no-cache-dir torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY ./prestart.sh /app/\n\n\n# Most DL models are quite large in terms of memory, using workers is a HUGE\n# slowdown because of the fork and GIL with python.\n# Using multiple pods seems like a better default strategy.\n# Feel free to override if it does not make sense for your library.\nARG max_workers=1\nENV MAX_WORKERS=$max_workers\nENV HUGGINGFACE_HUB_CACHE=/data\n\n# Necessary on GPU environment docker.\n# TIMEOUT env variable is used by nvcr.io/nvidia/pytorch:xx for another purpose\n# rendering TIMEOUT defined by uvicorn impossible to use correctly\n# We're overriding it to be renamed UVICORN_TIMEOUT\n# UVICORN_TIMEOUT is a useful variable for very large models that take more\n# than 30s (the default) to load in memory.\n# If UVICORN_TIMEOUT is too low, uvicorn will simply never loads as it will\n# kill workers all the time before they finish.\nRUN sed -i 's/TIMEOUT/UVICORN_TIMEOUT/g' /gunicorn_conf.py\nCOPY ./app /app/app\n"
}
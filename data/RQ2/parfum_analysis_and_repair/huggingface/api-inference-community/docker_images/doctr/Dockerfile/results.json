{
  "startTime": 1674249575673,
  "endTime": 1674249576742,
  "originalSmells": [
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 4,
        "lineEnd": 4,
        "columnStart": 25,
        "columnEnd": 59
      }
    },
    {
      "rule": "ruleAptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 4,
        "lineEnd": 4,
        "columnStart": 25,
        "columnEnd": 59
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM tiangolo/uvicorn-gunicorn:python3.8\nLABEL maintainer=\"me <me@example.com>\"\n\n# Add any system dependency here\nRUN apt-get update -y && apt-get install --no-install-recommends libgl1-mesa-glx -y && rm -rf /var/lib/apt/lists/*;\n\nRUN pip install --no-cache-dir -U pip\nRUN pip install --no-cache-dir torch==1.11 torchvision==0.12\nCOPY ./requirements.txt /app\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY ./prestart.sh /app/\n\n\n# Most DL models are quite large in terms of memory, using workers is a HUGE\n# slowdown because of the fork and GIL with python.\n# Using multiple pods seems like a better default strategy.\n# Feel free to override if it does not make sense for your library.\nARG max_workers=1\nENV MAX_WORKERS=$max_workers\nENV HUGGINGFACE_HUB_CACHE=/data\nENV TORCH_HOME=/data/torch_hub/\n\n# Necessary on GPU environment docker.\n# TIMEOUT env variable is used by nvcr.io/nvidia/pytorch:xx for another purpose\n# rendering TIMEOUT defined by uvicorn impossible to use correctly\n# We're overriding it to be renamed UVICORN_TIMEOUT\n# UVICORN_TIMEOUT is a useful variable for very large models that take more\n# than 30s (the default) to load in memory.\n# If UVICORN_TIMEOUT is too low, uvicorn will simply never loads as it will\n# kill workers all the time before they finish.\nRUN sed -i 's/TIMEOUT/UVICORN_TIMEOUT/g' /gunicorn_conf.py\nCOPY ./app /app/app\n"
}
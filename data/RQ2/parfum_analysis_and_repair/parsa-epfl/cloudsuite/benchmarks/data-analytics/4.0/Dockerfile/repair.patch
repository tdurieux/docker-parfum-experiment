diff --git a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/parsa-epfl/cloudsuite/benchmarks/data-analytics/4.0/Dockerfile b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/parsa-epfl/cloudsuite/benchmarks/data-analytics/4.0/Dockerfile/repaired.Dockerfile
index 1f0aa18..93db477 100644
--- a/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfiles/parsa-epfl/cloudsuite/benchmarks/data-analytics/4.0/Dockerfile
+++ b/Users/tdurieux/git/dinghy-experiment/data/evaluation/dockerfile_repair_results/parsa-epfl/cloudsuite/benchmarks/data-analytics/4.0/Dockerfile/repaired.Dockerfile
@@ -11,13 +11,13 @@ RUN apt-get update -y && apt-get install -y --no-install-recommends \
 # Install Mahout
 RUN set -x \
     && URL=https://downloads.apache.org/mahout/${MAHOUT_VERSION}/apache-mahout-distribution-${MAHOUT_VERSION}.tar.gz \
-    && curl ${URL} | tar -xzC /opt \
+    && curl -f ${URL} | tar -xzC /opt \
     && mv /opt/apache-mahout-distribution-${MAHOUT_VERSION} ${MAHOUT_HOME}
 
 # Download dataset
 # Use latest_link=$(curl -s https://dumps.wikimedia.org/enwiki/latest/ | grep  "enwiki-latest-pages-articles1.xml-" | grep -Eoi '<a [^>]+>' | cut -d '"' -f 2 | grep -E "*.bz2$") \
 #     && curl https://dumps.wikimedia.org/enwiki/latest/${latest_link} | bunzip2 > /root/wiki - to get the latest link and download.
-RUN curl https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles1.xml-p1p41242.bz2 | bunzip2 > /root/wiki
+RUN curl -f https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles1.xml-p1p41242.bz2 | bunzip2 > /root/wiki
 
 COPY files/*-site.xml ${HADOOP_CONF_DIR}/
 COPY files/categories /root/
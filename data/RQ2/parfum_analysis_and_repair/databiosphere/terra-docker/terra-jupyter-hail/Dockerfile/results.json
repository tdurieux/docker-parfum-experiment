{
  "startTime": 1674216966103,
  "endTime": 1674216967489,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 22,
        "lineEnd": 22,
        "columnStart": 7,
        "columnEnd": 35
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 23,
        "lineEnd": 23,
        "columnStart": 7,
        "columnEnd": 57
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 30,
        "lineEnd": 30,
        "columnStart": 8,
        "columnEnd": 41
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM us.gcr.io/broad-dsp-gcr-public/terra-jupyter-python:1.0.11\nUSER root\n\nENV PIP_USER=false\nENV PYTHONPATH $PYTHONPATH:/usr/lib/spark/python\nENV PYSPARK_PYTHON=python3\nENV HAIL_VERSION=0.2.96\n\nRUN find $JUPYTER_HOME/scripts -name '*.sh' -type f | xargs chmod +x \\\n    && $JUPYTER_HOME/scripts/kernel/kernelspec.sh $JUPYTER_HOME/scripts/kernel /opt/conda/share/jupyter/kernels \\\n    # Note Spark and Hadoop are mounted from the outside Dataproc VM.\n    # Make empty conf dirs for the update-alternatives commands.\n    && mkdir -p /etc/spark/conf.dist && mkdir -p /etc/hadoop/conf.empty && mkdir -p /etc/hive/conf.dist \\\n    && update-alternatives --install /etc/spark/conf spark-conf /etc/spark/conf.dist 100 \\\n    && update-alternatives --install /etc/hadoop/conf hadoop-conf /etc/hadoop/conf.empty 100 \\\n    && update-alternatives --install /etc/hive/conf hive-conf /etc/hive/conf.dist 100 \\\n    && apt-get update \\\n    && apt install -yq --no-install-recommends openjdk-8-jdk \\\n        g++ \\\n        liblz4-dev \\\n    # specify Java 8\n    && update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java \\\n    && pip3 install --no-cache-dir pypandoc gnomad \\\n    && pip3 install --no-cache-dir --no-dependencies hail==$HAIL_VERSION \\\n    && X=$(mktemp -d) \\\n    && requirements_file=$(mktemp) \\\n    && mkdir -p $X \\\n    && ( cd $X && pip3 download hail==$HAIL_VERSION --no-dependencies && \\\n        unzip hail*.whl && \\\n        grep 'Requires-Dist: ' hail*dist-info/METADATA | sed 's/Requires-Dist: //' | sed 's/ (//' | sed 's/)//' | grep -v 'pyspark' >$requirements_file && \\\n        pip install --no-cache-dir -r $requirements_file) \\\n    && rm -rf $X \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\nENV PIP_USER=true\nUSER $USER\n"
}
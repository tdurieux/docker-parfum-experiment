{
  "user": "rycus86",
  "name": "podlike",
  "namespace": "rycus86",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "Co-located containers as Docker Swarm services (like Kubernetes pods)",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 586317,
  "last_updated": "2019-11-05T11:23:07.546857Z",
  "date_registered": "2018-04-23T19:33:25.542363Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "rycus86",
  "has_starred": false,
  "full_description": "# Podlike\n\nAn attempt at managing co-located containers (like in a Pod in Kubernetes) mainly for services on top of Docker Swarm mode.\nThe general idea is the same: this container will act as a parent for the one or more children containers started as part of the *emulated* pod. Containers within this pod can use `localhost` (the loopback interface) to communicate with each other.\nThey can also share the same volumes, and can also see each other's PIDs, so sending UNIX signals between containers is possible.\n\nThese are always shared:\n\n- Cgroup\n- IPC namespace\n- Network namespace\n\nBy default, these are also shared, but optional:\n\n- PID namespace\n- Volumes\n\n## Use-cases\n\nSo, why would we want to do this on Docker Swarm?\n\n### Sidecars\n\nYou may want to always deploy an application with a supporting application, a sidecar. For example, a web application you want to be accessed only through a caching reverse proxy, or with authentication enabled, but without implementing these in the application itself.\n\n*See also the [sidecar example](https://github.com/rycus86/podlike/tree/master/examples/sidecar)*\n\n### Signals\n\nBy putting containers in the same PID namespace, you send UNIX signals from one to another. Maybe an internal-only small webapp, that sends a SIGHUP to Nginx when it receives a reload request.\n\n*See also the [signal example](https://github.com/rycus86/podlike/tree/master/examples/signal)*\n\n### Log collectors\n\nWith two containers sharing a local volume, you could collect and forward logs from files, that another container is writing. Maybe you have a legacy application with fixed file logging, but you'd still want to use modern log forwarders, like Fluentd.\n\n*See also the [logging example](https://github.com/rycus86/podlike/tree/master/examples/logging)*\n\n### Shared volume and signals\n\nBy sharing a local volume for multiple containers, one could generate configuration for another to use, for example. Combined with singal sending, you could also ask the other app to reload it, when it is written and ready.\n\n*See also the [volume example](https://github.com/rycus86/podlike/tree/master/examples/volume)*\n\n### Health-checks\n\nThis example modernizes an application, by providing a composite HTTP health-check endpoint for a Java application, that only exposes liveness on JMX.\n\n*See also the [volume example](https://github.com/rycus86/podlike/tree/master/examples/healthz)*\n\n## Configuration\n\nThe controller needs to run inside a Docker containers, and it needs access to the Docker engine through the API (either UNIX socket, TCP, etc.). The list of components comes from __container__ labels (not service labels). These labels need to start with `pod.component.`\n\nFor example:\n\n```yaml\nversion: '3.5'\nservices:\n\n  pod:\n    image: rycus86/podlike\n    command: -logs\n    labels:\n      # sample app with HTML responses\n      pod.component.app: |\n        image: rycus86/demo-site\n        environment:\n          - HTTP_HOST=127.0.0.1\n          - HTTP_PORT=12000\n      # caching reverse proxy\n      pod.component.proxy: |\n        image: nginx:1.13.10\n      # copy the config file for the proxy\n      pod.copy.proxy: >\n        /var/conf/nginx.conf:/etc/nginx/conf.d/default.conf\n    configs:\n      - source: nginx-conf\n        target: /var/conf/nginx.conf\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    ports:\n      - 8080:80\n\nconfigs:\n  nginx-conf:\n    file: ./nginx.conf\n    # the actual configuration proxies requests from port 80 to 12000 on localhost\n```\n\nOr as a simple container for testing:\n\n```shell\n$ docker run --rm -it --name podtest                      \\\n    -v /var/run/docker.sock:/var/run/docker.sock:ro       \\\n    -v $PWD/nginx.conf:/etc/nginx/conf.d/default.conf:ro  \\\n    --label pod.component.app='\nimage: rycus86/demo-site\nenvironment:\n  - HTTP_HOST=127.0.0.1\n  - HTTP_PORT=12000'                                      \\\n    --label pod.component.proxy='\nimage: nginx:1.13.10'                                     \\\n    -p 8080:80                                            \\\n    rycus86/podlike -logs\n```\n\nSee the [examples folder](https://github.com/rycus86/podlike/tree/master/examples) with more, small example stacks.\n\nThe properties of each component are the same ones a Compose project would accept, minus the unsupported ones (see below). This should make it easy to convert a Compose file into the configuration this app needs as a `pod.component.` label.\n\nTo make this more convenient, you can specify a Compose file to configure the components from, using the `pod.compose.file` label, which needs to point to a file inside the controller container. This will ignore any properties the app doesn't support, like ports, networking configuration, etc. (see below). This means, if you have a working Compose project, you're likely to be able to use it to feed the app, even without dropping the unsupported properties. You may still want to change things to work better as a group though.\n\n## Dragons!\n\nThis project is very much work in progress (see below). Even with all the tasks done, this will never enable full first-class support for pods on Docker Swarm the way Kubernetes does. Still, it might be useful for small projects or specific deployments.\n\nI'm not yet sure how the components' containers will interfere with Swarm scheduling, resource allocation, etc. Memory limits are honored, but the components are limited to the controller's limits at most. Memory reservation is allowed on the components if you really want to, but comes with a warning. If you set the reservation on the controller, the cgroup should take note of this for you for all the containers.\n\nI also haven't done extensive testing on other resource constraints, in terms of how they behave when running as part of a shared cgroup. For example, CPU and I/O (`blkio`) limits, ulimits, etc. Not sure yet how these settings would affect things overall, and the app doesn't necessarily try to validate them for you, so at this point, you'll have to try and see for yourself. *But do let me know how it goes, please!*\n\nThe current implementation also needs the Docker API connection, usually the engine's UNIX socket as a volume, which will be available to each of the components as well, unless volume sharing is disabled with `-volumes=false`.\n\nSome Swarm features are also *hacked around*, for example configs and secrets can be available to the controller container, but I haven't found easy way to share those with the component containers. These configuration can be copied at component startup, by adding a `pod.copy.<name>=/source/file/in/controller:/dest/file/in/component` label on the controller *(see examples on how to define this in YAML [here](https://github.com/rycus86/podlike/blob/master/engine/component_copy_test.go))*. It does mean, that on every startup or restart, these will be copied again, just be aware. Swarm service labels are also not available on container, and the controller doesn't assume it's running on a Swarm manager node, so we need to use container labels here, which is a bit of a shame.\n\nComponent reaping is done on a best-effort basis, killing the controller could leave you with zombie containers. With the components placed within the controller's cgroup, plus with PID sharing enabled, this is probably somewhat mitigated, but you could still potentialy end up having containers using memory and CPU after the controller dies. The components are also started with auto-remove, so getting information about them post-mortem might prove difficult.\n\n## Work in progress\n\nThis application is very much work in progress, so use it with caution!\n\n## Unsupported properties\n\n- `build`: Only pre-built images are supported\n- `cgroup_parent`: This is set by the controller\n- `container_name`: This is set by the controller\n- `depends_on`: Compose-style dependency is not supported\n- `dns`: DNS management is handled by the controller\n- `dns_opt`: DNS management is handled by the controller\n- `dns_search`: DNS management is handled by the controller\n- `domainname`: Networking is handled by the controller\n- `expose`: Expose ports by publishing them on the Swarm service\n- `extends`: Compose-style extends are not supported\n- `external_links`: Container links are not supported\n- `extra_hosts`: Networking is handled by the controller\n- `hostname`: Networking is handled by the controller\n- `init`: Not supported, the controller *attempts* to take care of it\n- `ipc`: IPC is set by the controller\n- `links`: Container links are not supported, and are probably not needed\n- `mac_address`: Networking is handled by the controller\n- `network_mode`: Network mode is set by the controller\n- `networks`: Assign networks through the Swarm service\n- `pid`: PID mode is set by the controller\n- `platform`: Use a Swarm service constraint instead\n- `ports`: Expose ports by publishing them on the Swarm service\n- `restart`: Restart modes are not supported\n- `scale`: Scale by increasing the number of Swarm service replicas\n- `volume_driver`: *Currently* managed by the controller, using `volumes_from`\n- `volumes`: *Currently* set by the controller, using `volumes_from`\n- `volumes_from`: *Currently* set by the controller\n\nAny other properties from the [v2 Compose file](https://docs.docker.com/compose/compose-file/compose-file-v2/) should be supported, and working as expected.\n\n## Command line usage\n\nThe application supports these command line flags, that you can pass to container or the service, using the `command` property if you're deploying from a stack YAML.\n\n```\nUsage of /podlike:\n  -logs\n    \tStream logs from the components\n  -pids\n    \tEnable (default) or disable PID sharing (default true)\n  -pull\n    \tAlways pull the images for the components when starting\n  -volumes\n    \tEnable (default) or disable volume sharing (default true)\n```\n\nAlternatively, the `healthcheck` argument starts a one-off run that returns the current health status of the app running in the same container. Check the [Dockerfile](Dockerfile) and the [healthcheck/client.go](https://github.com/rycus86/podlike/blob/master/healthcheck/client.go) source code to see how this works.\n\n## License\n\nMIT",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.distribution.manifest.list.v2+json",
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
{
  "user": "autopilotpattern",
  "name": "consul",
  "namespace": "autopilotpattern",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "Implementation of the Autopilot Pattern for Consul",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 4,
  "pull_count": 20383,
  "last_updated": "2017-11-27T22:08:48.851291Z",
  "date_registered": "2016-04-01T18:26:31.087148Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "autopilotpattern",
  "has_starred": false,
  "full_description": "# Consul with the Autopilot Pattern\n\n[Consul](http://www.consul.io/) in Docker, designed to be self-operating according to the autopilot pattern. This application demonstrates support for configuring the Consul raft so it can be used as a highly-available discovery catalog for other applications using the Autopilot pattern.\n\n[![DockerPulls](https://img.shields.io/docker/pulls/autopilotpattern/consul.svg)](https://registry.hub.docker.com/u/autopilotpattern/consul/)\n[![DockerStars](https://img.shields.io/docker/stars/autopilotpattern/consul.svg)](https://registry.hub.docker.com/u/autopilotpattern/consul/)\n\n## Using Consul with ContainerPilot\n\nThis design starts up all Consul instances with the `-bootstrap-expect` flag. This option tells Consul how many nodes we expect and automatically bootstraps when that many servers are available. We still need to tell Consul how to find the other nodes, and this is where [Triton Container Name Service (CNS)](https://docs.joyent.com/public-cloud/network/cns) and [ContainerPilot](https://joyent.com/containerpilot) come into play.\n\nThe ContainerPilot configuration has a management script that is run on each health check interval. This management script runs `consul info` and gets the number of peers for this node. If the number of peers is not equal to 2 (there are 3 nodes in total but a node isn't its own peer), then the script will attempt to find another node via `consul join`. This command will use the Triton CNS name for the Consul service. Because each node is automatically added to the A Record for the CNS name when it starts, the nodes will all eventually find at least one other node and bootstrap the Consul raft.\n\nWhen run locally for testing, we don't have access to Triton CNS. The `local-compose.yml` file uses the v2 Compose API, which automatically creates a user-defined network and allows us to use Docker DNS for the service.\n\n## Run it!\n\n1. [Get a Joyent account](https://my.joyent.com/landing/signup/) and [add your SSH key](https://docs.joyent.com/public-cloud/getting-started).\n1. Install the [Docker Toolbox](https://docs.docker.com/installation/mac/) (including `docker` and `docker-compose`) on your laptop or other environment, as well as the [Joyent Triton CLI](https://www.joyent.com/blog/introducing-the-triton-command-line-tool) (`triton` replaces our old `sdc-*` CLI tools).\n\nCheck that everything is configured correctly by running `./setup.sh`. This will check that your environment is setup correctly and will create an `_env` file that includes injecting an environment variable for a service name for Consul in Triton CNS. We'll use this CNS name to bootstrap the cluster.\n\n```bash\n$ docker-compose up -d\nCreating consul_consul_1\n\n$ docker-compose scale consul=3\nCreating and starting consul_consul_2 ...\nCreating and starting consul_consul_3 ...\n\n$ docker-compose ps\nName                        Command                 State       Ports\n--------------------------------------------------------------------------------\nconsul_consul_1   /usr/local/bin/containerpilot...   Up   53/tcp, 53/udp,\n                                                          8300/tcp, 8301/tcp,\n                                                          8301/udp, 8302/tcp,\n                                                          8302/udp, 8400/tcp,\n                                                          0.0.0.0:8500->8500/tcp\nconsul_consul_2   /usr/local/bin/containerpilot...   Up   53/tcp, 53/udp,\n                                                          8300/tcp, 8301/tcp,\n                                                          8301/udp, 8302/tcp,\n                                                          8302/udp, 8400/tcp,\n                                                          0.0.0.0:8500->8500/tcp\nconsul_consul_3   /usr/local/bin/containerpilot...   Up   53/tcp, 53/udp,\n                                                          8300/tcp, 8301/tcp,\n                                                          8301/udp, 8302/tcp,\n                                                          8302/udp, 8400/tcp,\n                                                          0.0.0.0:8500->8500/tcp\n\n$ docker exec -it consul_consul_3 consul info | grep num_peers\n    num_peers = 2\n\n```\n\n\n## Using this in your own composition\n\nThe Consul service definition can be dropped into any Docker Compose file. Set the ContainerPilot configuration for each other service to use the `CONSUL` environment variable as its Consul target and populate this with the CNS name. On Triton, you should consider using a Consul agent in the application container as a `coprocess`, and point this agent to the `CONSUL` environment variable. The relevant section of the ContainerPilot configuration might look like this:\n\n```json\n{\n  \"consul\": \"localhost:8500\",\n  \"coprocesses\": [\n    {\n      \"command\": [\"/usr/local/bin/consul\", \"agent\",\n                  \"-data-dir=/data\",\n                  \"-config-dir=/config\",\n                  \"-rejoin\",\n                  \"-retry-join\", \"{{ .CONSUL }}\",\n                  \"-retry-max\", \"10\",\n                  \"-retry-interval\", \"10s\"],\n      \"restarts\": \"unlimited\"\n    }]\n  }\n}\n```\n\nA more detailed example of a ContainerPilot configuration that uses a Consul agent co-process can be found in [autopilotpattern/nginx](https://github.com/autopilotpattern/nginx).\n\n## Triton-specific availability advantages\n\nSome details about how Docker containers work on Triton have specific bearing on the durability and availability of this service:\n\n1. Docker containers are first-order objects on Triton. They run on bare metal, and their overall availability is similar or better than what you expect of a virtual machine in other environments.\n1. Docker containers on Triton preserve their IP and any data on disk when they reboot.\n1. Linked containers in Docker Compose on Triton are distributed across multiple unique physical nodes for maximum availability in the case of  node failures.\n\n## Consul encryption\n\nConsul supports TLS encryption for RPC and symmetric pre-shared key encryption for its gossip protocol. Deploying these features requires managing these secrets, and a demonstration of how to do so can be found in the [Vault example](https://github.com/autopilotpattern/vault).\n\n## Credit where it's due\n\nThis project builds on the fine examples set by [Jeff Lindsay](https://github.com/progrium)'s ([Glider Labs](https://github.com/gliderlabs)) [Consul in Docker](https://github.com/gliderlabs/docker-consul/tree/legacy) work. It also, obviously, wouldn't be possible without the outstanding work of the [Hashicorp team](https://hashicorp.com) that made [consul.io](https://www.consul.io).",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json",
    "application/vnd.docker.distribution.manifest.v1+prettyjws",
    "application/octet-stream"
  ],
  "content_types": [
    "image",
    ""
  ]
}
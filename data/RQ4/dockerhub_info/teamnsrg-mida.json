{
  "user": "teamnsrg",
  "name": "mida",
  "namespace": "teamnsrg",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "MIDA Client: Connects to an AMQP queue and processes tasks",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 3258,
  "last_updated": "2022-12-10T19:08:08.490945Z",
  "date_registered": "2020-07-22T04:16:07.356482Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "teamnsrg",
  "has_starred": false,
  "full_description": "# MIDA: A Tool for Measuring the Web\n\n![Build](https://github.com/teamnsrg/mida/workflows/Go/badge.svg)\n[![Go Report Card](https://goreportcard.com/badge/github.com/teamnsrg/mida)](https://goreportcard.com/report/github.com/teamnsrg/mida)\n\nMIDA is meant to be a general tool for web measurement projects. It is built in Go \non top of Chrome/Chromium and the DevTools protocol, giving it a realistic vantage point\nto study the web and fine-grained access to information provided by Chrome Developer Tools.\n\n---\n\n## Getting Started\n\nGetting started with MIDA is easy! First, install:\n\n```bash\n$ wget files.mida.sprai.org/setup.py\n$ sudo python3 setup.py \n```\n\nNow we are ready to visit a site and collect some data:\n```bash\n$ mida go www.illinois.edu\n```\n\nYou can find the results of your crawl in the `results/` directory.\n\n## Easy At-Scale Crawling\n\nOne major benefit of MIDA is in being able to run large scale, highly configurable crawls\nwithout needing to write your own crawler code. Here's an example of a single MIDA command which\nwill crawl the Alexa Top 100K and gather a few specific types of data:\n\n```bash\n$ mida go -f https://files.mida.sprai.org/toplists/alexa.lst -n100000 -c8 --all-resources --screenshot --dom\n```\n\nBreaking this down by argument:\n\n`-f https://files.mida.sprai.org/toplists/alexa.lst`: This is a list of the Alexa Top Websites.\nYou can read from a local file or go get one hosted on the web somewhere\n\n`-n100000`: Read the top 100,000 entries from the list\n\n`-c8`: Run with 8 parallel crawlers (browser instances)\n\n`--all-resources`: Gather all of the actual files/resources required to render the web page.\nBeware, this takes a lot of space!\n\n`--screenshot`: Capture a screenshot after/if the load event for each website fires.\n\n`--dom`: Capture a JSON representation of the DOM for each website visited.",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
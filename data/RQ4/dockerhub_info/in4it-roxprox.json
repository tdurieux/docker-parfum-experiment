{
  "user": "in4it",
  "name": "roxprox",
  "namespace": "in4it",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "roxprox - envoy control plane",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 2044,
  "last_updated": "2022-09-28T14:35:16.585942Z",
  "date_registered": "2019-06-25T06:43:44.440905Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "in4it",
  "has_starred": false,
  "full_description": "# roxprox\n\nEnvoy autocert is an envoy control plane with AWS Cloud support.\n\n* Write & Store configuration in S3\n* Instant s3 notifications update the envoy proxy without having to do a manual reload or restart\n* ACME support to automatically verify, issue and setup letsencrypt certificates\n* authn: support for JWT authentication\n* authz: support for external grpc service which can authorize a connection\n* Access Log Server support\n* Compression support\n* Ratelimit support\n* Works stand-alone or serverless with AWS Fargate\n* Traffic only passes the envoy proxy\n\n## Run roxprox (local storage)\n\n```\ndocker network create roxprox\ndocker run --rm -it --name envoy-control-plane --network roxprox -v $(PWD)/resources/example-proxy:/app/config in4it/roxprox -storage-type local -storage-path config -loglevel debug\n```\n\n## Run roxprox (s3 storage)\n\n```\ndocker network create roxprox\ndocker run --rm -it --name envoy-control-plane --network roxprox in4it/roxprox -acme-contact <your-email-address> -storage-type s3 -storage-bucket your-bucket-name -aws-region your-aws-region\n```\n\n## Run envoy\nThere is an example envoy.yaml in the resources/ directory. Make sure to change the \"address: $IP\" to the ip/host of the control-plane. If you used the docker command above to create the network, you can use the following command to replace the IP:\n```\ncat resources/envoy.yaml |sed 's/$IP/'$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' envoy-control-plane |xargs)/ > resources/envoy-withip.yaml\n```\n\nThen run the envoy proxy:\n```\ndocker run --rm -it -p 10000:10000 -p 10001:10001 -p 9901:9901 --network roxprox -v \"$(PWD)/resources/envoy-withip.yaml\":/etc/envoy/envoy.yaml envoyproxy/envoy:v1.15-latest\n```\n\n## Run access log serve\n```\ncd  resources/access-log-server\nmake docker\ndocker run --rm -it -p 9001:9001 --network roxprox --name als als\n```\n\n## Configuration\nYou can configure endpoints using yaml definitions. Below are example yaml definitions that you can put in your data/ folder.\n\n\n### Simple reverse proxy (hostname + prefix)\n```\napi: proxy.in4it.io/v1\nkind: rule\nmetadata:\n  name: simple-reverse-proxy\nspec:\n  conditions:\n    - hostname: test1.example.com\n      prefix: /api\n  actions:\n    - proxy:\n        hostname: target-example.com\n        port: 443\n```\n\n### Simple reverse proxy (path)\n```\napi: proxy.in4it.io/v1\nkind: rule\nmetadata:\n  name: simple-reverse-proxy\nspec:\n  conditions:\n    - path: /fixed-url\n  actions:\n    - proxy:\n        hostname: target-example.com\n        port: 443\n```\n\n### Simple reverse proxy (regex)\n```\napi: proxy.in4it.io/v1\nkind: rule\nmetadata:\n  name: simple-reverse-proxy\nspec:\n  conditions:\n    - regex: \"/api/v.*/health\"\n  actions:\n    - proxy:\n        hostname: target-example.com\n        port: 443\n```\n\n### ALS\nALS enables the Access Log Server. You'll need to define a grpc cluster in envoy.yaml, because the access log server can only defined statically (a limitation in envoy). There is an example ALS server in [resources/access-log-server/](https://github.com/in4it/roxprox/tree/master/resources/access-log-server).\n\n```\napi: proxy.in4it.io/v1\nkind: accessLogServer\nmetadata:\n    name: accessLogServerExample\nspec:\n    address: \"als\"\n    port: 9001\n```\n\n### Authn\n```\napi: proxy.in4it.io/v1\nkind: rule\nmetadata:\n  name: simple-reverse-proxy\nspec:\n  auth:\n    jwtProvider: myJwtProvider\n  conditions:\n    - prefix: /\n  actions:\n    - proxy:\n        hostname: target-example.com\n        port: 443\n---\napi: proxy.in4it.io/v1\nkind: jwtProvider\nmetadata:\n  name: myJwtProvider\nspec:\n  remoteJwks: https://my-idp.com/.well-known/jwks.json\n  issuer: myIssuer\n  forward: true # forward jwt token to target\n```\n\n### Authorization example\n```\napi: proxy.in4it.io/v1\nkind: authzFilter\nmetadata:\n  name: example-authz\nspec:\n  hostname: localhost # hostname of service, can be localhost if deployed in same container / kubernetes pod / ecs task\n  port: 8080\n  timeout: 5s\n  failureModeAllow: false  # if true, a failure of the service will still let clients reach the target servers\n```\n\n### Directresponse (for a Healthcheck)\n```\napi: proxy.in4it.io/v1\nkind: rule\nmetadata:\n  name: healthcheck\nspec:\n  conditions:\n    - path: /.roxprox/health\n  actions:\n    - directResponse:\n        status: 200\n        body: \"OK\"\n```\n\n### Tracing\nTracing destination hostname (for example Datadog), can be defined in the envoy.yaml (because it's a static host).\n```\napi: proxy.in4it.io/v1\nkind: tracing\nmetadata:\n  name: tracing\nspec:\n  clientSampling: 100\n  randomSampling: 100\n  overallSampling: 100\n```\n\n### Compression\nCompression can automatically compress the traffic between the backend clusters and the clients. The client only has to pass the \"Content-Encoding\" header.\n\n```\napi: proxy.in4it.io/v1\nkind: compression\nmetadata:\n  name: compression\nspec:\n  type: gzip\n  disableOnEtagHeader: true\n```\n\n### Ratelimiting\nRatelimit allows you to ratelimit requests using descriptors (remote address, request headers, destination/source cluster). You define requestPerUnit and a unit type (second/minute/hour/day). Ratelimiting needs a grpc server configured by the name \"ratelimit\". See [github.com/in4it/roxprox-ratelimit](https://github.com/in4it/roxprox-ratelimit) for an in-memory ratelimit server.\n\n```\napi: proxy.in4it.io/v1\nkind: rateLimit\nmetadata:\n  name: ratelimit-example\nspec:\n  descriptors:\n    - remoteAddress: true\n  requestPerUnit: 1\n  Unit: hour\n---\napi: proxy.in4it.io/v1\nkind: rateLimit\nmetadata:\n  name: ratelimit-example-authorized\nspec:\n  descriptors:\n    - requestHeader: \"Authorization\"\n    - destinationCluster: true\n  requestPerUnit: 5\n  Unit: minute\n```\n\n### TLS using letsencrypt\n```\napi: proxy.in4it.io/v1\nkind: rule\nmetadata:\n  name: mocky\nspec:\n  certificate: \"letsencrypt\"\n  conditions:\n    - hostname: mocky-1.in4it.io\n    - hostname: mocky-2.in4it.io\n  actions:\n    - proxy:\n        hostname: www.mocky.io\n        port: 443\n```\n\nThis will run the ACME validation on both hostnames (mocky-1.in4it.io and mocky-2.in4it.io). If successful, it'll create an https listener that redirects to www.mocky.io, a mocking service.\n\n## mTLS\nmTLS listeners can be added on different ports than the default listener. You just need to provide server key/crt and CA cert.\n```\napi: proxy.in4it.io/v1\nkind: mTLS\nmetadata:\n  name: test-rule\nspec:\n  privateKey: |\n    replaceme\n  certificate: |\n    replaceme\n  caCertificate: |\n    replaceme\n  port: 10002\n  AllowedSubjectAltNames: [\"client1.example.com\"] # optional ALT Name subject restriction\n  AllowedIPRanges: [\"1.2.3.4/16\"] # optional IP restriction\n```\n\n## Run on AWS with terraform\n\nThere is a terraform module available in this repository. It'll configure an S3 bucket, a Network Loadbalancer, and 3 fargate containers. The container setup consist of 2 envoy proxies (one for http and one for https), and the roxprox server. To start using it, add the following code to your terraform project:\n\n```\nmodule \"roxprox\" {\n  source              = \"github.com/in4it/roxprox//terraform\"\n  release             = \"latest\"                                     # use a tag or use latest for master\n  acme_contact        = \"your-email\"                                 # email contact used by Let's encrypt, leave empty to disable TLS\n  control_plane_count = 1                                            # desired controle plane instances\n  envoy_proxy_count   = 1                                            # envoy proxy count (there will be still one for http and one for https, due to the AWS Fargate/NLB limitations)\n  subnets             = [\"subnet-1234abcd\"]                          # AWS subnet to use\n  s3_bucket           = \"roxprox\"                                    # s3 bucket to use\n}\n```\n\nYou'll still need to upload the configuration to the s3 bucket\n\n\n# Manual build \n\n```\nprotoc -I proto/ proto/notification.proto --go_out=plugins=grpc:proto/notification\nprotoc -I proto/ proto/config.proto --go_out=plugins=grpc:proto/config\nmake build-linux  # linux\nmake build-darwin # darwin\n```\n",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
{
  "user": "simplygenius",
  "name": "atmos",
  "namespace": "simplygenius",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "Atmos(phere) - Breathe easier with terraform",
  "is_private": false,
  "is_automated": true,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 741,
  "last_updated": "2022-05-19T17:35:17.759838Z",
  "date_registered": "2018-04-11T16:21:05.80675Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "simplygenius",
  "has_starred": false,
  "full_description": "[![Build Status](https://travis-ci.org/simplygenius/atmos.svg?branch=master)](https://travis-ci.org/simplygenius/atmos)\n[![Coverage Status](https://coveralls.io/repos/github/simplygenius/atmos/badge.svg?branch=master)](https://coveralls.io/github/simplygenius/atmos?branch=master)\n[![Gitter](https://badges.gitter.im/simplygenius/atmos.svg)](https://gitter.im/simplygenius/atmos?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n# Atmos\n\nAtmos(phere) - Breathe easier with terraform\n\nAtmos provides a layer of organization on top of terraform for creating cloud system architectures with Amazon Web Services.  It handles the plumbing so you can focus on your application.  The core atmos runtime is free and open-source, with a business friendly license (Apache).  It provides some basic recipes to help get you going with a service oriented architecture implemented with AWS Elastic Container Services.\n\nIn a nutshell, you provide the green, atmos delivers the rest:\n\n![Architecture Diagram](docs/images/architecture_diagram.png)\n\n## Goals\n\n* The whole is greater than the sum of its parts.  Assist in creating a cloud infrastructure _system_ rather than just discrete infrastructure components.  Learning aws and terraform is a lot to bite off when getting started.  It's much easier to start with a working system, and learn incrementally as you go by making changes to it.\n\n* The command line is king.  Using a CLI to iterate on and manage core infrastructure has always been more effective for me, so I aim to make things as convenient and usable as possible from there.\n\n* No lock-in.  Since atmos just provides you convenience on top of using terraform, and the templates for defining your infrastructure are entirely in terraform, its possible to migrate away (albeit giving up all the convenience atmos provides) if your goals ever diverge from those of atmos.\n\n* Your infrastructure is an important product.  It should have its own repo and be tracked under configuration management, not just clickety-click-clicked on in a UI and promptly forgotten what it is you actually did to get there.  The only guarantee you have, is that things are going to need to change, and you'll be much better off with a system that allows you to iterate easily.  Atmos gets you started with minimal up-front knowledge, but provides a path for your infrastructure to evolve.\n\n## Features\n\n* Manages AWS authentication, including MFA\n* Integrated MFA token generation for convenience.  This is technically not as secure since a laptop compromise exposes the key (vs a separate device for MFA).  Plans to add yubikey support to get both convenience and security.\n* Integrated secret management, with per-secret access permissions to minimize exposure footprint if something gets compromised\n* Manages multiple independent environments (e.g. dev, staging, production), allowing them to be as similar or divergent as desired.\n* Automates separation of environments across AWS accounts\n* Secure by default\n* Common recipe patterns to simplify maximal use of higher level AWS components. \n* Sets up dns for your domain along with a wildcard certificate for hassle free ssl on your services\n* Free and open source core with a business friendly license (Apache)\n\n\n## Installation\n\nFirst install the dependencies:\n * [Install docker](https://www.docker.com/community-edition) for deploying containers\n * Install terraform (optional if running atmos as a docker image): e.g. `brew install terraform@0.11` on macOS or Linux\n * Install the aws cli (optional, useful for managing aws credentials): e.g. `brew install awscli` on macOS or Linux\n\nThen install atmos:\n\nTo install as a gem:\n * gem install simplygenius-atmos\n * verify: `atmos --help`\n\nTo install/run as a docker image (experimental):\n * curl -sL https://raw.githubusercontent.com/simplygenius/atmos/master/exe/atmos-docker > /usr/local/bin/atmos\n * chmod +x /usr/local/bin/atmos\n * verify: `atmos --help`\n\nNote that when running as a docker image, UI notifications get forced inline as text output as atmos no longer has access to your current OS.\n\n## Usage\n\nUsage is available via the command line: `atmos --help`\nThe [terraform docs](https://www.terraform.io/docs/index.html) are excellent.\n\n## Quickstart\n\nSee the [screencast](https://simplygenius.wistia.com/medias/zamoxtlqe0) for a detailed walkthrough (~1 hour) of the quickstart.  Or try the [condensed screencast](https://simplygenius.wistia.com/medias/2syql5mnud) if you just want to take a quick look (~7m)\n\n[Create an AWS account](https://portal.aws.amazon.com/billing/signup)\nSetup root account access keys, make note of the numeric account id\n\nIt'll make your life easier dealing with multiple keys if you make use of the AWS shared credential store.  Save the access keys there with this command (picking your own name for the profile):\n\n```\naws configure --profile <root_profile_name>\n```\n\nCreate a new atmos project.  This should only contain files defining your infrastructure, and not your application:\n\n```\nmkdir my-ops-repo\ncd my-ops-repo\natmos new\n```\n\nInitialize the atmos project for aws.  When prompted, input a short name for your organization and the AWS account id to use for the ops environment:\n\n```\natmos generate --force aws/scaffold\n```\n\nThe `--force` is optional and just prevents the prompt for every change the generator is making to files in your repo.\n\nOptionally bring up config/atmos.yml in your editor and make any other desired changes.\n\nBootstrap your cloud provider to work with atmos.  Answer `yes` when prompted to apply the changes.\n\n```\nAWS_PROFILE=<root_profile_name> atmos -e ops bootstrap\nAWS_PROFILE=<root_profile_name> atmos -e ops apply\n```\n\nSetup a non-root user - using your email as the IAM username is convenient for email notifications in the future (e.g. per-user security validations like auto-expiry of access keys)\n\n```\nAWS_PROFILE=<root_profile_name> atmos user create -l -k -g all-users -g ops-admin your@email.address\naws configure --profile <user_profile_name>\n```\n\nIf you supply the \"-m\" flag, it will automatically create and activate a virtual MFA device with the user, and prompt you to save the secret to the atmos mfa keystore for integrated usage.  You can skip saving the secret and instead just copy/paste it into your MFA device of choice.  The \"user create\" command can also act in more of an upsert fashion, so to do something like reset a user's password and keys, you could do `atmos user create --force -l -m -k your@email.address`\n\nLogin to the aws console as that user, change your password and setup MFA there if you prefer doing it that way.  Make sure you log out and back in again with MFA before you try setting up the [role switcher](#per-user-role-switcher-in-console).\n\nNow that a non-root user is created, you should be able to do everything as that user, so you can remove the root access keys if desired.  Keeping them around can be useful though, as there are some AWS operations that can only be done as the root user.  Leaving them in your shared credential store, but deactivating them in the AWS console until needed is a reasonable compromise.  \n\nWhile you can do everything in a single account, I've found a better practice is to use a new account for each env (dev, staging, prod, etc), and leave the ops account providing authentication duties and acting as a jumping off point to the others.  This allows for easier role/permission management down the line as well as better isolation between environments, thereby enabling safe iteration in dev environments without risking production.\n\nCreate a new `dev` account, and bootstrap it to work with atmos\n\n```\nAWS_PROFILE=<user_profile_name> atmos account create dev\nAWS_PROFILE=<user_profile_name> atmos -e ops apply\nAWS_PROFILE=<user_profile_name> atmos -e dev bootstrap\n```\n\nNote that you can `export AWS_PROFILE=<user_profile_name>` in your environment, or keep using it per operation as preferred.\n\nUse the 'aws/service' template to setup an ECS Fargate based service, then apply it in the dev environment to make it active.  This template will also pull in some dependent templates to setup a vpc, dns for the provided domain and a wildcard cert to enable ssl for your service\n\n```\natmos generate --force aws/service\n# If you setup a db for your service, add its password to the secret store.\n# Otherwise the service container will fail to start if it is using the\n# ATMOS_SECRET_KEYS mechanism like the example app is using.\n# atmos -e dev secret set service_<service_name>_db_password sekret!\natmos -e dev apply\n\n```\n\nSetup your application repo to work with ECS by generating a Dockerfile.  For example, [here is the example app](https://github.com/simplygenius/atmos-example-app) used in the demo\n\nTo deploy your app to ECS, first use docker to build an image with a tag named the same as your service name\n\n```\n# In your app repo directory\ndocker build -t <service_name> .\n```\n\nThen use atmos to push and deploy that image to the ECR repo:\n\n```\natmos -e dev container deploy -c services <service_name>\n```\n\nThe atmos aws scaffold also sets up a user named _deployer_, with restricted permissions sufficient to do the deploy.  Add the [key/secret](https://github.com/simplygenius/atmos-recipes/blob/master/aws/scaffold/recipes/atmos-permissions.tf#L159) to the environment for your CI to get your CI to auto-deploy on successful build.\n\n```\nAWS_ACCESS_KEY_ID=<deployer_key> AWS_SECRET_ACCESS_KEY=<deployer_secret> atmos -e <env_based_on_branch> container deploy -r <env>-deployer -c services <service_name>\n```\n\nTo clean it all up:\n\n```\n# Applies flag to allow deleting empty buckets to existing resources.  By\n# default, this step is only required for non-development environments, but\n# doesn't hurt to use it for them too\nTF_VAR_force_destroy_buckets=true atmos -e dev apply\n\n# Destroys all non-bootstrap resources create by atmos\natmos -e dev destroy\n\n# Destroys the bootstrap resources (state, secret, lock storage and\n# cross-account access role)\nTF_VAR_force_destroy_buckets=true atmos -e dev -g bootstrap apply\natmos -e dev -g bootstrap destroy\n\n# For normal usage you should rarely need to cleanup the ops account, but\n# included here in case you want to completely purge the atmos resources after\n# trying things out.\n\n# Cleanup non-bootstrap ops\nAWS_PROFILE=<root_profile_name> TF_VAR_force_destroy_buckets=true atmos -e ops apply\nAWS_PROFILE=<root_profile_name> atmos -e ops destroy\n\n# Cleanup ops bootstrap\nAWS_PROFILE=<root_profile_name> TF_VAR_force_destroy_buckets=true atmos -e ops -g bootstrap apply\nAWS_PROFILE=<root_profile_name> atmos -e ops -g bootstrap destroy\n\n```\n\nThese are separate commands so that day-day usage where you want to tear down everything (e.g. CI spinning up then destroying while testing) doesn't compromise your ability to use atmos/terraform.  You can avoid the extra steps of applying with `TF_VAR_force_destroy_buckets=true` if you set `force_destroy_buckets: true` in atmos.yml\n\n## Per-User Role switcher in Console\n\nIf you are following the account-per-environment pattern, you will need to setup a role switcher for each account in the AWS console for your user.  The AWS console seems to store these in cookies, so if you make a mistake its easy to fix by clearing them.  First, login to the AWS console with your personal aws user that was created in the ops account.  Select the dropdown with your email at top right of the page, Switch Role.  Fill the details for the environment you want to be able to access from the console:\n\n* Account number for the environment (see environments->`<env>`-> account_id in `config/atmos.yml`).\n* Role `<env>-admin` - this is the role you assume in the destination account.\n* Pick a name (e.g. DevAdmin)\n* Pick a color that you like (e.g. Red=production, Yellow=staging, Green=dev)\n\n## Managing secrets\n\nSecrets are stored in a S3 bucket unique to each environment, and automatically passed into terraform when it is executed by atmos.  The secret key should be the same as a terraform variable name defined in your terraform recipes, and if the secret exists, it will override whatever default value you have setup for the terraform variable.\n\nTo set a secret:\n\n`atmos secret -e <env> set key value`\n\nFor other secret usage:\n \n`atmos secret --help`\n\n## Contributing\n\nBug reports and pull requests are welcome on GitHub at https://github.com/simplygenius/atmos.\n\n\n## License\n\nThe gem is available as open source under the terms of the [Apache 2.0 License](https://opensource.org/licenses/apache-2.0).\n",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
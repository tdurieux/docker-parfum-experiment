{
  "user": "nelhage",
  "name": "llama",
  "namespace": "nelhage",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "Llama Lambda runtime",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 62,
  "last_updated": "2021-02-07T05:18:25.800022Z",
  "date_registered": "2020-12-02T18:20:11.366834Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "nelhage",
  "has_starred": false,
  "full_description": "# llama -- A CLI for outsourcing computation to Amazon Lambda\n\nLlama is a tool for running UNIX commands inside of Amazon Lambda. Its\ngoal is to make it easy to outsource compute-heavy tasks to Lambda,\nwith its enormous available parallelism, from your shell.\n\nLambda is neither the cheapest nor the fastest compute available, but\nit has the advantage of supporting nearly-arbitrary parallelism with\nno need for provisioned capacity and minimum configuration, making it\nvery suitable for infrequent burst compute when interactive latency is\ndesirable.\n\n## An example\n\nThe [`optipng`](http://optipng.sourceforge.net/) command compresses\nPNG files and otherwise optimizes them to be as small as possible,\ntypically used in order to save bandwidth and speed load times on\nimage assets. `optipng` is somewhat computationally expensive and\ncompressing a large number of PNG files can be a slow operation. With\n`llama`, we can optimize a large of images by outsourcing the\ncomputation to lambda.\n\nI prepared a directory full of 151 PNG images of the original PokÃ©mon,\nand benchmarked how long it took to optimize them using 8 concurrent\nprocesses on my desktop:\n\n\n```console\n$ time ls -1 *.png | parallel -j 8 optipng {} -out optimized/{/}\n[...]\nreal    0m45.090s\nuser    5m33.745s\nsys     0m0.924s\n```\n\nOnce we've prepared and `optipng` lambda function (we'll talk about\nsetup in a later section), we can use `llama` to run the same\ncomputation in AWS Lambda:\n\n```console\n$ time ls -1 *.png | llama xargs -logs -j 151 optipng optipng '{{.I .Line}}' -out '{{.O (printf \"optimized/%s\" .Line)}}'\nreal    0m16.024s\nuser    0m2.013s\nsys     0m0.569s\n```\n\nWe use `llama xargs`, which works a bit like `xargs(1)`, but runs each\ninput line as a separate command in Lambda. It also uses the Go\ntemplate language to provide flexibility in substitutions, and offers\nthe special `.Input` and `.Output` methods (`.I` and `.O` for short)\nto mark files to be passed back and forth between the local\nenvironment and Lambda.\n\nLambda's CPUs are slower than my desktop and the network operations\nhave overhead, and so we don't see anywhere near a full `151/8`\nspeedup. However, the additional parallelism still nets us a 3x\nimprovement in real-time latency. Note also the vastly decreased\n`user` time, demonstrating that the CPU-intensive work has been\noffloaded, freeing up local compute resources for interactive\napplications or other use cases.\n\nThis operation consumed about 700 CPU-seconds in Lambda. I configured\n`optipng` to have 1792MB of memory, which is the point at which lambda\nallocates a full vCPU to the process. That comes out to about 1254400\nMB-seconds of usage, or about $0.017 assuming I'm already out of the\nLambda free tier.\n\n## `llamacc`\n\nLlama also includes a compiler frontend, `llamacc`, which is a drop-in\nreplacement for GCC (or clang), but which outsources the actual\ncompilation to Amazon Lambda. Coupled with a parallel build process\n(e.g. `make -j`), it can speed up compiles versus local builds,\nespecially on laptops without many cores.\n\nOn my Google Pixelbook, with 2 cores and 4 threads, using `llamacc`\nand `make -j24` cuts the time to build\n[boringssl](https://github.com/google/boringssl) in half compared to local compilation.\n\n`llamacc` is a work in progress and I hope for greater speedups with\nsome additional work.\n\n# Configuring llama\n\nLlama requires a few resources to be configured in AWS in order to\nwork. Llama includes a [CloudFormation][cf] template and a command\nwhich uses it to bootstrap all required resources. You can [read the\ntemplate][template] to see what it's going to do.\n\n[cf]: https://aws.amazon.com/cloudformation/\n[template]: https://github.com/nelhage/llama/blob/master/cmd/llama/internal/bootstrap/template.json\n\nFirst of all, we need configured AWS credentials on our development\nmachine. These should be configured in `~/.aws/credentials` so the AWS\nCLI and `llama` both can find them.\n\nOnce you have those, run `llama bootstrap` to create the required AWS\nresources. By default, it will prompt you for an AWS region to use;\nyou can avoid the prompt using (e.g.) `llama -region us-west-2\nbootstrap`.\n\n## Packaging functions\n\n`llama bootstrap` only needs to be run once, ever. Once it is\nsuccessful, we're ready to package code into Lambda functions for use\nwith `llama`. We'll follow these steps for each environment we want to\nrun code in using Llama.\n\nLlama supports old-type Lambda code packages, where the code is\ndistributed as a zip file, but the easiest way to use Llama is with\nLambda's new Docker container support. Llama can be seen as a bridge\nbetween the Lambda API and Docker containers, allowing us to invoke\narbitrary UNIX command lines within a container.\n\n### Building and uploading a container image\n\nTo run any code in a container using Llama on Lambda, you just need to\nadd the Llama runtime to the container, and point the docker\n`ENTRYPOINT` at it. The `images/optipng/Dockerfile` contains a minimal\nexample, used to create the container for the `optipng` demo\nabove. It's well-commented and explains the pattern you need to wrap\nan arbitrary image inside of Llama.\n\nWe can build that `optipng` container and publish it as a Lambda\nfunction using `scripts/new-function` in this repository:\n\n```console\n$ scripts/new-function optipng images/optipng\n```\n\nWe're now ready to `llama invoke optipng`. Try it out:\n\n```console\n$ llama invoke optipng optipng --help\n```\n\n# llamacc\n\nLlama ships with a `llamac` program that uses `llama` to execute the\nactual compilation inside of a Lambda. You can think of this as a\n[distcc](https://github.com/distcc/distcc) that doesn't require a\ndedicated cluster of your own.\n\nTo set it up, you'll need a Lambda function containing an appropriate\nllama-compatible GCC. You can build one using `images/gcc-9_3` in this\nrepository, or copy the pattern there if you need a different GCC\nversion. Build and upload it like so:\n\n```\n$ scripts/new-function gcc images/gcc-9_3\n```\n\nAnd now you can use `llamacc` to compile code, just like `gcc`, except\nthat the compilation happens in the cloud!\n\n\n```console\n$ cat > main.c\n#include <stdio.h>\n\nint main(void) {\n  printf(\"Hello, World.\\n\");\n  return 0;\n}\n$ export LLAMACC_VERBOSE=1; llamacc -c main.c -o main.o && llamacc main.o -o main\n2020/12/10 10:43:16 run cpp: [\"gcc\" \"-E\" \"-o\" \"-\" \"main.c\"]\n2020/12/10 10:43:16 run gcc: [\"llama\" \"invoke\" \"-o\" \"main.o\" \"-stdin\" \"gcc\" \"gcc\" \"-c\" \"-x\" \"cpp-output\" \"-o\" \"main.o\" \"-\"]\n2020/12/10 10:43:17 [llamacc] compiling locally: no supported input detected ([\"llamacc\" \"main.o\" \"-o\" \"main\"])\n```\n\nWe use `LLAMACC_VERBOSE` to make `llamacc` show what it's doing. We\ncan see that it runs `cpp` locally to preprocess the given source, and\nthen invokes `llama` to do the actual compilation in the\ncloud. Finally, it transpaerntly runs the link step locally.\n\nBecause `llamacc` uses the classic `distcc` strategy of running the\npreprocessor locally it is somewhat limited in its scalability, but it\ncan still get a significant speedup on large projects or on laptops\nwith slow CPUs with limited cores.\n\nYou can also compile C++ by symlinking `llamac++` to `llamacc`.\n\n## llamacc configuration\n\n`llamacc` takes a number of configuration options from the\nenvironment, so that they're easy to pass through your build\nsystem. The currently supported options include.\n\n|Variable|Meaning|\n|--------|-------|\n|`LLAMACC_VERBOSE`| Print commands executed by llamacc|\n|`LLAMACC_LOCAL`  | Run the compilation locally. Useful for e.g. `CC=llamacc ./configure` |\n|`LLAMACC_REMOTE_ASSEMBLE`| Assemble `.S` or `.s` files remotely, as well as C/C++. |\n|`LLAMACC_FUNCTION`| Override the name of the lambda function for the compiler|\n|`LLAMACC_FULL_PREPROCESS`| Run the full preprocessor locally, not just `#include` processing. Disables use of GCC-specific `-fdirectives-only`|\n\n# Other notes\n\n## Using a zip file\n\n\nLlama also supports packaging code using an old-style Lambda layer and\nzip file for code. In this approach, we are responsible for packaging\nall of our dependencies.\n\nBy way of example, we'll just package a small shell script for\nlambda. First, we need to make the Llama runtime available as a Lambda\nlayer:\n\n```console\n$ llama_runtime_arn=$(scripts/publish-runtime)\n```\n\nNow we can create a zip file containing our code, and publish the\nfunction:\n\n```console\n$ mkdir _obj\n$ zip -o _obj/hello.zip -j images/hello-llama/hello.sh\n$ aws lambda create-function \\\n    --function-name hello \\\n    --zip-file fileb://_obj/hello.zip \\\n    --runtime provided.al2 \\\n    --handler hello.sh \\\n    --timeout 60 \\\n    --memory-size 512 \\\n    --layers \"$llama_runtime_arn\" \\\n    --environment \"Variables={LLAMA_OBJECT_STORE=$LLAMA_OBJECT_STORE}\" \\\n    --role \"arn:aws:iam::${account_id}:role/llama\"\n```\n\nAnd invoke it:\n\n```console\n$ llama invoke hello world\nHello from Amazon Lambda\nReceived args: world\n```\n\n# Inspiration\n\nLlama is in large part inspired by [`gg`][gg], a tool for outsourcing\nbuilds to Lambda. Llama is a much simpler tool but shares some of the\nsame ideas and is inspired by a very similar vision of using Lambda as\nhigh-concurrency burst computation for interactive uses.\n\n[gg]: https://github.com/StanfordSNR/gg\n",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
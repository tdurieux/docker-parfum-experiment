{
  "user": "diyer22",
  "name": "bpycv",
  "namespace": "diyer22",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "bpycv's Docker",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 218,
  "last_updated": "2022-09-15T10:58:16.004741Z",
  "date_registered": "2020-12-11T06:28:35.628179Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "diyer22",
  "has_starred": false,
  "full_description": "# `bpycv`: computer vision and deep learning utils for Blender\n\n### Contents: [Features](#-features) | [Install](#-install) | [Demo](#-demo) | [Tips](#-tips) \n\n![0](https://user-images.githubusercontent.com/10448025/115022704-55937980-9ef0-11eb-952e-c85eb5fad4b8.jpg)      \n*Figure.1 Render instance annoatation, RGB image and depth*\n\n## â–® Features\n - [x] Render annotations for semantic segmentation, instance segmentation and panoptic segmentation \n - [x] Generate 6DoF pose ground truth\n - [x] Render depth ground truth\n - [x] Pre-defined domain randomization: \n    - [Light and background](https://github.com/DIYer22/bpycv_example_data/tree/main/background_and_light), automatic download from [HDRI Haven](https://hdrihaven.com/hdris/)\n    - [Distractors](https://arxiv.org/pdf/1804.06516) from [ShapeNet](https://shapenet.org/) (e.g. vase, painting, pallet in Figure.1)\n    - Textures from [Texture Haven](https://texturehaven.com/textures/)\n - [x] Very easy to install and run demo\n - [x] Support docker: `docker run -v /tmp:/tmp diyer22/bpycv` (see [Dockerfile](Dockerfile))\n - [x] A [Python Codebase](example/ycb_demo.py) for building synthetic datasets\n - [x] To Cityscapes format\n\n**News:** [We win ðŸ¥ˆ2nd place in IROS 2020 Open Cloud Robot Table Organization Challenge (OCRTOC)](https://github.com/DIYer22/bpycv/issues/15)\n\n## â–® Install\n`bpycv` support Blender 2.8+, 2.9+\n\n```bash\n# Get pip: equl to /blender-path/2.xx/python/bin/python3.7m -m ensurepip\nblender -b --python-expr \"__import__('ensurepip')._bootstrap()\" \n# Update pip toolchain\nblender -b --python-expr \"__import__('pip._internal')._internal.main(['install', '-U', 'pip', 'setuptools', 'wheel'])\"\n# pip install bpycv\nblender -b --python-expr \"__import__('pip._internal')._internal.main(['install', '-U', 'bpycv'])\"\n# Check bpycv ready\nblender -b -E CYCLES --python-expr 'import bpycv,boxx;boxx.tree(bpycv.render_data())'\n```\n\n## â–® Demo\n#### 1. Fast Instance Segmentation and Depth Demo\nCopy-paste this code to `Scripting/Text Editor` and click `Run Script` button(or `Alt+P`)\n```python\nimport cv2\nimport bpy\nimport bpycv\nimport random\nimport numpy as np\n\n# remove all MESH objects\n[bpy.data.objects.remove(obj) for obj in bpy.data.objects if obj.type == \"MESH\"]\n\nfor index in range(1, 20):\n    # create cube and sphere as instance at random location\n    location = [random.uniform(-2, 2) for _ in range(3)]\n    if index % 2:\n        bpy.ops.mesh.primitive_cube_add(size=0.5, location=location)\n        categories_id = 1\n    else:\n        bpy.ops.mesh.primitive_uv_sphere_add(radius=0.5, location=location)\n        categories_id = 2\n    obj = bpy.context.active_object\n    # set each instance a unique inst_id, which is used to generate instance annotation.\n    obj[\"inst_id\"] = categories_id * 1000 + index\n\n# render image, instance annoatation and depth in one line code\n# result[\"ycb_meta\"] is 6d pose GT\nresult = bpycv.render_data()\n\n# save result\ncv2.imwrite(\n    \"demo-rgb.jpg\", result[\"image\"][..., ::-1]\n)  # transfer RGB image to opencv's BGR\n\n# save instance map as 16 bit png\n# the value of each pixel represents the inst_id of the object to which the pixel belongs\ncv2.imwrite(\"demo-inst.png\", np.uint16(result[\"inst\"]))\n\n# convert depth units from meters to millimeters\ndepth_in_mm = result[\"depth\"] * 1000\ncv2.imwrite(\"demo-depth.png\", np.uint16(depth_in_mm))  # save as 16bit png\n\n# visualization inst_rgb_depth for human\ncv2.imwrite(\"demo-vis(inst_rgb_depth).jpg\", result.vis()[..., ::-1])\n```\nOpen `./demo-vis(inst_rgb_depth).jpg`:   \n\n![demo-vis(inst_rgb_depth)](https://user-images.githubusercontent.com/10448025/115022679-4ad8e480-9ef0-11eb-9a42-cdfbf7e9d2ae.jpg)\n\n#### 2. YCB Demo\n\n```shell\nmkdir ycb_demo\ncd ycb_demo/\n\n# prepare code and example data\ngit clone https://github.com/DIYer22/bpycv\ngit clone https://github.com/DIYer22/bpycv_example_data\n\ncd bpycv/example/\n\nblender -b -P ycb_demo.py\n\ncd dataset/vis/\nls .  # visualize result here\n# 0.jpg\n```\nOpen visualize result `ycb_demo/bpycv/example/dataset/vis/0.jpg`:   \n![0](https://user-images.githubusercontent.com/10448025/115022704-55937980-9ef0-11eb-952e-c85eb5fad4b8.jpg)    \n*instance_map | RGB | depth*\n\n[example/ycb_demo.py](example/ycb_demo.py) Inculding:\n- Domain randomization for background, light and distractor (from ShapeNet)\n- Codebase for building synthetic datasets base on YCB dataset\n\n#### 3. 6DoF Pose Demo\nGenerate and visualize 6DoF pose GT: [example/6d_pose_demo.py](example/6d_pose_demo.py)\n\n<img src=\"https://user-images.githubusercontent.com/10448025/74708759-5e3ee000-5258-11ea-8849-0174c34d507c.png\" style=\"width:300px\">\n\n\n## â–® Tips\n > Blender may can't direct load `.obj` or `.dea` file from YCB and ShapeNet dataset.  \n > It's better to transefer and format using [`meshlabserver`](https://github.com/cnr-isti-vclab/meshlab/releases) by run `meshlabserver -i raw.obj -o for_blender.obj -m wt`\n\n<br>\n<br>\n<div align=\"center\">\n\n**[suggestion](https://github.com/DIYer22/bpycv/issues) and pull request are welcome** ðŸ˜Š\n</div>",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
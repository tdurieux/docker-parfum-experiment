{
  "user": "sufforest",
  "name": "solidbin",
  "namespace": "sufforest",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 238,
  "last_updated": "2020-11-21T14:21:03.567997Z",
  "date_registered": "2019-08-15T10:31:50.763577Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "sufforest",
  "has_starred": false,
  "full_description": "# SolidBin: Improving Metagenome Binning with Semi-supervised Normalized Cut\nA genome binning method for contig binning, based on semi-supervised spectral clustering method.\n\n## <a name=\"started\"></a>Getting Started\n\n### <a name=\"docker\"></a>Conda\n\nWe recommend using conda to run SolidBin. Download [here](https://www.continuum.io/downloads)\n\n### <a name=\"docker\"></a>Obtain SolidBin and create an environment\nAfter installing Anaconda (or miniconda), fisrt obtain SolidBin:\n\n```sh\ngit clone https://github.com/sufforest/SolidBin\n```\nThen simply create a solidbin environment \n\n```sh\ncd SolidBin\nconda env create -f environment.yml\nsource activate solidbin\n```\n\n### <a name=\"docker\"></a>Install checkM (python3 version) like this\n\n(please make sure you have installed openssl)\n\n```sh\ncd CheckM-1.0.18\npython setup.py install\n```\nInstall checkM database:\n\nCheckM relies on a number of precalculated data files which can be downloaded from https://data.ace.uq.edu.au/public/CheckM_databases/. (More details are available at https://github.com/Ecogenomics/CheckM/wiki/Installation#how-to-install-checkm):\n\n```sh\nmkdir <checkm_data_dir>\ncd <checkm_data_dir>\nwget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz\ntar xzf checkm_data_2015_01_16.tar.gz \ncheckm data setRoot .\n```\n\n### <a name=\"docker\"></a>Add dependencies\n\nYou can run these commands to make the files executable\n```sh\nchmod +x ~path_to_SolidBin/auxiliary/test_getmarker.pl\nchmod +x ~path_to_SolidBin/auxiliary/FragGeneScan1.19/run_FragGeneScan.pl\nchmod +x ~path_to_SolidBin/auxiliary/hmmer-3.1b1/bin/hmmsearch\nchmod +x ~path_to_SolidBin/auxiliary/auxiliary/FragGeneScan1.19/FragGeneScan\nchmod +x ~path_to_SolidBin/auxiliary/auxiliary/FragGeneScan1.19/FGS_gff.py\n```\n\n\n## <a name=\"preprocessing\"></a>Preprocessing\n\nThe preprocessing steps aim to generate coverage profile and composition profile as input to our program.\n\nThere are several binning methods that can generate these two types of information (such as CONCOCT and MetaWRAP) and we provide one method to generate the input files as follows.\n### Coverage Profile\nThe users need to modify gen_cov.sh according to the data path and run it. The separator in the coverage file should be tab characters.\n\nFor conda environment, you should check whether perl is installed. or install it using conda. \n```sh\nconda activate solidbin\nconda install perl\n```\n\n```sh\n(conda activate solidbin)\nconda install click\ncd SolidBin\nbash scripts/gen_cov.sh\n```\n\n### Composition Profile\n\nComposition profile is the vector representation of contigs and we use kmer (k=4 in the example) to generate this information. Users can keep the contigs longer than contig_length_threshold, such as 1000, for binning as follows:\n\n```\nbash scripts/run.sh test_data/input/final.contigs.fa 1000 4 \n```\nHere we choose k=4. By default we usually keep contigs longer than 1000, you can specify a different number. The kmer_file will be generated in the /path/to/contig_file. And the script will also keep the contigs longer than contig_length_threshold for the coverage_file.\n\n\n### Coalignment file\nTo generate must-link constraints and cannot-link constraints, you can use TAXAassign [here](https://github.com/umerijaz/taxaassign) to obtain the assignments of some contigs, and then run like this:\n```sh\npython /scripts/filter_unclassified_taxaassign_output.py --TAXAassign_file TAXAassign_output_file\npython /scripts/gen_constraints.py --TAXAassign_file TAXAassign_output_file.filter_unclassified.csv\n```\n\n### <a name=\"docker\"></a>Docker (not the lastest version)\n\nWe also provide our docker image. If you are more familiar with docker, you can just get our image by:\n\n```sh\ndocker pull sufforest/solidbin\n```\n\nA simple way to use our image is just mount you data directory and run:\n\n```sh\ndocker run -it -v /data/StrainMock/input:/input -v /data/StrainMock/output:/output solidbin python SolidBin.py --contig_file /input/StrainMock_Contigs_cutup_10K_nodup_filter_1K.fasta --composition_profiles /input/kmer_4.csv --coverage_profiles /input/cov_inputtableR.tsv --output /output/result.tsv --log /output/log.txt\n```\n\nSuppose that /data/StrainMock contains the data in your machine, this command mount two directories into the container so that our SolidBin can use them.\n\nIf you do not have composition or coverage profiles,  you can just enter the container and generate them by yourself.\n\n```sh\ndocker run -it -v /data/StrainMock/input:/input -v /data/StrainMock/output:/output solidbin sh\n```\n\n\n\n\n## <a name=\"usage\"></a>Usage\n\n\n> - Usage:         [--contig_file CONTIG_FILE]\n                   [--coverage_profiles COVERAGE_PROFILES]\n                   [--composition_profiles COMPOSITION_PROFILES]\n                   [--priori_ml_list ML_LIST] \n                   [--priori_cl_list CL_LIST] \n                   [--output OUTPUT]\n                   [--log LOG_LOCATION]\n                   [--clusters CLUSTERS]\n                   [-a ALPHA]\n                   [-b BETA]\n                   [--use_sfs]\n\n### Usage example (SolidBin-naive) :\n\n```sh\npython SolidBin.py --contig_file test_data/input/final.contigs_1000.fa --coverage_profiles test_data/input/coverage_f1000.tsv --composition_profiles test_data/input/kmer_4_f1000.csv --output test_data/output/result.tsv --log test_data/output/log.txt\n```\n\"result.tsv\" is the original binning result. \"result.tsv.with_postprocess.tsv\" is the binning result using checkM to improve the binning performance.\n> - arguments\n\n  \t--contig_file CONTIG_FILE: \n              The contigs file.\n\t\n  \t--coverage_profiles COVERAGE_PROFILES: \n              The coverage_profiles, containing a table where each\n              row correspond to a contig, and each column correspond\n              to a sample. All values are separated with tabs.\n  \t--composition_profiles COMPOSITION_PROFILES: \n              The composition profiles, containing a table where\n              each row correspond to a contig, and each column\n              correspond to the kmer composition of particular kmer.\n              All values are separated with comma.\n\t\n  \t--output OUTPUT:\n              The output file, storing the binning result.\n\n> - optional\n\n  \t--priori_ml_list ML_LIST:\n              The list of contig pairs under must-link constraints, one row for one constraint in\n              the format: contig_1,contig_2,weight.\n                        \n  \t--priori_cl_list CL_LIST:\n              The list of contig pairs under cannot-link constraints, one row for one constraint in\n              the format: contig_1,contig_2,weight.\n    --log LOG_LOCATION:\n              The log file, storing the binning process and parameters.\n    \n    --clusters CLUSTERS: \n              Specify the number of clusters. If not specified, the\n              cluster number is estimated by single-copy genes.\n                        \n    -a ALPHA:\n              Specify the parameter of must-link constraints, if not specified, \n              alpha is estimated by an one-dimensional search.\n    -b BETA:\n              Specify the parameter of cannot-link constraints, if not specified, \n              beta is estimated by an one-dimensional search.\n                  \n    --use_sfs:\n              Use sequence feature similarity to generate must-link constraints.\n> - different SolidBin modes\n\n  SolidBin mode | Usage  \n  ------------- | -------------\n SolidBin-naive | --contig_file --coverage_profiles --composition_profiles --output --log \n SolidBin-SFS   | --contig_file --coverage_profiles --composition_profiles --output --log --use_sfs\n SolidBin-coalign   | --contig_file --coverage_profiles --composition_profiles --output --log --priori_ml_list\n SolidBin-CL   | --contig_file --coverage_profiles --composition_profiles --output --log --priori_cl_list\n SolidBin-SFS-CL   | --contig_file --coverage_profiles --composition_profiles --output --log --priori_cl_list --use_sfs\n\n## <a name=\"preprocessing\"></a>Contacts and bug reports\nPlease send bug reports or questions (such as the appropriate modes for your datasets) to\nZiye Wang: zwang17@fudan.edu.cn and Dr. Shanfeng Zhu: zhusf@fudan.edu.cn\n\n## <a name=\"preprocessing\"></a>References\n\n[1] Lu, Yang Young, et al. \"COCACOLA: binning metagenomic contigs using sequence COmposition, read CoverAge, CO-alignment and paired-end read LinkAge.\" Bioinformatics 33.6 (2017): 791-798.\n\n[2] Alneberg, Johannes, et al. \"Binning metagenomic contigs by coverage and composition.\" Nature methods 11.11 (2014): 1144.             \n\n[3] Parks DH, Imelfort M, Skennerton CT, Hugenholtz P, Tyson GW. 2015. \"CheckM: assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes.\" Genome Research, 25: 1043â€“1055.\n\n[4] Graham ED, Heidelberg JF, Tully BJ. (2017) \"BinSanity: unsupervised clustering of environmental microbial assemblies using coverage and affinity propagation.\" PeerJ 5:e3035\n\n## <a name=\"preprocessing\"></a>Citation\nWang Z., et al. \"SolidBin: Improving Metagenome Binning with Semi-supervised Normalized Cut.\" Bioinformatics. 2019 Apr 12. pii: btz253. doi: 10.1093/bioinformatics/btz253.\n\nNote: If you cite SolidBin in your paper, please specify the mode of SolidBin you used in your paper to avoid confusion. Thank you. \n\n\n\n",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
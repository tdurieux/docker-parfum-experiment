{
  "user": "viasite",
  "name": "site-audit-seo",
  "namespace": "viasite",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "Tool for SEO site audit, crawl site, lighthouse each page, web viewer",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 268,
  "last_updated": "2022-05-14T19:36:18.595359Z",
  "date_registered": "2020-12-20T10:07:39.031053Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "viasite",
  "has_starred": false,
  "full_description": "[![npm](https://img.shields.io/npm/v/site-audit-seo)](https://www.npmjs.com/package/site-audit-seo) [![npm](https://img.shields.io/npm/dt/site-audit-seo)](https://www.npmjs.com/package/site-audit-seo)\n\nWeb service and CLI tool for SEO site audit: crawl site, lighthouse all pages, view public reports in browser. Also output to console, json, csv, xlsx, Google Drive.\n\nWeb view report - [site-audit-seo-viewer](https://viasite.github.io/site-audit-seo-viewer/).\n\nDemo:\n- [Default report](https://viasite.github.io/site-audit-seo-viewer/?url=https://site-audit.viasite.ru/reports/blog.popstas.ru-default.json)\n- [Lighthouse report](https://viasite.github.io/site-audit-seo-viewer/?url=https://site-audit.viasite.ru/reports/blog.popstas.ru-lighthouse.json)\n- [Default + Basic Lighthouse report](https://viasite.github.io/site-audit-seo-viewer/?url=https://site-audit.viasite.ru/reports/blog.popstas.ru-default-plus-lighthouse.json)\n\nРусское описание [ниже](#русский)\n\n![site-audit-demo](assets/site-audit-demo.gif)\n\n## Using without install\nOpen https://viasite.github.io/site-audit-seo-viewer/.\n\n## Features:\n- Crawls the entire site, collects links to pages and documents\n- Does not follow links outside the scanned domain (configurable)\n- Analyse each page with Lighthouse (see below)\n- Analyse main page text with Mozilla Readability and Yake\n- Search pages with SSL mixed content\n- Scan list of urls, `--url-list`\n- Set default report fields and filters\n- Scan presets\n- Documents with the extensions `doc`,` docx`, `xls`,` xlsx`, `ppt`,` pptx`, `pdf`,` rar`, `zip` are added to the list with a depth == 0\n\n## Technical details:\n- Does not load images, css, js (configurable)\n- Each site is saved to a file with a domain name in `~/site-audit-seo/`\n- Some URLs are ignored ([`preRequest` in `src/scrap-site.js`](src/scrap-site.js#L98))\n\n### XLSX features\n- The first row and the first column are fixed\n- Column width and auto cell height are configured for easy viewing\n- URL, title, description and some other fields are limited in width\n- Title is right-aligned to reveal the common part\n- Validation of some columns (status, request time, description length)\n- Export xlsx to Google Drive and print URL\n\n### Web viewer features:\n- Fixed table header and url column\n- Add/remove columns\n- Column presets\n- Field groups by categories\n- Filters presets (ex. `h1_count != 1`)\n- Color validation\n- Verbose page details (`+` button)\n- Direct URL to same report with selected fields, filters, sort\n- Stats for whole scanned pages, validation summary\n- Persistent URL to report when `--upload` using\n- Switch between last uploaded reports\n- Rescan current report\n\n\n### Fields list (18.08.2020):\n- url\n- mixed_content_url\n- canonical\n- is_canonical\n- previousUrl\n- depth\n- status\n- request_time\n- title\n- h1\n- page_date\n- description\n- keywords\n- og_title\n- og_image\n- schema_types\n- h1_count\n- h2_count\n- h3_count\n- h4_count\n- canonical_count\n- google_amp\n- images\n- images_without_alt\n- images_alt_empty\n- images_outer\n- links\n- links_inner\n- links_outer\n- text_ratio_percent\n- dom_size\n- html_size\n- lighthouse_scores_performance\n- lighthouse_scores_pwa\n- lighthouse_scores_accessibility\n- lighthouse_scores_best-practices\n- lighthouse_scores_seo\n- lighthouse_first-contentful-paint\n- lighthouse_speed-index\n- lighthouse_largest-contentful-paint\n- lighthouse_interactive\n- lighthouse_total-blocking-time\n- lighthouse_cumulative-layout-shift\n- and 150 more lighthouse tests!\n\n\n## Install\n\n## Install with docker-compose\n``` bash\ngit clone https://github.com/viasite/site-audit-seo\ncd site-audit-seo\ngit clone https://github.com/viasite/site-audit-seo-viewer data/front\ndocker-compose pull # for skip build step\ndocker-compose up -d\n```\n\nService will available on http://localhost:5302\n\n##### Default ports:\n- Backend: `5301`\n- Frontend: `5302`\n- Yake: `5303`\n\nYou can change it in `.env` file or in `docker-compose.yml`.\n\n\n## Install with NPM:\n``` bash\nnpm install -g site-audit-seo\n```\n\n#### For linux users\n``` bash\nnpm install -g site-audit-seo --unsafe-perm=true\n```\n\nAfter installing on Ubuntu, you may need to change the owner of the Chrome directory from root to user.\n\nRun this (replace `$USER` to your username or run from your user, not from `root`):\n``` bash\nsudo chown -R $USER:$USER \"$(npm prefix -g)/lib/node_modules/site-audit-seo/node_modules/puppeteer/.local-chromium/\"\n```\n\nError details [Invalid file descriptor to ICU data received](https://github.com/puppeteer/puppeteer/issues/2519).\n\n## Command line usage:\n```\n$ site-audit-seo --help\nUsage: site-audit-seo -u https://example.com --upload\n\nOptions:\n  -u --urls <urls>             Comma separated url list for scan\n  -p, --preset <preset>        Table preset (minimal, seo, headers, parse, lighthouse, lighthouse-all) (default: \"seo\")\n  -e, --exclude <fields>       Comma separated fields to exclude from results\n  -d, --max-depth <depth>      Max scan depth (default: 10)\n  -c, --concurrency <threads>  Threads number (default: by cpu cores)\n  --lighthouse                 Appends base Lighthouse fields to preset\n  --delay <ms>                 Delay between requests (default: 0)\n  -f, --fields <json>          Field in format --field 'title=$(\"title\").text()' (default: [])\n  --no-skip-static             Scan static files\n  --no-limit-domain            Scan not only current domain\n  --docs-extensions            Comma-separated extensions that will be add to table (default: doc,docx,xls,xlsx,ppt,pptx,pdf,rar,zip)\n  --follow-xml-sitemap         Follow sitemap.xml (default: false)\n  --ignore-robots-txt          Ignore disallowed in robots.txt (default: false)\n  -m, --max-requests <num>     Limit max pages scan (default: 0)\n  --no-headless                Show browser GUI while scan\n  --no-remove-csv              No delete csv after xlsx generate\n  --out-dir <dir>              Output directory (default: \".\")\n  --csv <path>                 Skip scan, only convert csv to xlsx\n  --xlsx                       Save as XLSX (default: false)\n  --gdrive                     Publish sheet to google docs (default: false)\n  --json                       Output results in JSON (default: false)\n  --upload                     Upload JSON to public web (default: false)\n  --no-color                   No console colors\n  --lang <lang>                Language (en, ru, default: system language)\n  --open-file                  Open file after scan (default: yes on Windows and MacOS)\n  --no-open-file               Don't open file after scan\n  --no-console-validate        Don't output validate messages in console\n  -V, --version                output the version number\n  -h, --help                   display help for command\n```\n\n\n\n## Custom fields\n\n### Linux/Mac:\n``` bash\nsite-audit-seo -d 1 -u https://example -f 'title=$(\"title\").text()' -f 'h1=$(\"h1\").text()'\n```\n\n### Windows:\n``` bash\nsite-audit-seo -d 1 -u https://example -f title=$('title').text() -f h1=$('h1').text()\n```\n\n## Remove fields from results\nThis will output fields from `seo` preset excluding canonical fields:\n``` bash\nsite-audit-seo -u https://example.com --exclude canonical,is_canonical\n```\n\n## Lighthouse\n### Analyse each page with Lighthouse\n``` bash\nsite-audit-seo -u https://example.com --preset lighthouse\n```\n\n### Analyse seo + Lighthouse\n``` bash\nsite-audit-seo -u https://example.com --lighthouse\n```\n\n## Config file\nYou can copy [.site-audit-seo.conf.js](.site-audit-seo.conf.js) to your home directory and tune options.\n\n## Send to InfluxDB\nIt is beta feature. How to config:\n\n1. Add this to `~/.site-audit-seo.conf`:\n\n``` js\nmodule.exports = {\n  influxdb: {\n    host: 'influxdb.host',\n    port: 8086,\n    database: 'telegraf',\n    measurement: 'site_audit_seo', // optional\n    username: 'user',\n    password: 'password',\n    maxSendCount: 5, // optional, default send part of pages\n  }\n};\n```\n\n2. Use `--influxdb-max-send` in terminal.\n\n3. Create command for scan your urls:\n\n```\nsite-audit-seo -u https://page-with-url-list.txt --url-list --lighthouse --upload --influxdb-max-send 100 >> ~/log/site-audit-seo.log\n```\n\n4. Add command to cron.\n\n\n## Plugins\n- [Readability](https://github.com/popstas/site-audit-seo-readability) - main page text length, reading time\n- [Yake](https://github.com/popstas/site-audit-seo-yake) - keywords extraction from main page text\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for details about plugin development.\n\n#### Install plugins:\n```\ncd data\nnpm install site-audit-seo-readability\nnpm install site-audit-seo-yake\n```\n\n#### Disable plugins:\nYou can add argument such: `--disable-plugins readability,yake`. It more faster, but less data extracted.\n\n## Credentials\nBased on [headless-chrome-crawler](https://github.com/yujiosaka/headless-chrome-crawler) (puppeteer). Used forked version [@popstas/headless-chrome-crawler](https://github.com/popstas/headless-chrome-crawler).\n\n## Bugs\n1. Sometimes it writes identical pages to csv. This happens in 2 cases:\n1.1. Redirect from another page to this (solved by setting `skipRequestedRedirect: true`, hardcoded).\n1.2. Simultaneous request of the same page in parallel threads.\n2. Sometimes a number appears instead of the URL, it occurs at the stage of converting csv to xlsx, don't know why.\n\n\n## Free audit tools alternatives\n- [WebSite Auditor (Link Assistant)](https://www.link-assistant.com/) - desktop app, 500 pages\n- [Screaming Frog SEO Spider](https://www.screamingfrog.co.uk/seo-spider/) - desktop app, same as site-audit-seo, 500 pages\n- [Seobility](https://www.seobility.net/) - 1 project up to 1000 pages free\n- [Neilpatel (Ubersuggest)](https://app.neilpatel.com/) - 1 project, 150 pages\n- [Semrush](https://semrush.com/) - 1 project, 100 pages per month free\n- [Seoptimer](https://www.seoptimer.com/) - good for single page analysis\n\n\n## Free data scrapers\n- [Web Scraper](https://webscraper.io/) - free for local use extension\n- [Portia](https://github.com/scrapinghub/portia) - self-hosted visual scraper builder, scrapy based\n- [Crawlab](https://github.com/crawlab-team/crawlab) - distributed web crawler admin platform, self-hosted with Docker\n- [OutWit Hub](https://www.outwit.com/#hub) - free edition, pro edition for $99\n- [Octoparse](https://www.octoparse.com/) - 10 000 records free\n- [Parsers.me](https://parsers.me/) - 1 000 pages per run free\n- [website-scraper](https://www.npmjs.com/package/website-scraper) - opensource, CLI, download site to local directory\n- [website-scraper-puppeteer](https://www.npmjs.com/package/website-scraper-puppeteer) - same but puppeteer based\n- [Gerapy](https://github.com/Gerapy/Gerapy) - distributed Crawler Management Framework Based on Scrapy, Scrapyd, Django and Vue.js\n\n## Русский\nСканирование одного или несколько сайтов в файлы csv и xlsx.\n\n## Особенности:\n- Обходит весь сайт, собирает ссылки на страницы и документы\n- Сводка результатов после сканирования\n- Документы с расширениями `doc`, `docx`, `xls`, `xlsx`, `pdf`, `rar`, `zip` добавляются в список с глубиной 0\n- Поиск страниц с SSL mixed content\n- Каждый сайт сохраняется в файл с именем домена\n- Не ходит по ссылкам вне сканируемого домена (настраивается)\n- Не загружает картинки, css, js (настраивается)\n- Некоторые URL игнорируются ([`preRequest` в `src/scrap-site.js`](src/scrap-site.js#L112))\n- Можно прогнать каждую страницу по Lighthouse (см. ниже)\n- Сканирование произвольного списка URL, `--url-list`\n\n### Особенности XLSX:\n- Первый ряд и первая колонка закрепляются\n- Ширина колонок и автоматическая высота ячеек настроены для удобного просмотра\n- URL, title, description и некоторые другие поля ограничены по ширине\n- Title выравнивается по правому краю для выявления общей части\n- Валидация некоторых колонок (status, request time, description length)\n- Загрузка xlsx на Google Drive и вывод ссылки\n\n## Установка:\n``` bash\nnpm install -g site-audit-seo\n```\n\n#### Если у вас Ubuntu\n``` bash\nnpm install -g site-audit-seo --unsafe-perm=true\n```\n\n```\nnpm run postinstall-puppeteer-fix\n```\n\nИли запустите это (замените `$USER` на вашего юзера, либо запускайте под юзером, не под `root`):\n``` bash\nsudo chown -R $USER:$USER \"$(npm prefix -g)/lib/node_modules/site-audit-seo/node_modules/puppeteer/.local-chromium/\"\n```\n\nПодробности ошибки [Invalid file descriptor to ICU data received](https://github.com/puppeteer/puppeteer/issues/2519).\n\n\n## Использование\n```\nsite-audit-seo -u https://example.com --upload\n```\n\n\n## Кастомные поля\nМожно передать дополнительные поля так:\n``` bash\nsite-audit-seo -d 1 -u https://example -f \"title=$('title').text()\" -f \"h1=$('h1').text()\"\n```\n\n## Lighthouse\n### Прогнать каждую страницу по Lighthouse\n``` bash\nsite-audit-seo -u https://example.com --preset lighthouse\n```\n\n### Обычный seo аудит + Lighthouse\n``` bash\nsite-audit-seo -u https://example.com --lighthouse\n```\n\n## Как посчитать контент по csv\n1. Открыть в блокноте\n2. Документы посчитать поиском `,0`\n3. Листалки исключить поиском `?`\n4. Вычесть 1 (шапка)\n\n\n## Баги\n1. Иногда пишет в csv одинаковые страницы. Это бывает в 2 случаях:\n1.1. Редирект с другой страницы на эту (решается установкой `skipRequestedRedirect: true`, сделано).\n1.2. Одновременный запрос одной и той же страницы в параллельных потоках.\n2. Иногда вместо URL появляется цифра, происходит на этапе конвертации csv в xlsx, не знаю почему.\n\n\n## TODO:\n- Unique links\n- [Offline w3c validation](https://www.npmjs.com/package/html-validator)\n- [Words count](https://github.com/IonicaBizau/count-words)\n- [Sentences count](https://github.com/NaturalNode/natural)\n- Do not load image with non-standard URL, like [this](https://lh3.googleusercontent.com/pw/ACtC-3dd9Ng2Jdq713vsFqqTrNT6j_nyH3mFsRAzPbIAzWvDoRkiKSW2MIQOxrtpPVab4e9BElcL_Rlr8eGT68R7ZBnLCHpnHHJNRcd8JadddrxpVVClu1iOnkxPUQXOx-7OoNDmeEtH0xyg7NkEI8VF0oJRXQ=w1423-h1068-no?authuser=0)\n- External follow links\n- Broken images\n- Breadcrumbs - https://github.com/glitchdigital/structured-data-testing-tool\n- joeyguerra/schema.js - https://gist.github.com/joeyguerra/7740007\n- smhg/microdata-js - https://github.com/smhg/microdata-js\n- indicate page scan error",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
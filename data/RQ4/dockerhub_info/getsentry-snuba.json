{
  "user": "getsentry",
  "name": "snuba",
  "namespace": "getsentry",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "Sentry storage service.",
  "is_private": false,
  "is_automated": true,
  "can_edit": false,
  "star_count": 4,
  "pull_count": 39965223,
  "last_updated": "2023-01-06T01:16:33.670624Z",
  "date_registered": "2018-03-29T23:04:20.626586Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "getsentry",
  "has_starred": false,
  "full_description": "<img src=\"https://raw.githubusercontent.com/getsentry/snuba/master/snuba/web/static/img/snuba.svg?sanitize=true\" width=\"150\" height=\"71\"/>\n\nA service providing fast event searching, filtering and aggregation on arbitrary fields.\n\n## Sentry + Snuba\n\nAdd/change the following lines in `~/.sentry/sentry.conf.py`:\n\n    SENTRY_SEARCH = 'sentry.search.snuba.EventsDatasetSnubaSearchBackend'\n    SENTRY_TSDB = 'sentry.tsdb.redissnuba.RedisSnubaTSDB'\n    SENTRY_EVENTSTREAM = 'sentry.eventstream.snuba.SnubaEventStream'\n\nRun:\n\n    sentry devservices up\n\nAccess raw clickhouse client (similar to psql):\n\n    docker exec -it sentry_clickhouse clickhouse-client\n\nData is written into the table `sentry_local`: `select count() from sentry_local;`\n\n## Requirements (Only required if you are developing against Snuba)\n\nSnuba assumes:\n\n1. A Clickhouse server endpoint at `CLICKHOUSE_HOST` (default `localhost`).\n2. A redis instance running at `REDIS_HOST` (default `localhost`). On port\n   `6379`\n\nA quick way to get these services running is to set up sentry, then use:\n\n    sentry devservices up --exclude=snuba\n\n## Install / Run (Only required if you are developing against Snuba)\n\n    mkvirtualenv snuba --python=python3.7\n    workon snuba\n    make install-python-dependencies\n    make install-librdkafka\n    make setup-git\n\n    # Run API server\n    snuba api\n\n## API\n\nSnuba exposes an HTTP API (default port: `1218`) with the following endpoints.\n\n- [/](/): Shows this page.\n- [/dashboard](/dashboard): Query dashboard\n- [/query](/query): Endpoint for querying clickhouse.\n- [/config](/config): Console for runtime config options\n\n## Settings\n\nSettings are found in `settings.py`\n\n- `CLICKHOUSE_HOST` : The hostname for the clickhouse service.\n- `REDIS_HOST` : The host redis is running on.\n- `DATASET_MODE` : If \"local\" runs Clickhouse local tables instead of distributed ones.\n\n## Tests\n\n    pip install -e .\n    make test\n\n## Testing Against Sentry\n\n```\nworkon snuba\ngit checkout your-snuba-branch\nsnuba api\n```\nAnd then in another terminal\n```\nworkon sentry\ngit checkout master\ngit pull\nsentry devservices up --exclude=snuba\n```\nThis will get the most recent version of Sentry on master, and bring up all snuba's dependencies.\n\nYou will want to run the following Sentry tests:\n```\nUSE_SNUBA=1 make test-acceptance\nUSE_SNUBA=1 make test-snuba\nmake test-python\n```\nNote that python tests do not currently pass with the `USE_SNUBA` flag, but should be fixed in the future. For now, simply run it without `USE_SNUBA` flag (which determines the version of TagStore). Note also that we check for the existance of `USE_SNUBA` rather than take into account the value. `USE_SNUBA=0` does not currently work as intended.\n\n## Querying\n\nTry out queries on the [query console](/query). Queries are submitted as a JSON\nbody to the [/query](/query) endpoint.\n\n### The Query Payload\n\nAn example query body might look like:\n\n    {\n        \"project\":[1,2],\n        \"selected_columns\": [\"tags[environment]\"],\n        \"aggregations\": [\n            [\"max\", \"received\", \"last_seen\"]\n        ],\n        \"conditions\": [\n            [\"tags[environment]\", \"=\", \"prod\"]\n        ],\n        \"from_date\": \"2011-07-01T19:54:15\",\n        \"to_date\": \"2018-07-06T19:54:15\"\n        \"granularity\": 3600,\n        \"groupby\": [\"group_id\", \"time\"],\n        \"having\": [],\n        \"issues\": [],\n    }\n\n#### selected_columns, groupby\n\n`groupby` is a list of columns (or column aliases) that will be translated into\na SQL `GROUP BY` clause. These columns are automatically included in the query\noutput.  `selected_columns` is a list of additional columns that should be added\nto the `SELECT` clause.\n\nTrying to use both of these in the same query will probably result in an invalid\nquery, as you cannot select a bare column, while grouping by another column, as\nthe value of the extra selected column for a given output row (group) would be\nambiguous.\n\n#### aggregations\n\nThis is an array of 3-tuples of the form:\n\n    [function, column, alias]\n\nwhich is transformed into the SQL:\n\n    function(column) AS alias\n\nSome aggregation function are generated by other functions, eg topK. so an\nexample query would send:\n\n    [\"top5\", \"environment\", \"top_five_envs\"]\n\nTo produce the SQL:\n\n    topK(5)(environment) AS top_five_envs\n\nCount is a somewhat special case, it doesn't have a column argument, and is\nspecified as \"count()\", not \"count\".\n\n    [\"count()\", null, \"item_count\"]\n\nAggregations are also included in the output columns automatically.\n\n\n#### conditions\n\nConditions are used to construct the WHERE clause, and consist of an\narray of 3-tuples (in their most basic form):\n\n    [column_name, operation, literal]\n\nValid operations:\n\n    ['>', '<', '>=', '<=', '=', '!=', 'IN', 'NOT IN', 'IS NULL', 'IS NOT NULL', 'LIKE', 'NOT LIKE'],\n\nFor example:\n\n    [\n        ['platform', '=', 'python'],\n    ]\n\n    platform = 'python'\n\nTop-level sibling conditions are `AND`ed together:\n\n    [\n        ['w', '=', '1'],\n        ['x', '=', '2'],\n    ]\n\n    w = '1' AND x = '2'\n\nThe first position (column_name) can be replaced with an array that\nrepresents a function call. The first item is a function name, and nested\narrays represent the arguments supplied to the preceding function:\n\n    [\n        [['fn1', []], '=', '1'],\n    ]\n\n    fn1() = '1'\n\nMultiple arguments can be provided:\n\n    [\n        [['fn2', ['arg', 'arg']], '=', '2'],\n    ]\n\n    fn2(arg, arg) = '2'\n\nFunction calls can be nested:\n\n    [\n        [['fn3', ['fn4', ['arg']]], '=', '3'],\n    ]\n\n    fn3(fn4(arg)) = '3'\n\nAn alias can be provided at the end of the top-level function array. This\nalias can then be used elsewhere in the query, such as `selected_columns`:\n\n    [\n        [['fn1', [], 'alias'], '=', '1'],\n    ]\n\n    (fn1() AS alias) = '1'\n\nTo do an `OR`, nest the array one level deeper:\n\n    [\n        [\n            ['w', '=', '1'],\n            ['x', '=', '2'],\n        ],\n    ]\n\n    (w = '1' OR x = '2')\n\nSibling arrays at the second level are `AND`ed (note this is the same\nas the simpler `AND` above):\n\n    [\n        [\n            ['w', '=', '1'],\n        ],\n        [\n            ['x', '=', '2'],\n        ],\n    ]\n\n    (w = '1' AND x = '2')\n\nAnd these two can be combined to mix `OR` and `AND`:\n\n    [\n        [\n            ['w', '=', '1'], ['x', '=', '2']\n        ],\n        [\n            ['y', '=', '3'], ['z', '=', '4']\n        ],\n    ]\n\n    (w = '1' OR x = '2') AND (y = '3' OR z = '4')\n\n#### from_date / to_date\n\n#### granularity\n\nSnuba provides a magic column `time`, that you can use in groupby or filter\nexpressions. This column gives a floored time value for each event so that\nevents in the same minute/hour/day/etc. can be grouped.\n\n`granularity` determines the number of seconds in each of these time buckets.\nEg, to count the number of events by hour, you would do\n\n    {\n        \"aggregations\": [[\"count()\", \"\", \"event_count\"]],\n        \"granularity\": 3600,\n        \"groupby\": \"time\"\n    }\n\n#### having\n\n#### issues\n\n#### project\n\n#### sample\n\nSample is a numeric value. If it is < 1, then it is interpreted to mean \"read\nthis percentage of rows\". eg.\n\n    \"sample\": 0.5\n\nWill read 50% of all rows.\n\nIf it is > 1, it means \"read up to this number of rows\", eg.\n\n    \"sample\": 1000\n\nWill read 1000 rows maximum, and then return a result.\n\nNote that sampling does not do any adjustment/correction of aggregates. so if\nyou do a count() with 10% sampling, you should multiply the results by 10 to\nget an approximate value for the 'real' count. For sample > 1 you cannot do\nthis adjustment as there is no way to tell what percentage of rows were read.\nFor other aggregations like uniq(), min(), max(), there is no adjustment you\ncan do, the results will simply deviate more and more from the real value in an\nunpredictable way as the sampling rate approaches 0.\n\nQueries with sampling are stable. Ie the same query with the same sampling\nfactor over the same data should consistently return the exact same result.\n\n\n### Groups / Issues\n\nSnuba provides a magic column `group_id` that can be used to group events by issue.\n\nBecause events can be reassigned to different issues through merging, and\nbecause snuba does not support updates, we cannot store the issue id for an\nevent in snuba. If you want to filter or group by `group_id`, you need to pass a\nlist of `group_ids` into the query.  This list is a mapping from issue ids to the\nevent `primary_hash`es in that issue. Snuba automatically expands this mapping\ninto the query so that filters/grouping on `group_id` will just work.\n\n### Tags\n\nEvent tags are stored in one of 2 ways. Promoted tags are the ones we expect to\nbe queried often and as such are stored as top level columns. The list of\npromoted tag columns is defined in settings and is somewhat fixed in the\nschema. The rest of an event's tags are stored as a key-value map.  In practice\nthis is implemented as 2 columns of type Array(String), called `tags.key` and\n`tags.value`\n\nThe snuba service provides 2 mechanisms for abstracting this tiered tag\nstructure by providing some special columns that will be resolved to the\ncorrect SQL expression for the type of tag. These mechanisms should generally\nnot be used in conjunction with each other.\n\n#### When you know the names of the tags you want.\n\nYou can use the `tags[name]` anywhere you would use a normal column name in an\nexpression, and it will resolve to the value of the tag with `name`, regardless\nof whether that tag is promoted or not. Use this syntax when you are looking\nfor a specific named tag. eg.\n\n    # Find all events in production with user_custom_key defined.\n    \"conditions\": [\n        [\"tags[environment]\", \"=\", \"prod\"],\n        [\"tags[custom_user_tag]\", \"IS NOT NULL\"]\n    ],\n<!-- -->\n\n    # Find the number of unique environments\n    \"aggregations\": [\n        [\"uniq\", \"tags[environment]\", \"unique_envs\"],\n    ],\n\n#### When you don't know the name, or want to query all tags.\n\nThese are virtual columns that can be used to get results when the names of the\ntags are not explicitly known. Using `tags_key` or `tags_value` in an\nexpression will expand all of the promoted and non-promoted tags so that there\nis one row per tag (an array-join in Clickhouse terms). For each row, the name\nof the tag will be in the `tags_key` column, and the value in the `tags_value`\ncolumn.\n\n    # Find the top 5 most often used tags\n    \"aggregations\": [\n        [\"top5\", \"tags_key\", \"top_tag_keys\"],\n    ],\n<!-- -->\n\n    # Find any tags whose *value* is `bar`\n    \"conditions\": [\n        [\"tags_value\", \"=\", \"bar\"],\n    ],\n\n\nNote, when using this expression. the thing you are counting is tags, not events, so if you\nhave 10 events, each of which has 10 tags, then a `count()` of `tags_key` will return 100.\n",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
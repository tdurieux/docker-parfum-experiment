{
  "user": "mw55",
  "name": "natrix",
  "namespace": "mw55",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "Open-source bioinformatics pipeline for the preprocessing of raw sequencing data.",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 161,
  "last_updated": "2021-05-03T14:35:07.088129Z",
  "date_registered": "2020-10-18T11:56:58.291297Z",
  "collaborator_count": 1,
  "affiliation": null,
  "hub_user": "mw55",
  "has_starred": false,
  "full_description": "<p align=\"center\"> \n<img src=\"documentation/images/logo.png\" alt=\"logo\" width=\"500\"/>\n</p>\n\nNatrix is an open-source bioinformatics pipeline for the preprocessing of raw sequencing data.\nThe need for a scalable, reproducible workflow for the processing of environmental amplicon data led to the development of Natrix. It is divided into quality assessment, read assembly, dereplication, chimera detection, split-sample merging, ASV or OTU-generation and taxonomic assessment. The pipeline is written in [Snakemake](https://snakemake.readthedocs.io) (KÃ¶ster and Rahmann 2018), a workflow management engine for the development of data analysis workflows. Snakemake ensures reproducibility of a workflow by automatically deploying dependencies of workflow steps (rules) and scales seamlessly to different computing environments like servers, computer clusters or cloud services. While Natrix was only tested with 16S and 18S amplicon data, it should also work for other kinds of sequencing data. The pipeline contains seperate rules for each step of the pipeline and each rule that has additional dependencies has a seperate [conda](https://conda.io/) environment that will be automatically created when starting the pipeline for the first time. The encapsulation of rules and their dependencies allows for hassle-free sharing of rules between workflows.\n\n![DAG of an example workflow](documentation/images/combined_graph_4.png)\n*DAG of an example workflow: each node represents a rule instance to be executed. The direction of each edge represents the order in which the rules are executed, which dashed lines showing rules that are exclusive to the OTU version and dotted lines rules exclusive to the ASV variant of the workflow. Disjoint paths in the DAG can be executed in parallel. Below is a schematic representation of the main steps of the pipeline, the color coding represents which rules belong to which main step.*\n\nIf you use Natrix, please cite:\nWelzel, M., Lange, A., Heider, D. et al. Natrix: a Snakemake-based workflow for processing, clustering, and taxonomically assigning amplicon sequencing reads. BMC Bioinformatics 21, 526 (2020). https://doi.org/10.1186/s12859-020-03852-4\n\n---\n\n## Dependencies\n* [Conda](https://conda.io/en/latest/index.html)\n* [GNU screen](https://www.gnu.org/software/screen/) (optional)\n\nConda can be downloaded as part of the [Anaconda](https://www.anaconda.com/) or the [Miniconda](https://conda.io/en/latest/miniconda.html) plattforms (Python 3.7). We recommend to install miniconda3. \nUsing Linux you can get it with:\n\n```shell\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n$ bash Miniconda3-latest-Linux-x86_64.sh\n```\n\nGNU screen can be found in the repositories of most Linux distributions:\n\n* Debian / Ubuntu based: apt-get install screen\n* RHEL based: yum install screen\n* Arch based: pacman -S screen\n\nAll other dependencies will be automatically installed using conda environments and can be found in the corresponding environment.yaml files in the *envs* folder and the `natrix.yaml` file in the root directory of the pipeline.\n\n---\n\n## Getting Started\n\nTo install Natrix, you'll need the open-source package management system [conda](https://conda.io/en/latest/index.html) and, if you want to try Natrix using the accompanying `pipeline.sh` script you'll need [GNU screen](https://www.gnu.org/software/screen/).\nAfter cloning this repository to a folder of your choice, it is recommended to create a general natrix conda environment with the accompanying `natrix.yaml`. In the main folder of the cloned repository, execute the following command:\n\n```shell\n$ conda env create -f natrix.yaml\n```\nThis will create a conda environment containing all dependencies for Snakemake itself. \n\nWith Natrix comes an example [primertable](#Example-primertable) *example_data.csv*, [configfile](#Configfile) *example_data.yaml* and an example amplicon dataset in the folder *example_data*.\n\nTo try out Natrix using the example data, type in the following command:\n\n```shell\n$ ./pipeline.sh\nPlease enter the name of the project\n$ example_data\n```\nThe pipeline will then start a screen session using the project name (here, *example_data*) as session name and will beginn downloading dependencies for the rules. To detach from the screen session, press **Ctrl+a, d** (*first press Ctrl+a and then d*). To reattach to a running screen session, type in:\n\n```shell\n$ screen -r\n```\n\nWhen the workflow has finished, you can press **Ctrl+a, k** (*first press Ctrl+a and then k*). This will end the screen session and any processes that are still running.\n\n---\n\n## Tutorial\n\n### Prerequisites: dataset, primertable and configuration file\nThe FASTQ files need to follow a specific naming convention:\n<p align=\"center\"> \n<img src=\"documentation/images/filename.png\" alt=\"naming\" width=\"400\"/>\n</p>\n\n```\nsamplename_unit_direction.fastq.gz\n```\nwith:\n* *samplename* as the name of the sample, without special characters.\n* *unit*, identifier for [split-samples](#AmpliconDuo-/-Split-sample-approach) (*A*, *B*). If the split-sample approach is not used, the unit identifier is simply *A*.\n* *direction*, identifier for forward (*R1*) and reverse (*R2*) reads of the same sample. If the reads are single-end, the direction identifier is *R1*.\n\nA dataset should look like this (two samples, paired-end, no split-sample approach):\n```\nS2016RU_A_R1.fastq.gz\nS2016RU_A_R2.fastq.gz\nS2016BY_A_R1.fastq.gz\nS2016BY_A_R2.fastq.gz\n```\nBesides the FASTQ data from the sequencing process Natrix needs a [primertable](#Example-primertable) containing the sample names and, if they exists in the data, the length of the poly-N tails, the sequence of the primers and the barcodes used for each sample and direction. Besides the sample names all other information can be omitted if the data was already preprocessed or did not contain the corresponding subsequence. Natrix also needs a [configuration](#Configfile) file in YAML format, specifying parameter values for tools used in the pipeline.\n\nThe primertable, configfile and the folder containing the FASTQ files all have to be in the root directory of the pipeline and have the same name (with their corresponding file extensions, so *project*.yaml, *project*.csv and the *project* folder containing the FASTQ files). The first [configfile](#Configfile) entry (`filename`) also needs to be the name of the project.\n\n### Running Natrix with the `pipeline.sh` script\n\nIF everything is configured correctly, you can start the pipeline by typing in the following commands into your terminal emulator:\n\n```shell\n$ ./pipeline.sh\nPlease enter the name of the project\n$ example_data\n```\n\nThe pipeline will then start a screen session using the project name as session name and will beginn downloading dependencies for the rules. To detach from the screen session, press **Ctrl+a, d** (*first press Ctrl+a and then d*). To reattach to a running screen session, type in:\n\n```shell\n$ screen -r\n```\n\nWhen the workflow has finished, you can press **Ctrl+a, k** (*first press Ctrl+a and then k*). This will end the screen session and any processes that are still running.\n\n### Running Natrix with Docker or docker-compose\n#### Pulling the image from Dockerhub\nNatrix can be run inside a Docker-container. Therefore, Docker has to be installed. Please have a look at the [Docker website](https://docs.docker.com/) to find out how to install Docker and set up an environment if you have not used it before.\n\nThe easiest way to run the docker container is to download the pre-build container from [dockerhub](https://hub.docker.com/r/mw55/natrix).\n```shell\n$ docker pull mw55/natrix\n```\nThe docker container has all environments pre-installed, eliminating the need for downloading the environments during first-time initialization of the workflow.\nTo connect to the shell inside the docker container, input the following command:\n```shell\ndocker run -it --label natrix_container -v </host/database>:/app/database -v </host/results>:/app/results -v </host/input_folder>:/app/input -v </host/demultiplexed>:/app/demultiplexed mw55/natrix bash\n```\n*/host/database* is the full path to a local folder, in which you wish to install the database (SILVA or NCBI). This part is optional and only needed if \nyou want to use BLAST for taxonomic assignment.\n\n*/host/results* is the full path to a local folder in which the results of the workflow should be stored for the container to use.\n\n*/host/input_folder* is the full path to a local folder in which the input (the *project* folder, the *project*.yaml and the *project*.csv) should be saved.\n\n*/host/demultiplexed* is the full path to a local folder in which the demultiplexed data, or, if demultiplexing is turned off, the input data will be saved.\n\n\nAfter you connected to the container shell, you can follow the [running Natrix manually](#running-natrix-manually) tutorial.\n\n#### Directly starting the workflow using docker-compose\n\nAlternatively, you can start the workflow using the the docker-compose command in the root directory of the workflow (it will pull the latest natrix image from DockerHub):\n```shell\n$ PROJECT_NAME=\"<project>\" docker-compose up (-d)\n```\nwith *project* being the name of your project. e.g.:\n```shell\n$ PROJECT_NAME=\"example_data\" docker-compose up # sudo might be needed\n```\n\nall output folders will be available at /srv/docker/natrix/\nmake sure to copy your *project* folder, *project*.yaml and *project*.csv files to /srv/docker/natrix/input/ or create a new volume-mapping using the docker-compose.yml file.\nBy default the container will wait until the input files exist.\nAt first launch the container will download the required databases to /srv/docker/natrix/databases/, this process might take a while.\n\n#### Building the container yourself\nIf you prefer to build the docker container yourself from the repository (for example, if you modified the source code of Natrix) the container can be build and started directly: (*host* folders have to be changed!)\n\n```shell\ndocker build . --tag natrix\ndocker run -it --label natrix_container -v </host/database>:/app/database -v </host/results>:/app/results -v </host/input_folder>:/app/input -v </host/demultiplexed>:/app/demultiplexed natrix bash # -v /host/database:/app/database is optional\n```\n\nYou will then be at the command prompt inside the docker container, from there you can follow the tutorial for [running Natrix manually](#running-natrix-manually).\n\n### Running Natrix manually\n\nIf you prefer to run the preperation script and snakemake manually, you have to start by activating the snakemake environment:\n\n```shell\n$ conda activate natrix\n```\n\nFollowed by running the preperation script, with *project* being the name of your project:\n\n```shell\n$ python3 create_dataframe.py <project>.yaml\n```\n\nThis command will create the `units.tsv` file, containing the file information in a way that Natrix can use it.\n\nTo start the main pipeline, type in:\n```shell\nsnakemake --use-conda --configfile <project>.yaml --cores <cores>\n```\nwith *project* being the name of your project and *cores* being the amount of cores you want to allocate for Natrix to use.\n\nShould the pipeline prematurely terminate (either because of an error or by deliberatly stopping it) running the command above again will start the pipeline from the point it was terminated.\n\n---\n\n## Cluster execution\nNatrix can be run easily on a cluster system using either conda or the docker container.\nAdding --cluster to the start command of Natrix, together with a command to submit jobs (e. g. qsub) is enough for most \ncluster computing environments. An example command would be:\n\n```shell\n$ snakemake -s <path/to/Snakefile> --use-conda --configfile <path/to/configfile.yaml> --cluster \"qsub -N <project name> -S /bin/bash/\" --jobs 100\n```\n\nFurther qsub arguments including brief explanations can be found under [qsub arguments](http://bioinformatics.mdc-berlin.de/intro2UnixandSGE/sun_grid_engine_for_beginners/how_to_submit_a_job_using_qsub.html).\nFor additional commands that should be executed for each job the argument --jobscript *path/to/jobscript.sh* can be used. \nA simple jobscript that sources before the execution of each job the bashrc and activates the snakemake environment looks like this:\n\n```shell\n#!/usr/bin/env bash\n\nsource ~/.bashrc\nconda activate natrix\n\n{exec_job}\n```\n\nInstead of directly passing the cluster submission arguments to the snakemake command it is also possible to\nwrite a profile that contains cluster commands and arguments. The use of profiles allows the assignment \nof rule-specific hardware requirements. For example, the BLAST rule benefits from a large amount of CPU cores, while other\nrules, like AmpliconDuo, do not. With profiles, rules could be assigned a low amount of CPU cores as default, with rules\nlike BLAST being assigned a larger amount of cores. This allows optimal usage of cluster resources and shorter waiting times.\nThe creation of profiles is largely dependant on the software and hardware available on the cluster.\nWith a profile Natrix can simply be run with\n\n```shell\n$ snakemake -s <path/to/Snakefile> --profile myprofile \n```\n\nThe Snakemake documentation contains a tutorial for [profile creation](https://snakemake.readthedocs.io/en/stable/executing/cli.html#profiles) \nand the [Snakemake profiles GitHub page](https://github.com/snakemake-profiles/doc) contains example profiles for different\ncluster systems.\n\n## Output\n\nAfter the workflow is finished, the original data can be found under *Natrix-Pipeline/demultiplexed/*, while files created during the workflow can be found under *Natrix-Pipeline/results/*.\n<p align=\"center\"> \n<img src=\"documentation/images/output_files.png\" alt=\"ouput\" width=\"700\"/>\n</p>\n\n*Output file hierarchy, blue nodes represent folders, orange nodes represent files that are created in both variants of the workflow, green nodes are files exclusive to the OTU variant and purple nodes are files exclusive to the ASV variant of the workflow.*\n\n|Folder                               |File(s)                   |Description                                                                                              |\n|-------------------------------------|--------------------------|---------------------------------------------------------------------------------------------------------|\n|qc                                   |FastQC reports            |Quality reports of the FastQC application.                                                               |\n|                                     |MultiQC report            |Aggregated FastQC reports in a single file.                                                              |\n|logs                                 |Logfiles                  |Logfiles of the different rules.                                                                         |\n|assembly (one folder for each sample)|sample_low_qual.fastq     |Sequences of sample that did not pass the prinseq quality filtering.                                     |\n|                                     |sample_assembled.fastq    |With PANDAseq assembled sequences.                                                                       |\n|                                     |sample_singletons.fastq   |Sequences that could not be assembled.                                                                   |\n|                                     |sample.fasta              |FASTA file of the assembled sequences.                                                                   |\n|                                     |sample.dereplicated.fasta |Dereplicated sequences of sample                                                                         |\n|                                     |sample_chimera.fasta      |Sequences of sample that are thought to be of chimeric origin.                                           |\n|finalData                            |sample.nonchimera.fasta   |Sequences of sample that passed the chimera detection rule.                                              |\n|                                     |unfiltered_table.csv      |Table containing the sequences of all samples and their abundances per sample.                           |\n|                                     |filtered_table.csv        |Table containing the sequences of all samples and their abundances per sample after filtering.           |\n|                                     |filtered_out_table.csv    |Table containing the sequences that did not pass the filtering rule.                                     |\n|                                     |filtered.fasta            |The sequences of the filtered_table.csv in FASTA format.                                                 |\n|                                     |filtered_blast_table.csv  |Table containing the sequences of the filtered_table.csv and the taxonomic information assigned to each. |\n|figures                              |ampliconduo_unfiltered.png|Discordance graph before the filtering.                                                                  |\n|                                     |ampliconduo_filtered.png  |Discordance graph after filtering.                                                                       |\n|                                     |AmpliconDuo.Rdata         |RData file containing the results of the AmpliconDuo statistical analysis.                               |\n\n---\n\n# Steps of the Pipeline\n## Initial demultiplexing\nThe sorting of reads according to their barcode is known as demultiplexing.\n\n## Quality control\nFor quality control the pipeline uses the programs FastQC (Andrews 2010), MultiQC (Ewels\net al. 2016) and PRINSEQ (Schmieder and Edwards 2011).\n\n### FastQC \nFastQC generates a quality report for each FASTQ file, containing information such\nas the per base and average sequence quality (using the Phred quality score), overrepresented\nsequences, GC content, adapter and the k-mer content of the FASTQ file.\n\n### MultiQC \nMultiQC aggregates the FastQC reports for a given set of FASTQ files into a\nsingle report, allowing reviews of all FASTQ files at once. \n\n### PRINSEQ\nPRINSEQ is used to filter out sequences with an average quality score below\nthe threshold that can be defined in the configuration file of the pipeline.\n\n## Read assembly\n### Define primer\nThe define_primer rule specifies the subsequences to be removed by the\nassembly rule, specified by entries of the configuration file and a primer table that contains information about the primer and barcode sequences used and the length of the poly-N subsequences. Besides removing the subsequences based on their nucleotide sequence, it is also possible to remove them based solely on their length using an offset. Using an offset can be useful if the sequence has many uncalled bases in the primer region, which could otherwise hinder matches between the target sequence defined in the primer table and the sequence read.\n\n### Assembly & removal of undesired subsequences (OTU-variant)\nTo assemble paired-end reads and remove the subsequences described in the pre-\nvious section PANDAseq (Masella et al. 2012) is used, which uses a probabilistic error correction to assemble overlapping forward- and reverse-reads. After assembly and sequence trimming, it will remove sequences that do not meet a minimal or maximal length threshold, have an assembly quality score below a threshold that can be defined in the configuration file and sequences whose forward- and reverse-read do not have an sufficiently long overlap. The thresholds for each of these procedures can be adjusted in the configuration file. If the reads are single-end, the subsequences (poly-N, barcode and the primer) that are defined in the define_primer rule are removed, followed by the removal of sequences that do not meet a minimal or maximal length threshold as defined in the configuration file.\n\n### Removal of undesired subsequences (ASV-variant)\nIn the ASV variant of the workflow Cutadapt (Martin 2011) is used to remove the undesired subsequences defined in the primer table.\n\n### ASV denoising (ASV-Variant)\nAfter the removal of undesired subsequences ASVs are generated using the DADA2 (Callahan et al. 2016) algorithm. It dereplicates the dataset and uses a denoising algorithm. This algorithm infers if a sequence was produced by a different sequence based on the composition, quality, and abundance of a sequence and an Illumina error model. After ASV generation, exactly overlapping forward and reverse reads are assembled. The assembled ASVs are saved as FASTA files for downstream analysis.\n\n## Similarity clustering\n### Conversion of FASTQ files to FASTA files (OTU-variant)\nThe rule copy_to_fasta converts the FASTQ files to FASTA files to reduce the disc space occupied by the files and to allow the usage of CD-HIT, which requires FASTA formated sequencing files. Clustering of similar sequences The CD-HIT-EST algorithm (Fu et al. 2012) clusters sequences together if they are either identical or if a sequence is a subsequence of another. This clustering approach is known as dereplication. Beginning with the longest sequence of the dataset as the first representative sequence, it iterates through the dataset in order of\ndecreasing sequence length, comparing at each iteration the current query sequence to all representative sequences. If the sequence identity threshold defined in the configuration file is met for a representative sequence, the counter of the representative sequence is increased by one. If the threshold could not be met for any of the existing representative sequences, the query sequence is added to the pool of representative sequences. Cluster sorting The cluster_sorting rule uses the output of the cdhit rule to count the amount of sequences represented by each cluster, followed by sorting the representative sequences in descending order according to the cluster size and adds a specific header to each\nsequence as required by the UCHIME chimera detection algorithm.\n\n## Chimera detection\n### VSEARCH\nVSEARCH is a open-source alternative to the USEARCH toolkit, which aims to functionally replicate the algorithms used by USEARCH for which the source code is not openly available and which are often only rudimentarily described (Rognes et al. 2016). Natrix uses as an alternative to UCHIME the VSEARCH uchime3_denovo algorithm to detect chimeric sequences (further referred to as VSEARCH3). The VSEARCH3 algorithm is a replication of the UCHIME2 algorithm with optimized standard parameters. The UCHIME2 algorithm is described by R. Edgar 2016 as follows:\n\n\"Given a query sequence *Q*, UCHIME2 uses the UCHIME algorithm to construct a model\n(*M*), then makes a multiple alignment of *Q* with the model and top hit (*T*, the most similar reference sequence). The following metrics are calculated from the alignment: number of differences d<sub>QT</sub> between Q and T and d<sub>QM</sub> between *Q* and *M*, the alignment score (*H*) using eq. 2 in R. C. Edgar et al. 2011. The fractional divergence with respect to the top hit is calculated as div<sub>T</sub> = (d<sub>QT</sub> â d<sub>QM</sub>)/|Q|. If divT is large, the model is a much better match than the top hit and the query is more likely to be chimeric, and conversely if div<sub>T</sub> is small, the model is more likely to be a fake.\" \n\nThe difference between the UCHIME2 and UCHIME3 algorithm is that to be selected as a potential parent, a sequence needs to have at least 16 times the abundance of the query sequence in the UCHIME3 algorithm, while it only needs double the abundance of the query sequence to be selected as a potential parent in the UCHIME2 algorithm.\n\n## Table creation and filtering\n### Merging of all FASTA files into a single table\nFor further processing the rule unfiltered_table merges all FASTA files into a single, nested dictionary, containing each sequence as the key with another dictionary as value, whose keys are all (split -) samples in which the sequence occurred in and as values the abundance of the sequence in the particular (split - ) sample. For further pipeline processing the dictionary is temporarily saved in JSON format. To ease statistical analysis of the data the dictionary is also exported as a comma separated table. Filtering In the filtering rule of the pipeline all sequences that do not occur in both splitsamples of at least one sample are filtered out. For single-sample data, the filtering rule uses an abundance cutoff value that can be specified in the configuration file to filter out all sequences which have abundances less\nor equal the specified cutoff value. The filtered data and the filtered out data is subsequently exported as comma separated tables.\n\n### Table conversion to FASTA files\nAs the swarm rule needs FASTA files as input, the resulting table of the filtering is converted to a FASTA file by the rule write_fasta.\n\n## AmpliconDuo / Split-",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
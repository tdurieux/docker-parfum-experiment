{
  "user": "olitheolix",
  "name": "square",
  "namespace": "olitheolix",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 906,
  "last_updated": "2021-02-23T21:22:42.687433Z",
  "date_registered": "2019-02-04T06:25:39.710602Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "olitheolix",
  "has_starred": false,
  "full_description": "[![](https://img.shields.io/badge/license-Apache%202-blue.svg)]()\n[![](https://img.shields.io/badge/python-3.7+-blue.svg)]()\n[![](https://img.shields.io/badge/latest-v1.1.4-blue.svg)]()\n[![](https://github.com/olitheolix/square/workflows/build/badge.svg)]()\n[![](https://img.shields.io/codecov/c/github/olitheolix/square.svg?style=flat)]()\n\n\n*Square* is to Kubernetes what Terraform is to Cloud: match the cluster state\nto what the local manifests dictate.\n\nSquare is completely stateless. Unlike other tools it does not create resources\nlike ConfigMaps or inject special annotations to track state - the local\nmanifests are all there is.\n\n# Installation\nGrab a [binary release](https://github.com/olitheolix/square/releases) or\ninstall it into a Python 3.7+ environment with `pip install kubernetes-square\n--upgrade`.\n```console\nfoo@bar:~$ square version\n1.1.4\n```\n\nYou may also use a pre-built Docker image:\n```console\nfoo@bar:~$ docker run -ti --rm olitheolix/square:v1.1.4 version\n1.1.4\n```\n\n# Usage\nA sensible first step is to create the `.square.yaml` file with `square config`\nand edit it. The only two really important fields are `kubeconfig` and\n`folder`, which denote the location of `kubeconfig` and where to store the\nmanifests. You may also want to update `selectors.kinds`, `selectors.labels`\nand `selectors.namespaces` to target specific resource types with specific\nlabels in specific namespaces. All other options have sensible defaults.\n\nThe `.square.yaml` file is optional. All options in that file, except\n`filters`, can be passed via command line arguments.\n\nAfter that, the typical workflow to manage the resources specified in\n`.square.yaml` is:\n\n```console\n# Import resources from cluster (if you want to).\nsquare get\n\n# Show the deployment plan.\nsquare plan\n\n# Show the deployment plan and apply it.\nsquare apply\n```\n\n## Supported Clusters And Versions\n*Square* supports Minikube, Kubernetes in Docker (KinD), EKS and GKE. Any\ncluster version `v1.11+` should work.\n\n# Examples\nThese example assume that you have *no* `.square.yaml`.\n\n*Square* will use the `KUBECONFIG` environment variable to locate the\nKubernetes credentials. Alternatively, you can specify the credentials with the\n`--kubeconfig` and `--context` arguments.\n\n## Get Current Cluster State\nDownload all _Namespace_- and _Deployment_ manifests from the cluster and save\nthem to `./manifests`:\n\n```console\nfoo@bar:~$ kubectl apply -f integration-test-cluster/test-resources.yaml\n...\nfoo@bar:~$ square get ns deployment --groupby ns kind --folder manifests/\nfoo@bar:~$ tree manifests\nmanifests/\n├── default\n│   └── namespace.yaml\n├── _global_\n│   └── clusterrole.yaml\n├── kube-public\n│   └── namespace.yaml\n├── kube-system\n│   ├── deployment.yaml\n│   └── namespace.yaml\n└── square-tests\n    ├── deployment.yaml\n    └── namespace.yaml\n```\n\nThese are the YAML files from the [integration test\ncluster](integration-test-cluster) (a Minikube). The `--groupby` argument\ndetermine the layout of `manifests/`. In this case, each namespace becomes a\nfolder and the manifests are grouped by resource type. The only folder that\ndoes not correspond to a Kubernetes namespace is `_global_` because it harbours\nall non-namespaced resources like `ClusterRole` or `ClusterRoleBinding`.\n\nThe file names, as well as the manifest order inside those files are\nirrelevant. *Square* will always compile them into a flat list internally. As\nsuch, you are free to rename the files, or move manifests across to different\nfiles. You can still use `square get ...` afterwards and *Square* will update\nthe right resources in the right files. If it finds a resource on the server\nthat is not yet defined in any of the files it will create the corresponding\nfile.\n\n### Group By Label\n*Square* can also use _one_ resource label and make it part of the manifests\nfolder hierarchy. Here is the [integration test\ncluster](integration-test-cluster):\n\n```console\nfoo@bar:~$ kubectl apply -f integration-test-cluster/test-resources.yaml\n...\nfoo@bar:~$ square get --groupby ns label=app kind --folder manifests/\nfoo@bar:~$ tree manifests\nmanifests/\n├── default\n│   └── _other\n│       ├── namespace.yaml\n│       ├── secret.yaml\n│       ├── serviceaccount.yaml\n│       └── service.yaml\n├── _global_\n│   ├── demoapp\n│   │   ├── clusterrolebinding.yaml\n│   │   └── clusterrole.yaml\n│   └── _other\n│       ├── clusterrolebinding.yaml\n│       └── clusterrole.yaml\n├── kube-public\n│   └── _other\n│       ├── configmap.yaml\n│       ├── namespace.yaml\n│       ├── rolebinding.yaml\n│       ├── role.yaml\n│       └── serviceaccount.yaml\n├── kube-system\n│   ├── kube-proxy\n│   │   └── configmap.yaml\n│   └── _other\n│       ├── configmap.yaml\n│       ├── daemonset.yaml\n│       ├── deployment.yaml\n│       ├── namespace.yaml\n│       ├── rolebinding.yaml\n│       ├── role.yaml\n│       ├── secret.yaml\n│       ├── serviceaccount.yaml\n│       └── service.yaml\n└── square-tests\n    ├── demoapp\n    │   ├── configmap.yaml\n    │   ├── cronjob.yaml\n    │   ├── daemonset.yaml\n    │   ├── deployment.yaml\n    │   ├── horizontalpodautoscaler.yaml\n    │   ├── ingress.yaml\n    │   ├── namespace.yaml\n    │   ├── persistentvolumeclaim.yaml\n    │   ├── rolebinding.yaml\n    │   ├── role.yaml\n    │   ├── secret.yaml\n    │   ├── serviceaccount.yaml\n    │   ├── service.yaml\n    │   └── statefulset.yaml\n    └── _other\n        ├── secret.yaml\n        └── serviceaccount.yaml\n```\n\nAs you can see, *Square* co-located all resources that are in the same\nnamespace *and* have the same `app` label. Resources without an `app` label it\nput into the catch-all folder `_other` and non-namespaced resources into the\n`_global_` folder.\n\n## Create A Plan\nFollowing on with the example, the local files and the cluster state\nare now in sync:\n\n```console\nfoo@bar:~$ square plan ns\n--------------------------------------------------------------------------------\nPlan: 0 to add, 0 to change, 0 to destroy.\n```\n\nTo make this more interesting, add a label to the _Namespace_ manifest in\n`square-tests/demoapp/namespace.yaml`. It should look something like this:\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: default\n  labels:\n    foo: bar\nspec:\n  finalizers:\n  - kubernetes\n```\n\nSave the file and create a plan:\n```console\nfoo@bar:~$ square plan ns\nPatch NAMESPACE default/default\n    ---\n    +++\n    @@ -1,6 +1,8 @@\n     apiVersion: v1\n     kind: Namespace\n     metadata:\n    +  labels:\n    +    foo: bar\n       name: default\n     spec:\n       finalizers:\n--------------------------------------------------------------------------------\nPlan: 0 to add, 1 to change, 0 to destroy.\n```\n\nThis will show the difference in standard `diff` format. In words: *Square*\nwould patch the `default` namespace to bring the K8s cluster into the state\nprescribed by the local manifests. Let's apply the plan to do just that:\n\n```console\nfoo@bar:~$ square apply ns\nPatch NAMESPACE default/default\n    ---\n    +++\n    @@ -1,6 +1,8 @@\n     apiVersion: v1\n     kind: Namespace\n     metadata:\n    +  labels:\n    +    foo: bar\n       name: default\n     spec:\n       finalizers:\n\nCompiled 1 patches.\nPatch(url='https://192.168.0.177:8443/api/v1/namespaces/default', ops=[{'op': 'add', 'path': '/metadata/labels', 'value': {'foo': 'bar'}}])\n\nfoo@bar:~$ square plan ns\n--------------------------------------------------------------------------------\nPlan: 0 to add, 0 to change, 0 to destroy.\n```\n\n*Square* will first print the  *diff* we saw earlier already, followed by the\nJSON patch it sent to K8s to update the _Namespace_.\n\nUse *kubectl* to ensure the patch worked and the Namespace now has a `foo:bar` label.\n\n```console\nfoo@bar:~$ kubectl describe ns default\nName:         default\nLabels:       foo=bar\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n\n```\n\n## Apply The Plan To Create and Destroy Resources\nThe `apply` operation we just saw will also create and delete resources as\nnecessary. To add a new resource, simply add its manifest to `manifests/`. It\ndoes not matter if it is in a new file or added to an existing one.\n\nFor instance, to deploy the latest *Square* image from\n[Dockerhub](https://hub.docker.com/r/olitheolix/square), download the [example\nmanifests](examples/square.yaml) into the `manifests/` folder and use *Square*\nto deploy it:\n\n```console\nfoo@bar:~$ wget https://github.com/olitheolix/square/raw/master/examples/square.yaml -O manifests/square.yaml\nfoo@bar:~$ square apply all\nCreate NAMESPACE square/square\n    apiVersion: v1\n    kind: Namespace\n    metadata:\n      name: square\n\nCreate SERVICEACCOUNT square/square\n    apiVersion: v1\n    kind: ServiceAccount\n    metadata:\n      name: square\n      namespace: square\n\nCreate CLUSTERROLE None/square\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: ClusterRole\n    metadata:\n      name: square\n    rules:\n    - apiGroups:\n      - ''\n      - apps\n      - rbac.authorization.k8s.io\n      - extensions\n      resources:\n      - clusterrolebindings\n      - clusterroles\n      - configmaps\n      - daemonsets\n      - deployments\n      - ingresses\n      - namespaces\n      - persistentvolumeclaims\n      - rolebindings\n      - roles\n      - secrets\n      - services\n      - statefulsets\n      verbs:\n      - get\n      - list\n      - update\n      - patch\n\nCreate CLUSTERROLEBINDING None/square\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: ClusterRoleBinding\n    metadata:\n      name: square\n    roleRef:\n      apiGroup: rbac.authorization.k8s.io\n      kind: ClusterRole\n      name: square\n    subjects:\n    - kind: ServiceAccount\n      name: square\n      namespace: square\n\nCreate DEPLOYMENT square/square\n    apiVersion: extensions/v1beta1\n    kind: Deployment\n    metadata:\n      name: square\n      namespace: square\n    spec:\n      replicas: 1\n      selector:\n        matchLabels:\n          app: square\n      template:\n        metadata:\n          labels:\n            app: square\n        spec:\n          containers:\n          - command:\n            - sleep\n            - 10000d\n            image: olitheolix/square:latest\n            imagePullPolicy: Always\n            name: square\n          serviceAccountName: square\n          terminationGracePeriodSeconds: 1\n\nCreating NAMESPACE square/square\nCreating SERVICEACCOUNT square/square\nCreating CLUSTERROLE None/square\nCreating CLUSTERROLEBINDING None/square\nCreating DEPLOYMENT square/square\nCompiled 0 patches.\n\nfoo@bar:~$ kubectl -n square get po\nNAME                     READY   STATUS    RESTARTS   AGE\nsquare-b6bc65f6d-2xmzm   1/1     Running   0          37s\n```\n\n# Deploy On A Cluster\n*Square* does not require anything installed on your cluster to work. However,\nit will require the appropriate RBACs if you want to run it in a Pod. The\n[examples folder](examples) contains an example of how to deploy the\n[official Docker image](https://hub.docker.com/r/olitheolix/square).\n\nThis can be useful for automation tasks. For instance, you may want to\ntrack the configuration drift in your cluster over time.\n\n# Use It As A Library\nYou can also use *Square* as a library in your own projects. See\n[here](examples/as_library.py) for an example.\n\n# Automated Tests\n*Square* ships with a comprehensive set of unit tests:\n\n    pipenv run pytest\n\nTo also pick up the integration tests you need to first download the [KinD\nbinary](https://github.com/kubernetes-sigs/kind/releases) for your platform and\nput it in your path. Then start the integration test cluster with:\n\n    cd integration-test-cluster\n    ./start_cluster.sh\n",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
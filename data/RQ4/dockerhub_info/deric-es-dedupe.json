{
  "user": "deric",
  "name": "es-dedupe",
  "namespace": "deric",
  "repository_type": "image",
  "status": 1,
  "status_description": "active",
  "description": "A tool to remove duplicate documents from Elastic",
  "is_private": false,
  "is_automated": false,
  "can_edit": false,
  "star_count": 0,
  "pull_count": 1215,
  "last_updated": "2021-09-29T08:17:25.985703Z",
  "date_registered": "2018-05-02T19:23:04.814108Z",
  "collaborator_count": 0,
  "affiliation": null,
  "hub_user": "deric",
  "has_starred": false,
  "full_description": "# ES-dedupe\n\n[![](https://images.microbadger.com/badges/version/deric/es-dedupe.svg)](https://microbadger.com/images/deric/es-dedupe)\n[![](https://images.microbadger.com/badges/image/deric/es-dedupe.svg)](https://microbadger.com/images/deric/es-dedupe)\n\nA tool for removing duplicated documents that are grouped by some unique field (e.g. `--field Uuid`). Removal process consists of two phases:\n\n 1. Aggregate query find documents that have same `field` value and at least 2 occurences. One copy of such document is left in ES all other are deleted via Bulk API (almost all, usually - there's always some catch). We wait for index update after each `DELETE` operatation. Processed documents are logged into `/tmp/es_dedupe.log`.\n 2. Unfortunately aggregate queries are not necessarily exact. Based on `/tmp/es_dedupe.log` logfile we query for each `field` value and DELETE document copies on other shards. Depending on number of nodes and shards in cluster there might be still document that aggregate query didn't return. In order to disable 2nd step use `--no-check` flag.\n\n## Docker\n\nRunning from Docker:\n```\ndocker run -it -e ES=locahost -e INDEX=my-index -e FIELD=id deric/es-dedupe:latest\n```\nYou can either override Docker command or use ENV variable to pass arguments.\n\n## Usage\nUse `-h/--help` to see supported options:\n```\ndocker run --rm deric/es-dedupe:latest dedupe --help\n```\n\n```\ndocker run --rm deric/es-dedupe:latest dedupe -H localhost -P 9200 -i exact-index-name -f Uuid > es_dedupe.log 2>&1\n```\nwill try to find duplicated documents in an index called 'exact-index-name' where documents are grouped by `Uuid` field.\n\n```\ndocker run --rm deric/es-dedupe:latest dedupe -H localhost -P 9200 --all --prefix 'esindexprefix' --prefixseparator '-' --indexexclude '^excludedindex.*' -f fingerprint > es_dedupe.log 2>&1\n```\nwill try to find duplicated documents in all indices known to the ES instance on localhost:9200, that look akin to 'esindexprefix-\\*' while excluding all indices starting with 'excludedindex', where documents are grouped by `fingerprint` field.\n\n * `-a` will process all indexes known to the ES instance that match the prefix and prefixseparator.\n * `-b` batch size - critical for performance ES queries might take several minutes, depending on size of your indexes\n * `-f` name of field that should be unique\n * `-h` displays help\n * `-m` number of duplicated documents with same unique field value\n * `-t` document type in ES\n * `--sleep 60` time between aggregation requests (gives ES time to run GC on heap), 15 seconds seems to be enough to avoid triggering ES flood protection though.\n\nWARNING: Running huge bulk operations on ES cluster might influence performance of your cluster or even crash some nodes if heap\nis not large enough. Increment `-b` and `-m` parameters with caution! ES returns at most `b * m` documents, eventually you might hit\nmaximum POST request size with bulk requests.\n\nA log file containing documents with unique fields is written into `/tmp/es_dedupe.log`.\n\nBy design ES aggregate queries are not necessarily precise. Depending on your cluster setup, some documents won't be deleted due to\ninaccurate shard statistics.\n\n`--check_log` will query for documents found by aggregate and queries check whether were actually deleted.\n```\ndocker run --rm deric/es-dedupe:latest dedupe --check_log /tmp/es_dedupe.log --noop\n```\n\n```\n== Starting ES deduplicator....\nPRETENDING to delete:\n{\"delete\":{\"_index\":\"nginx_access_logs-2017.03.17\",\"_type\":\"nginx.access\",\"_id\":\"AVrdoYEJy1wo8jcgI7t5\"}}\n\n== Total. OK: 4 (80.00%) out of 5. Fixable: 1. Missing: 0\nQueried for 5 documents, retrieved status of 5 (100.00%).\n```\n\n## Performance\n\nMost time is spent on ES queries, choose `--batch` and `--max_dupes` size wisely! Between each bulk request script sleeps for `--sleep {seconds}`.\n\nDelete queries are send via `_bulk` API. Processing batch with several thousand documents takes several seconds:\n```\n== Starting ES deduplicator....\nUsing index nginx_access_logs-2017.03.17\nES query took 0:00:27.552083, retrieved 0 unique docs\nUsing index nginx_access_logs-2017.03.18\nES query took 0:01:30.705209, retrieved 10000 unique docs\nDeleted 333129 duplicates, in total 333129. Batch processed in 0:00:09.999008, running time 0:02:08.259759\nES query took 0:01:58.288188, retrieved 10000 unique docs\nDeleted 276673 duplicates, in total 609802. Batch processed in 0:00:08.487847, running time 0:04:16.037037\n```\n\n## Requirements\nFor the installation  use the tools provided by your operating system.\n\nOn Linux   this can be one of the following:  yum, dnf, apt, yast, emerge, ..\n\n* Install python (2 or 3, both will work)\n* Install python*ujson and python*requests for the fitting python version\n\n\nOn Windows you are pretty much on your own, but fear not, you can do the following ;-)\n\n* Download and install a python version from https://www.python.org/ .\n* Open a console terminal and head to the repository copy of es-deduplicator, then run:\npip install -r requirements.txt\n\n\n## History\n\nOriginally written in bash which performed terribly due to slow JSON processing with pipes and `jq`. Python with `ujson` seems to be better fitted for this task.\n",
  "permissions": {
    "read": true,
    "write": false,
    "admin": false
  },
  "media_types": [
    "application/vnd.docker.container.image.v1+json"
  ],
  "content_types": [
    "image"
  ]
}
{
  "startTime": 1674243899468,
  "endTime": 1674243900086,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 62,
        "lineEnd": 62,
        "columnStart": 4,
        "columnEnd": 50
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 188,
        "lineEnd": 188,
        "columnStart": 7,
        "columnEnd": 50
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 199,
        "lineEnd": 199,
        "columnStart": 7,
        "columnEnd": 46
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 212,
        "lineEnd": 212,
        "columnStart": 7,
        "columnEnd": 60
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 278,
        "lineEnd": 278,
        "columnStart": 4,
        "columnEnd": 45
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 302,
        "lineEnd": 302,
        "columnStart": 4,
        "columnEnd": 57
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 341,
        "lineEnd": 341,
        "columnStart": 8,
        "columnEnd": 46
      }
    },
    {
      "rule": "gpgUseBatchFlag",
      "position": {
        "lineStart": 100,
        "lineEnd": 100,
        "columnStart": 10,
        "columnEnd": 61
      }
    },
    {
      "rule": "gpgUseBatchFlag",
      "position": {
        "lineStart": 102,
        "lineEnd": 102,
        "columnStart": 7,
        "columnEnd": 28
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# WARNING: THIS DOCKERFILE IS NOT INTENDED FOR PRODUCTION USE OR DEPLOYMENT.\n#\n# Base image for the whole Docker file\nARG APT_DEPS_IMAGE=\"airflow-apt-deps\"\nARG PYTHON_BASE_IMAGE=\"python:3.6-slim\"\n############################################################################################################\n# This is the base image with APT dependencies needed by Airflow. It is based on a python slim image\n# Parameters:\n#    PYTHON_BASE_IMAGE - base python image (python:x.y-slim)\n############################################################################################################\nFROM ${PYTHON_BASE_IMAGE} as airflow-apt-deps\n\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\n\n# Need to repeat the empty argument here otherwise it will not be set for this stage\n# But the default value carries from the one set before FROM\nARG PYTHON_BASE_IMAGE\nENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE}\n\nARG AIRFLOW_VERSION=\"2.0.0.dev0\"\nENV AIRFLOW_VERSION=$AIRFLOW_VERSION\n\n# Print versions\nRUN echo \"Base image: ${PYTHON_BASE_IMAGE}\"\nRUN echo \"Airflow version: ${AIRFLOW_VERSION}\"\n\n# Make sure noninteractie debian install is used and language variables set\nENV DEBIAN_FRONTEND=noninteractive LANGUAGE=C.UTF-8 LANG=C.UTF-8 LC_ALL=C.UTF-8 \\\n    LC_CTYPE=C.UTF-8 LC_MESSAGES=C.UTF-8\n\n# By increasing this number we can do force build of all dependencies\nARG DEPENDENCIES_EPOCH_NUMBER=\"1\"\n# Increase the value below to force renstalling of all dependencies\nENV DEPENDENCIES_EPOCH_NUMBER=${DEPENDENCIES_EPOCH_NUMBER}\n\n# Install curl and gnupg2 - needed to download nodejs in the next step\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n           curl \\\n           gnupg2 \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n\n# Install basic apt dependencies\nRUN curl -f -sL https://deb.nodesource.com/setup_10.x | bash - \\\n    && apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n           apt-utils \\\n           build-essential \\\n           curl \\\n           dirmngr \\\n           freetds-bin \\\n           freetds-dev \\\n           git \\\n           gosu \\\n           libffi-dev \\\n           libkrb5-dev \\\n           libpq-dev \\\n           libsasl2-2 \\\n           libsasl2-dev \\\n           libsasl2-modules \\\n           libssl-dev \\\n           locales  \\\n           netcat \\\n           nodejs \\\n           rsync \\\n           sasl2-bin \\\n           sudo \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Install MySQL client from Oracle repositories (Debian installs mariadb)\nRUN KEY=\"A4A9406876FCBD3C456770C88C718D3B5072E1F5\" \\\n    && GNUPGHOME=\"$(mktemp -d)\" \\\n    && export GNUPGHOME \\\n    && for KEYSERVER in $(shuf -e \\\n            ha.pool.sks-keyservers.net \\\n            hkp://p80.pool.sks-keyservers.net:80 \\\n            keyserver.ubuntu.com \\\n            hkp://keyserver.ubuntu.com:80 \\\n            pgp.mit.edu); do \\\n          gpg --batch --keyserver \"${KEYSERVER}\" --recv-keys \"${KEY}\" && break || true; \\\n       done \\\n    && gpg --batch --export \"${KEY}\" | apt-key add - \\\n    && gpgconf --kill all \\\n    rm -rf \"${GNUPGHOME}\"; \\\n    apt-key list > /dev/null \\\n    && echo \"deb http://repo.mysql.com/apt/ubuntu/ trusty mysql-5.6\" | tee -a /etc/apt/sources.list.d/mysql.list \\\n    && apt-get update \\\n    && apt-get install --no-install-recommends -y \\\n        libmysqlclient-dev \\\n        mysql-client \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean && rm -rf /var/lib/apt/lists/*\n\nRUN adduser airflow \\\n    && echo \"airflow ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/airflow \\\n    && chmod 0440 /etc/sudoers.d/airflow\n\n############################################################################################################\n# This is an image with all APT dependencies needed by CI. It is built on top of the airlfow APT image\n# Parameters:\n#     airflow-apt-deps - this is the base image for CI deps image.\n############################################################################################################\nFROM airflow-apt-deps as airflow-ci-apt-deps\n\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\n\nENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/\n\nARG APT_DEPS_IMAGE\nENV APT_DEPS_IMAGE=${APT_DEPS_IMAGE}\n\nRUN echo \"${APT_DEPS_IMAGE}\"\n\n# Note the ifs below might be removed if Buildkit will become usable. It should skip building this\n# image automatically if it is not used. For now we still go through all layers below but they are empty\nRUN if [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]]; then \\\n        # Note missing man directories on debian-stretch\n        # https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=863199\n        mkdir -pv /usr/share/man/man1 \\\n        && mkdir -pv /usr/share/man/man7 \\\n        && apt-get update \\\n        && apt-get install --no-install-recommends -y \\\n          gnupg \\\n          krb5-user \\\n          ldap-utils \\\n          less \\\n          lsb-release \\\n          net-tools \\\n          openjdk-8-jdk \\\n          openssh-client \\\n          openssh-server \\\n          postgresql-client \\\n          python-selinux \\\n          sqlite3 \\\n          tmux \\\n          unzip \\\n          vim \\\n        && apt-get autoremove -yqq --purge \\\n        && apt-get clean \\\n        && rm -rf /var/lib/apt/lists/* \\\n        ;\\\n    fi\n\nENV HADOOP_DISTRO=cdh \\\n    HADOOP_MAJOR=5 \\\n    HADOOP_DISTRO_VERSION=5.11.0 \\\n    HADOOP_VERSION=2.6.0 \\\n    HIVE_VERSION=1.1.0\nENV HADOOP_URL=https://archive.cloudera.com/${HADOOP_DISTRO}${HADOOP_MAJOR}/${HADOOP_DISTRO}/${HADOOP_MAJOR}/\nENV HADOOP_HOME=/tmp/hadoop-cdh HIVE_HOME=/tmp/hive\n\nRUN \\\nif [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]]; then \\\n    mkdir -pv ${HADOOP_HOME} \\\n    && mkdir -pv ${HIVE_HOME} \\\n    && mkdir /tmp/minicluster \\\n    && mkdir -pv /user/hive/warehouse \\\n    && chmod -R 777 ${HIVE_HOME} \\\n    && chmod -R 777 /user/ \\\n    ;\\\nfi\n# Install Hadoop\n# --absolute-names is a work around to avoid this issue https://github.com/docker/hub-feedback/issues/727\nRUN \\\nif [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]]; then \\\n    HADOOP_URL=${HADOOP_URL}hadoop-${HADOOP_VERSION}-${HADOOP_DISTRO}${HADOOP_DISTRO_VERSION}.tar.gz \\\n    && HADOOP_TMP_FILE=/tmp/hadoop.tar.gz \\\n    && curl -f -sL ${HADOOP_URL} > ${HADOOP_TMP_FILE} \\\n    && tar xzf ${HADOOP_TMP_FILE} --absolute-names --strip-components 1 -C ${HADOOP_HOME} \\\n    && rm ${HADOOP_TMP_FILE} \\\n    ; \\\nfi\n\n# Install Hive\nRUN \\\nif [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]]; then \\\n    HIVE_URL=${HADOOP_URL}hive-${HIVE_VERSION}-${HADOOP_DISTRO}${HADOOP_DISTRO_VERSION}.tar.gz \\\n    && HIVE_TMP_FILE=/tmp/hive.tar.gz \\\n    && curl -f -sL ${HIVE_URL} > ${HIVE_TMP_FILE} \\\n    && tar xzf ${HIVE_TMP_FILE} --strip-components 1 -C ${HIVE_HOME} \\\n    && rm ${HIVE_TMP_FILE} \\\n    ; \\\nfi\n\nENV MINICLUSTER_URL=https://github.com/bolkedebruin/minicluster/releases/download/\nENV MINICLUSTER_VER=1.1\n# Install MiniCluster TODO: install it differently. Installing to /tmp is probably a bad idea\nRUN \\\nif [[ \"${APT_DEPS_IMAGE}\" == \"airflow-ci-apt-deps\" ]]; then \\\n    MINICLUSTER_URL=${MINICLUSTER_URL}${MINICLUSTER_VER}/minicluster-${MINICLUSTER_VER}-SNAPSHOT-bin.zip \\\n    && MINICLUSTER_TMP_FILE=/tmp/minicluster.zip \\\n    && curl -f -sL ${MINICLUSTER_URL} > ${MINICLUSTER_TMP_FILE} \\\n    && unzip ${MINICLUSTER_TMP_FILE} -d /tmp \\\n    && rm ${MINICLUSTER_TMP_FILE} \\\n    ; \\\nfi\n\nENV PATH \"${PATH}:/tmp/hive/bin\"\n\n############################################################################################################\n# This is the target image - it installs PIP and NPM dependencies including efficient caching\n# mechanisms - it might be used to build the bare airflow build or CI build\n# Parameters:\n#    APT_DEPS_IMAGE - image with APT dependencies. It might either be base deps image with airflow\n#                     dependencies or CI deps image that contains also CI-required dependencies\n############################################################################################################\nFROM ${APT_DEPS_IMAGE} as main\n\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-e\", \"-u\", \"-x\", \"-c\"]\n\nWORKDIR /opt/airflow\n\nRUN echo \"Airflow version: ${AIRFLOW_VERSION}\"\n\nARG APT_DEPS_IMAGE\nENV APT_DEPS_IMAGE=${APT_DEPS_IMAGE}\n\nARG AIRFLOW_USER=airflow\nENV AIRFLOW_USER=${AIRFLOW_USER}\n\nARG HOME=/home/airflow\nENV HOME=${HOME}\n\nARG AIRFLOW_HOME=${HOME}/airflow\nENV AIRFLOW_HOME=${AIRFLOW_HOME}\n\nARG AIRFLOW_SOURCES=/opt/airflow\nENV AIRFLOW_SOURCES=${AIRFLOW_SOURCES}\n\nRUN mkdir -pv ${AIRFLOW_HOME} \\\n    mkdir -pv ${AIRFLOW_HOME}/dags \\\n    mkdir -pv ${AIRFLOW_HOME}/logs \\\n    && chown -R ${AIRFLOW_USER}.${AIRFLOW_USER} ${AIRFLOW_HOME}\n\n# Increase the value here to force reinstalling Apache Airflow pip dependencies\nARG PIP_DEPENDENCIES_EPOCH_NUMBER=\"1\"\nENV PIP_DEPENDENCIES_EPOCH_NUMBER=${PIP_DEPENDENCIES_EPOCH_NUMBER}\n\n# Optimizing installation of Cassandra driver\n# Speeds up building the image - cassandra driver without CYTHON saves around 10 minutes\nARG CASS_DRIVER_NO_CYTHON=\"1\"\n# Build cassandra driver on multiple CPUs\nARG CASS_DRIVER_BUILD_CONCURRENCY=\"8\"\n\nENV CASS_DRIVER_BUILD_CONCURRENCY=${CASS_DRIVER_BUILD_CONCURRENCY}\nENV CASS_DRIVER_NO_CYTHON=${CASS_DRIVER_NO_CYTHON}\n\n# By default PIP install run without cache to make image smaller\nARG PIP_NO_CACHE_DIR=\"true\"\nENV PIP_NO_CACHE_DIR=${PIP_NO_CACHE_DIR}\nRUN echo \"Pip no cache dir: ${PIP_NO_CACHE_DIR}\"\n\n# PIP version used to install dependencies\nARG PIP_VERSION=\"19.0.1\"\nENV PIP_VERSION=${PIP_VERSION}\nRUN echo \"Pip version: ${PIP_VERSION}\"\n\nRUN pip install --no-cache-dir --upgrade pip==${PIP_VERSION}\n\n# We are copying everything with airflow:airflow user:group even if we use root to run the scripts\n# This is fine as root user will be able to use those dirs anyway.\n\n# Airflow sources change frequently but dependency configuration won't change that often\n# We copy setup.py and other files needed to perform setup of dependencies\n# This way cache here will only be invalidated if any of the\n# version/setup configuration change but not when airflow sources change\nCOPY --chown=airflow:airflow setup.py ${AIRFLOW_SOURCES}/setup.py\nCOPY --chown=airflow:airflow setup.cfg ${AIRFLOW_SOURCES}/setup.cfg\n\nCOPY --chown=airflow:airflow airflow/version.py ${AIRFLOW_SOURCES}/airflow/version.py\nCOPY --chown=airflow:airflow airflow/__init__.py ${AIRFLOW_SOURCES}/airflow/__init__.py\nCOPY --chown=airflow:airflow airflow/bin/airflow ${AIRFLOW_SOURCES}/airflow/bin/airflow\n\n# Airflow Extras installed\nARG AIRFLOW_EXTRAS=\"all\"\nENV AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS}\nRUN echo \"Installing with extras: ${AIRFLOW_EXTRAS}.\"\n\n# First install only dependencies but no Apache Airflow itself\n# This way regular changes in sources of Airflow will not trigger reinstallation of all dependencies\n# And this Docker layer will be reused between builds.\nRUN pip install --no-cache-dir --no-use-pep517 -e \".[${AIRFLOW_EXTRAS}]\"\n\nCOPY --chown=airflow:airflow airflow/www/package.json ${AIRFLOW_SOURCES}/airflow/www/package.json\nCOPY --chown=airflow:airflow airflow/www/package-lock.json ${AIRFLOW_SOURCES}/airflow/www/package-lock.json\n\nWORKDIR ${AIRFLOW_SOURCES}/airflow/www\n\n# Install necessary NPM dependencies (triggered by changes in package-lock.json)\nRUN gosu ${AIRFLOW_USER} npm ci\n\nCOPY --chown=airflow:airflow airflow/www/ ${AIRFLOW_SOURCES}/airflow/www/\n\n# Package NPM for production\nRUN gosu ${AIRFLOW_USER} npm run prod\n\n# Always apt-get update/upgrade here to get latest dependencies before\n# we redo pip install\nRUN apt-get update \\\n    && apt-get upgrade -y --no-install-recommends \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Cache for this line will be automatically invalidated if any\n# of airflow sources change\nCOPY --chown=airflow:airflow . ${AIRFLOW_SOURCES}/\n\nWORKDIR ${AIRFLOW_SOURCES}\n\n# Always add-get update/upgrade here to get latest dependencies before\n# we redo pip install\nRUN apt-get update \\\n    && apt-get upgrade -y --no-install-recommends \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Additional python deps to install\nARG ADDITIONAL_PYTHON_DEPS=\"\"\n\nRUN if [[ -n \"${ADDITIONAL_PYTHON_DEPS}\" ]]; then \\\n        pip install --no-cache-dir ${ADDITIONAL_PYTHON_DEPS}; \\\n    fi\n\nCOPY --chown=airflow:airflow ./scripts/docker/entrypoint.sh /entrypoint.sh\n\nUSER ${AIRFLOW_USER}\n\nWORKDIR ${AIRFLOW_SOURCES}\n\nENV PATH=\"${HOME}:${PATH}\"\n\nEXPOSE 8080\n\nENTRYPOINT [\"/usr/local/bin/dumb-init\", \"--\", \"/entrypoint.sh\"]\n\nCMD [\"--help\"]\n"
}
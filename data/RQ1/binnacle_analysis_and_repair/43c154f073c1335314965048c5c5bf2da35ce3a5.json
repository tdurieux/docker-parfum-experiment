{
  "startTime": 1674244901056,
  "endTime": 1674244901664,
  "originalSmells": [
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 53,
        "lineEnd": 53,
        "columnStart": 4,
        "columnEnd": 85
      }
    },
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 54,
        "lineEnd": 54,
        "columnStart": 4,
        "columnEnd": 87
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 58,
        "lineEnd": 58,
        "columnStart": 4,
        "columnEnd": 28
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 59,
        "lineEnd": 59,
        "columnStart": 4,
        "columnEnd": 41
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 8,
        "lineEnd": 8,
        "columnStart": 22,
        "columnEnd": 67
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 8,
        "lineEnd": 8,
        "columnStart": 22,
        "columnEnd": 67
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 10,
        "lineEnd": 40,
        "columnStart": 22,
        "columnEnd": 21
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Copyright 2016 Yahoo Inc.\n# Licensed under the terms of the Apache 2.0 license.\n# Please see LICENSE file in the project root for terms.\n#\n# This file is the dockerfile to setup caffeonspark cpu standalone version.\n\nFROM nvidia/cuda:7.5-cudnn5-devel-ubuntu14.04\n\nRUN apt-get update && apt-get install --no-install-recommends -y software-properties-common && rm -rf /var/lib/apt/lists/*;\nRUN add-apt-repository ppa:openjdk-r/ppa\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        build-essential \\\n        vim \\\n        cmake \\\n        git \\\n        wget \\\n        libatlas-base-dev \\\n        libboost-all-dev \\\n        libgflags-dev \\\n        libgoogle-glog-dev \\\n        libhdf5-serial-dev \\\n        libleveldb-dev \\\n        liblmdb-dev \\\n        libopencv-dev \\\n        libprotobuf-dev \\\n        libsnappy-dev \\\n        protobuf-compiler \\\n        python-dev \\\n        python-numpy \\\n        python-pip \\\n        python-scipy \\\n        maven \\\n        unzip \\\n        zip \\\n        unzip \\\n        libopenblas-dev \\\n        openssh-server \\\n        openssh-client \\\n        libopenblas-dev \\\n        libboost-all-dev \\\n        openjdk-8-jdk && rm -rf /var/lib/apt/lists/*;\n\nRUN rm -rf /var/lib/apt/lists/*\n\n\n# Passwordless SSH\nRUN ssh-keygen -y -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -y -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN cp /root/.ssh/id_rsa.pub ~/.ssh/authorized_keys\n\n\n# Apache Hadoop and Spark section\nRUN wget https://apache.mirrors.tds.net/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz\nRUN wget https://archive.apache.org/dist/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz\n\nRUN gunzip hadoop-2.6.4.tar.gz\nRUN gunzip spark-1.6.0-bin-hadoop2.6.tgz\nRUN tar -xf hadoop-2.6.4.tar && rm hadoop-2.6.4.tar\nRUN tar -xf spark-1.6.0-bin-hadoop2.6.tar && rm spark-1.6.0-bin-hadoop2.6.tar\n\nRUN sudo cp -r hadoop-2.6.4 /usr/local/hadoop\nRUN sudo cp -r spark-1.6.0-bin-hadoop2.6 /usr/local/spark\n\nRUN rm hadoop-2.6.4.tar spark-1.6.0-bin-hadoop2.6.tar\nRUN rm -rf hadoop-2.6.4/ spark-1.6.0-bin-hadoop2.6/\n\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode\nRUN sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode\n\n# Environment variables\nENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64\nENV HADOOP_HOME=/usr/local/hadoop\nENV SPARK_HOME=/usr/local/spark\nENV PATH $PATH:$JAVA_HOME/bin\nENV PATH $PATH:$HADOOP_HOME/bin\nENV PATH $PATH:$HADOOP_HOME/sbin\nENV PATH $PATH:$SPARK_HOME/bin\nENV PATH $PATH:$SPARK_HOME/sbin\nENV HADOOP_MAPRED_HOME /usr/local/hadoop\nENV HADOOP_COMMON_HOME /usr/local/hadoop\nENV HADOOP_HDFS_HOME /usr/local/hadoop\nENV HADOOP_CONF_DIR /usr/local/hadoop/etc/hadoop\nENV YARN_CONF_DIR /usr/local/hadoop/etc/hadoop\nENV YARN_HOME /usr/local/hadoop\nENV HADOOP_COMMON_LIB_NATIVE_DIR /usr/local/hadoop/lib/native\nENV HADOOP_OPTS \"-Djava.library.path=$HADOOP_HOME/lib\"\n\n# Clone CaffeOnSpark\nENV CAFFE_ON_SPARK=/opt/CaffeOnSpark\nWORKDIR $CAFFE_ON_SPARK\nRUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\n\n# Some of the Hadoop part extracted from \"https://hub.docker.com/r/sequenceiq/hadoop-docker/~/dockerfile/\"\nRUN mkdir $HADOOP_HOME/input\nRUN cp $HADOOP_HOME/etc/hadoop/*.xml $HADOOP_HOME/input\nRUN cd /usr/local/hadoop/input\n\n# Copy .xml files.\nRUN cp ${CAFFE_ON_SPARK}/scripts/*.xml  ${HADOOP_HOME}/etc/hadoop\n\n# Format namenode and finish hadoop, spark installations.\nRUN $HADOOP_HOME/bin/hdfs namenode -format\n\nRUN ls /root/.ssh/\nADD config/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\n\nADD config/bootstrap.sh /etc/bootstrap.sh\nRUN chown root:root /etc/bootstrap.sh\nRUN chmod 700 /etc/bootstrap.sh\n\nENV BOOTSTRAP /etc/bootstrap.sh\n\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n\n# workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n\n# fix the 254 error code\nRUN sed  -i \"/^[^#]*UsePAM/ s/.*/#&/\"  /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\n\nRUN service ssh start && $HADOOP_HOME/etc/hadoop/hadoop-env.sh && $HADOOP_HOME/sbin/start-dfs.sh && $HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/root\nRUN service ssh start && $HADOOP_HOME/etc/hadoop/hadoop-env.sh && $HADOOP_HOME/sbin/start-dfs.sh && $HADOOP_HOME/bin/hdfs dfs -put $HADOOP_HOME/etc/hadoop/ input\n\nCMD [\"/etc/bootstrap.sh\", \"-bash\"]\n\n# Hdfs ports\nEXPOSE 50010 50020 50070 50075 50090 8020 9000\n# Mapred ports\nEXPOSE 10020 19888\n#Yarn ports\nEXPOSE 8030 8031 8032 8033 8040 8042 8088\n#Other ports\nEXPOSE 49707 2122\n\n# Continue with CaffeOnSpark build.\n# ENV CAFFE_ON_SPARK=/opt/CaffeOnSpark\nWORKDIR $CAFFE_ON_SPARK\n# RUN git clone https://github.com/yahoo/CaffeOnSpark.git . --recursive\nRUN cp caffe-public/Makefile.config.example caffe-public/Makefile.config\nRUN echo \"INCLUDE_DIRS += ${JAVA_HOME}/include\" >> caffe-public/Makefile.config\n#RUN sed -i \"s/# USE_CUDNN := 1/USE_CUDNN := 1/g\" caffe-public/Makefile.config\nRUN sed -i \"s|BLAS := atlas|BLAS := open|g\" caffe-public/Makefile.config\n\nRUN make build\n\nENV LD_LIBRARY_PATH $LD_LIBRARY_PATH:$CAFFE_ON_SPARK/caffe-public/distribute/lib:$CAFFE_ON_SPARK/caffe-distri/distribute/lib\n\nWORKDIR /root\n"
}
{
  "startTime": 1674251447652,
  "endTime": 1674251447721,
  "originalSmells": [
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 5,
        "lineEnd": 5,
        "columnStart": 7,
        "columnEnd": 72
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# ch-test-scope: standard\nFROM debian:stretch\n\n# Install needed OS packages.\nRUN apt-get update \\\n    && apt-get install --no-install-recommends -y less openjdk-8-jre-headless procps python wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# We want ch-ssh\nRUN touch /usr/bin/ch-ssh\n\n# Download and install Spark.\n#\n# We're staying on Spark 2.0 because 2.1.0 introduces Hive for metadata\n# handling somehow [1]. Data for this goes in $CWD by default, which is / and\n# not writeable in Charliecloud containers. So you get thousands of lines of\n# stack trace from pyspark. Workarounds exist, including cd to /tmp first or\n# configure hive-site.xml [2], but I'm not willing to put up with that crap\n# for demo purposes. Maybe it will be fixed in a 2.1 point release.\n#\n# [1]: http://spark.apache.org/docs/latest/sql-programming-guide.html#upgrading-from-spark-sql-20-to-21\n# [2]: https://community.cloudera.com/t5/Advanced-Analytics-Apache-Spark/Spark-displays-SQLException-when-Hive-not-installed/td-p/37954\nENV URLPATH http://d3kbcqa49mib13.cloudfront.net\nENV DIR spark-2.0.2-bin-hadoop2.7\nENV TAR $DIR.tgz\nRUN wget -nv $URLPATH/$TAR\nRUN tar xf $TAR && mv $DIR spark && rm $TAR\n\n# Very basic default configuration, to make it run and not do anything stupid.\nRUN printf '\\\nSPARK_LOCAL_IP=127.0.0.1\\n\\\nSPARK_LOCAL_DIRS=/tmp\\n\\\nSPARK_LOG_DIR=/tmp\\n\\\nSPARK_WORKER_DIR=/tmp\\n\\\n' > /spark/conf/spark-env.sh\n\n# Move config to /mnt/0 so we can provide a different config if we want\nRUN    mv /spark/conf /mnt/0 \\\n    && ln -s /mnt/0 /spark/conf\n"
}
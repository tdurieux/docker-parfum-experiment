{
  "startTime": 1674245678730,
  "endTime": 1674245678844,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 10,
        "lineEnd": 10,
        "columnStart": 4,
        "columnEnd": 101
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 18,
        "lineEnd": 18,
        "columnStart": 4,
        "columnEnd": 100
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 25,
        "lineEnd": 25,
        "columnStart": 4,
        "columnEnd": 90
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 10,
        "lineEnd": 10,
        "columnStart": 4,
        "columnEnd": 101
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 18,
        "lineEnd": 18,
        "columnStart": 4,
        "columnEnd": 100
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 30,
        "lineEnd": 30,
        "columnStart": 4,
        "columnEnd": 35
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM java:openjdk-8-jdk\n\nENV hadoop_ver 2.6.1\nENV spark_ver 1.5.1\n\n# Get Hadoop from US Apache mirror and extract just the native\n# libs. (Until we care about running HDFS with these containers, this\n# is all we need.)\nRUN mkdir -p /opt && \\\n    cd /opt && \\\n    curl -f https://www.us.apache.org/dist/hadoop/common/hadoop-${hadoop_ver}/hadoop-${hadoop_ver}.tar.gz | \\\n        tar -zx hadoop-${hadoop_ver}/lib/native && \\\n    ln -s hadoop-${hadoop_ver} hadoop && \\\n    echo Hadoop ${hadoop_ver} native libraries installed in /opt/hadoop/lib/native\n\n# Get Spark from US Apache mirror.\nRUN mkdir -p /opt && \\\n    cd /opt && \\\n    curl -f https://www.us.apache.org/dist/spark/spark-${spark_ver}/spark-${spark_ver}-bin-hadoop2.6.tgz | \\\n        tar -zx && \\\n    ln -s spark-${spark_ver}-bin-hadoop2.6 spark && \\\n    echo Spark ${spark_ver} installed in /opt\n\n# Add the GCS connector.\nRUN cd /opt/spark/lib && \\\n    curl -f -O https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar\n\n# if numpy is installed on a driver it needs to be installed on all\n# workers, so install it everywhere\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y python-numpy && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\nADD log4j.properties /opt/spark/conf/log4j.properties\nADD start-common.sh /\nADD core-site.xml /opt/spark/conf/core-site.xml\nADD spark-defaults.conf /opt/spark/conf/spark-defaults.conf\nENV PATH $PATH:/opt/spark/bin\n"
}
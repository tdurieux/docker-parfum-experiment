{
  "startTime": 1674246802815,
  "endTime": 1674246803149,
  "originalSmells": [
    {
      "rule": "configureShouldUseBuildFlag",
      "position": {
        "lineStart": 69,
        "lineEnd": 69,
        "columnStart": 4,
        "columnEnd": 15
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 54,
        "lineEnd": 54,
        "columnStart": 4,
        "columnEnd": 59
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# This dockerfile downloads ImageNet to ${DATA_DIR}/imagenet\n#\n# * Set IMAGENET_USERNAME and IMAGENET_ACCESS_KEY\n# * Mount a 500GB+ file system on ${DATA_DIR}\n# * Launch and wait\n#\n# It can also be used to train the model. In this context,\n# it assumes being run on Kubernetes therefore\n#  * takes CLUSTER_CONF as an ENV, set by k8s via configMap\n#  * takes POD_NAME as entry to evaluate its role and task id via POD_NAME=<job name>-<task id>\n#\n# Note: this is hugely inspired from Google's Tensorflow Worshop\n# from August (old version of https://github.com/amygdala/tensorflow-workshop)\nFROM tensorflow/tensorflow:1.0.1\nMAINTAINER Samuel Cozannet <samuel.cozannet@madeden.com>\n\n# This is an evolution of the Dockerfile.devel published by Google for Tensorflow.\n# It adds building the server in the container at the end as documented in\n# https://tensorflow.github.io/serving/serving_inception\n# then we moved to\n\nENV PORT=8500\nENV MODEL_NAME=inception\nENV MODEL_PATH=/var/tensorflow/output\nENV BATCHING=\"--enable_batching\"\n\n# build variables for serving\nENV PYTHON_BIN_PATH=\"/usr/bin/python\"\nENV PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\nENV CC_OPT_FLAGS=\"-march=native\"\nENV TF_NEED_JEMALLOC=1\nENV TF_NEED_GCP=0\nENV TF_NEED_HDFS=0\nENV TF_ENABLE_XLA=0\nENV TF_NEED_OPENCL=0\nENV TF_NEED_CUDA=0\nENV TF_CUDA_VERSION=v8.0\n\nRUN apt update && apt install -yqq --no-install-recommends \\\n        jq \\\n\t\tcurl \\\n\t\tgit && \\\n\tapt-get clean && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    git clone https://github.com/tensorflow/models.git /models\n\n# We need to add a custom PPA to pick up JDK8, since trusty doesn't\n# have an openjdk8 backport.  openjdk-r is maintained by a reliable contributor:\n# Matthias Klose (https://launchpad.net/~doko).  It will do until\n# we either update the base image beyond 14.04 or openjdk-8 is\n# finally backported to trusty; see e.g.\n#   https://bugs.launchpad.net/trusty-backports/+bug/1368094\nRUN add-apt-repository -y ppa:openjdk-r/ppa && \\\n    apt-get update && \\\n    apt-get install --no-install-recommends -y openjdk-8-jdk openjdk-8-jre-headless && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN echo \"build --spawn_strategy=standalone --genrule_strategy=standalone\" \\\n    >>/root/.bazelrc\nENV BAZELRC /root/.bazelrc\n\nCOPY bin/bazel /bin/bazel\nCOPY bin/bazel-real /bin/bazel-real\n\n# PYTHON_BIN_PATH=/usr/bin/python PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages CC_OPT_FLAGS=\"-march=native\" TF_NEED_JEMALLOC=1 TF_NEED_GCP=0 TF_NEED_HDFS=0 TF_ENABLE_XLA=0 TF_NEED_OPENCL=0 TF_NEED_CUDA=0 TF_CUDA_VERSION=v8.0 ./configure\n\nRUN git clone --recurse-submodules https://github.com/tensorflow/serving /serving && \\\n    cd /serving/tensorflow && \\\n    ./configure --build=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" && \\\n    cd .. && \\\n    bazel build -c opt tensorflow_serving/...\n\nCOPY serve.sh /serve.sh\n\nCMD [ \"/serve.sh\" ]\n"
}
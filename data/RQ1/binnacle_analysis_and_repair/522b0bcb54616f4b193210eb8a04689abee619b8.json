{
  "startTime": 1674250435761,
  "endTime": 1674250436421,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 87,
        "lineEnd": 87,
        "columnStart": 4,
        "columnEnd": 100
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 88,
        "lineEnd": 88,
        "columnStart": 4,
        "columnEnd": 105
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 132,
        "lineEnd": 132,
        "columnStart": 4,
        "columnEnd": 116
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 87,
        "lineEnd": 87,
        "columnStart": 4,
        "columnEnd": 100
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 133,
        "lineEnd": 133,
        "columnStart": 4,
        "columnEnd": 43
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 134,
        "lineEnd": 134,
        "columnStart": 4,
        "columnEnd": 39
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM ubuntu:bionic\n\nMAINTAINER akchinSTC\n\nARG NB_USER=\"jovyan\"\nARG NB_UID=\"1000\"\nARG NB_GID=\"100\"\n\nUSER root\n\nENV HADOOP_PREFIX=/usr/hdp/current/hadoop \\\n    ANACONDA_HOME=/opt/conda\n\nENV SHELL=/bin/bash \\\n    NB_USER=$NB_USER \\\n    NB_UID=$NB_UID \\\n    NB_GID=$NB_GID \\\n    LC_ALL=en_US.UTF-8 \\\n    LANG=en_US.UTF-8 \\\n    LANGUAGE=en_US.UTF-8 \\\n    JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 \\\n    SPARK_HOME=/usr/hdp/current/spark2-client \\\n    PYSPARK_PYTHON=$ANACONDA_HOME/bin/python \\\n    HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop\n\nENV HOME=/home/$NB_USER \\\n    PATH=$ANACONDA_HOME/bin:$HADOOP_PREFIX/bin:$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH\n\nENV SPARK_VER 2.4.1\nENV HADOOP_VER 2.7.7\n\n# INSTALL / DOWNLOAD ALL NEEDED PACKAGES\nRUN apt-get update && apt-get -yq dist-upgrade \\\n && apt-get install -yq --no-install-recommends \\\n    wget \\\n    bzip2 \\\n    tar \\\n    curl \\\n    less \\\n    nano \\\n    ca-certificates \\\n    libkrb5-dev \\\n    sudo \\\n    locales \\\n    gcc \\\n    fonts-liberation \\\n    unzip \\\n    libsm6 \\\n    libxext-dev \\\n    libxrender1 \\\n    openjdk-8-jdk \\\n    openssh-server \\\n    openssh-client \\\n && rm -rf /var/lib/apt/lists/*\n\nRUN echo \"en_US.UTF-8 UTF-8\" > /etc/locale.gen && \\\n    locale-gen\n\nADD fix-permissions /usr/local/bin/fix-permissions\n# Create jovyan user with UID=1000 and in the 'users' group\n# and make sure these dirs are writable by the `users` group.\nRUN groupadd wheel -g 11 && \\\n    echo \"auth required pam_wheel.so use_uid\" >> /etc/pam.d/su && \\\n    useradd -m -s /bin/bash -N -u $NB_UID $NB_USER && \\\n    mkdir -p $ANACONDA_HOME && \\\n    mkdir -p /usr/hdp/current && \\\n    mkdir -p /usr/local/share/jupyter && \\\n    chown $NB_USER:$NB_GID $ANACONDA_HOME && \\\n    chmod g+w /etc/passwd && \\\n    chmod +x /usr/local/bin/fix-permissions && \\\n    fix-permissions $HOME && \\\n    fix-permissions $ANACONDA_HOME && \\\n    fix-permissions /usr/hdp/current && \\\n    fix-permissions /usr/local/share/jupyter\n\n# Create service user 'jovyan'. Pin uid/gid to 1000.\nRUN useradd -m -s /bin/bash -N -u 1111 elyra && \\\n    useradd -m -s /bin/bash -N -u 1112 bob  && \\\n    useradd -m -s /bin/bash -N -u 1113 alice\n\nUSER $NB_UID\n\n# Setup work directory for backward-compatibility\nRUN mkdir /home/$NB_USER/work && \\\n    fix-permissions /home/$NB_USER\n\n# DOWNLOAD HADOOP AND SPARK\nRUN curl -f -s https://www.us.apache.org/dist/hadoop/common/hadoop-$HADOOP_VER/hadoop-$HADOOP_VER.tar.gz | tar -xz -C /usr/hdp/current\nRUN curl -f -s https://archive.apache.org/dist/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-hadoop2.7.tgz | tar -xz -C /usr/hdp/current\n\n# SETUP SPARK AND HADOOP SYMLINKS\nRUN cd /usr/hdp/current && ln -s ./hadoop-$HADOOP_VER hadoop && ln -s ./spark-${SPARK_VER}-bin-hadoop2.7 spark2-client\n\n# INSTALL MINI-CONDA AND PYTHON PACKAGES\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh && \\\n    /bin/bash ~/miniconda.sh -f -b -p $ANACONDA_HOME && \\\n    rm ~/miniconda.sh && \\\n    conda clean -tipsy && \\\n    rm -rf /home/$NB_USER/.cache/yarn && \\\n    fix-permissions $ANACONDA_HOME && \\\n    fix-permissions /home/$NB_USER\n\nRUN conda install --yes --quiet \\\n    'pip' \\\n    'jupyter' \\\n    'r-devtools' \\\n    'r-stringr' \\\n    'r-argparse' && \\\n    conda clean -tipsy && \\\n    fix-permissions $ANACONDA_HOME && \\\n    fix-permissions /home/$NB_USER\n\nRUN Rscript -e 'install.packages(c(\"IRkernel\",\"versions\"), repos=\"http://cran.cnr.berkeley.edu\", lib=\"/opt/conda/lib/R/library\")' \\\n            -e 'IRkernel::installspec(prefix = \"/usr/local\")' \\\n            -e 'versions::install.versions(\"SparkR\", \"2.4.1\", lib=\"/opt/conda/lib/R/library\")' && \\\n    fix-permissions $ANACONDA_HOME && \\\n    fix-permissions /home/$NB_USER\n\n# SETUP HADOOP CONFIGS\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\\nexport HADOOP_PREFIX=/usr/hdp/current/hadoop\\nexport HADOOP_HOME=/usr/hdp/current/hadoop\\n:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/hdp/current/hadoop/etc/hadoop/:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n# SETUP PSEUDO - DISTRIBUTED CONFIGS FOR HADOOP\nCOPY [\"core-site.xml.template\", \"hdfs-site.xml\", \"mapred-site.xml\", \"yarn-site.xml.template\", \\\n      \"$HADOOP_PREFIX/etc/hadoop/\"]\n\n# working around docker.io build error\nRUN ls -la /usr/hdp/current/hadoop/etc/hadoop/*-env.sh && \\\n    chmod +x /usr/hdp/current/hadoop/etc/hadoop/*-env.sh && \\\n    ls -la /usr/hdp/current/hadoop/etc/hadoop/*-env.sh\n\n# Install Toree\nRUN cd /tmp && \\\n    curl -f -O https://dist.apache.org/repos/dist/release/incubator/toree/0.3.0-incubating/toree-pip/toree-0.3.0.tar.gz && \\\n    pip install --no-cache-dir --upgrade setuptools --user && \\\n    pip install --no-cache-dir /tmp/toree-0.3.0.tar.gz && \\\n    jupyter toree install --spark_home=$SPARK_HOME --kernel_name=\"Spark 2.4.1\" --interpreters=Scala && \\\n    rm -f /tmp/toree-0.3.0.tar.gz && \\\n    fix-permissions $ANACONDA_HOME && \\\n    fix-permissions /home/$NB_USER\n\n# SETUP PASSWORDLESS SSH FOR $NB_USER\nRUN ssh-keygen -q -N \"\" -t rsa -f /home/$NB_USER/.ssh/id_rsa && \\\n    cp /home/$NB_USER/.ssh/id_rsa.pub /home/$NB_USER/.ssh/authorized_keys && \\\n    chmod 0700 /home/$NB_USER\n\nUSER root\n\n# SETUP PASSWORDLESS SSH\nRUN yes y | ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key && \\\n    yes y | ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key && \\\n    yes y | ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa && \\\n    cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n\nRUN ssh-keygen -A\nCOPY ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config && \\\n    chown root:root /root/.ssh/config && \\\n    echo \"Port 2122\" >> /etc/ssh/sshd_config && \\\n    echo \"${NB_USER} ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\nRUN service ssh restart\n\nCOPY ssh_config /home/$NB_USER/.ssh/config\nRUN chmod 600 /home/$NB_USER/.ssh/config && \\\n    chown $NB_USER: /home/$NB_USER/.ssh/config\n\nCOPY bootstrap-yarn-spark.sh /usr/local/bin/\nRUN chown $NB_USER: /usr/local/bin/bootstrap-yarn-spark.sh && \\\n    chmod 0700 /usr/local/bin/bootstrap-yarn-spark.sh\n\nCMD [\"/usr/local/bin/bootstrap-yarn-spark.sh\"]\n\nLABEL Hadoop.version=$HADOOP_VER\nLABEL Spark.version=$SPARK_VER\n\n# Hdfs ports\nEXPOSE 50010 50020 50070 50075 50090 8020 9000 \\\n# Mapred ports\n19888 \\\n#Yarn ports\n8030 8031 8032 8033 8040 8042 8088 \\\n#Other ports\n49707 2122\n\nUSER $NB_USER\n\n"
}
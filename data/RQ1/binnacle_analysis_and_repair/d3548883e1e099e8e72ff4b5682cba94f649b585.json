{
  "startTime": 1674234406173,
  "endTime": 1674234406350,
  "originalSmells": [
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 5,
        "lineEnd": 5,
        "columnStart": 4,
        "columnEnd": 72
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 5,
        "lineEnd": 5,
        "columnStart": 4,
        "columnEnd": 72
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM ubuntu:16.04\n\nARG PYTHON_VERSION=3.6\n\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y wget bzip2 build-essential openjdk-8-jdk ssh sudo && \\\n    apt-get clean && rm -rf /var/lib/apt/lists/*;\n\n# Add ubuntu user and enable password-less sudo\nRUN useradd -mU -s /bin/bash -G sudo ubuntu && \\\n    echo \"ubuntu ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\n\n# Set up SSH. Docker will cache this layer. So the keys end up the same on all containers.\n# We use a non-default location to simulate MLR, which doesn't have passwordless SSH.\n# We still keep the private key for manual SSH during debugging.\nRUN ssh-keygen -q -f ~/.ssh/docker -N \"\" && \\\n    cp ~/.ssh/docker.pub ~/.ssh/authorized_keys\n\n# Install Open MPI\nRUN wget --quiet https://github.com/uber/horovod/files/1596799/openmpi-3.0.0-bin.tar.gz -O /tmp/openmpi.tar.gz && \\\n    cd /usr/local && \\\n    tar -zxf /tmp/openmpi.tar.gz && \\\n    ldconfig && \\\n    rm -r /tmp/openmpi.tar.gz\n\n# Install Miniconda.\n# Reference: https://hub.docker.com/r/continuumio/miniconda/~/dockerfile/\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.11-Linux-x86_64.sh -O ~/miniconda.sh && \\\n    /bin/bash ~/miniconda.sh -b -p /opt/conda && \\\n    rm ~/miniconda.sh && \\\n    ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh && \\\n    echo \". /opt/conda/etc/profile.d/conda.sh\" >> ~/.bashrc\n\n# Create sparkdl conda env.\nENV PYTHON_VERSION $PYTHON_VERSION\nCOPY ./environment.yml /tmp/environment.yml\nRUN /opt/conda/bin/conda create -n sparkdl python=$PYTHON_VERSION && \\\n    /opt/conda/bin/conda env update -n sparkdl -f /tmp/environment.yml && \\\n    echo \"conda activate sparkdl\" >> ~/.bashrc\n\n# Install Spark and update env variables.\nENV SCALA_VERSION 2.11.8\nENV SPARK_VERSION 2.4.0\nENV SPARK_BUILD \"spark-${SPARK_VERSION}-bin-hadoop2.7\"\nENV SPARK_BUILD_URL \"https://dist.apache.org/repos/dist/release/spark/spark-2.4.0/${SPARK_BUILD}.tgz\"\nRUN wget --quiet $SPARK_BUILD_URL -O /tmp/spark.tgz && \\\n    tar -C /opt -xf /tmp/spark.tgz && \\\n    mv /opt/$SPARK_BUILD /opt/spark && \\\n    rm /tmp/spark.tgz\nENV SPARK_HOME /opt/spark\nENV PATH $SPARK_HOME/bin:$PATH\nENV PYTHONPATH /opt/spark/python/lib/py4j-0.10.7-src.zip:/opt/spark/python/lib/pyspark.zip:$PYTHONPATH\nENV PYSPARK_PYTHON python\n\n# Set env variables for tests.\nENV RUN_ONLY_LIGHT_TESTS True\n\n# The sparkdl dir will be mmounted here.\nVOLUME /mnt/sparkdl\nWORKDIR /mnt/sparkdl\n\nENTRYPOINT service ssh restart && /bin/bash\n"
}
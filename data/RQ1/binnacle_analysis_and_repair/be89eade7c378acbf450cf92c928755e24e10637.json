{
  "startTime": 1674252895174,
  "endTime": 1674252895357,
  "originalSmells": [
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 90,
        "lineEnd": 90,
        "columnStart": 8,
        "columnEnd": 114
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 83,
        "lineEnd": 83,
        "columnStart": 4,
        "columnEnd": 60
      }
    },
    {
      "rule": "sha256sumEchoOneSpaces",
      "position": {
        "lineStart": 15,
        "lineEnd": 15,
        "columnStart": 4,
        "columnEnd": 98
      }
    },
    {
      "rule": "sha256sumEchoOneSpaces",
      "position": {
        "lineStart": 51,
        "lineEnd": 51,
        "columnStart": 4,
        "columnEnd": 127
      }
    },
    {
      "rule": "sha256sumEchoOneSpaces",
      "position": {
        "lineStart": 91,
        "lineEnd": 91,
        "columnStart": 8,
        "columnEnd": 159
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM quay.io/geomesa/accumulo-geomesa:__TAG__\n\nMAINTAINER GeoMesa team <geomesa@ccri.com>\n\nARG TAG\nARG GEOMESA_VERSION\nARG ACCUMULO_VERSION\nARG THRIFT_VERSION\n\nENV GEOMESA_VERSION ${GEOMESA_VERSION}\n\nUSER root\n\n# Install Tini\nRUN wget --quiet https://github.com/krallin/tini/releases/download/v0.10.0/tini && \\\n    echo \"1361527f39190a7338a0b434bd8c88ff7233ce7b9a4876f3315c22fce7eca1b0  *tini\" | sha256sum -c - && \\\n    mv tini /usr/local/bin/tini && \\\n    chmod +x /usr/local/bin/tini\n\n# Configure environment\nENV CONDA_DIR /opt/conda\nENV PATH $CONDA_DIR/bin:$PATH\nENV SHELL /bin/bash\nENV NB_USER hdfs\nENV NB_UID 1000\nENV LC_ALL en_US.UTF-8\nENV LANG en_US.UTF-8\nENV LANGUAGE en_US.UTF-8\n\nENV NB_HOME /var/lib/hadoop-hdfs\n# Setup work directory\nRUN mkdir $CONDA_DIR && chown hdfs:hdfs $CONDA_DIR\n\nUSER $NB_USER\n\n# Setup hdfs home directory\nRUN mkdir -p ${NB_HOME}/work  && \\\n    mkdir ${NB_HOME}/.jupyter && \\\n    mkdir ${NB_HOME}/work/js\n\nCOPY js/ ${NB_HOME}/work/js/\n\n# copy the sample notebook\nUSER root\nCOPY GDELT+Analysis.ipynb ${NB_HOME}/work/\nRUN chmod a+rw ${NB_HOME}/work/*.ipynb\nUSER $NB_USER\n\n# Install conda as hdfs\nRUN cd /tmp && \\\n    wget --quiet https://repo.continuum.io/miniconda/Miniconda3-4.1.11-Linux-x86_64.sh && \\\n    echo \"efd6a9362fc6b4085f599a881d20e57de628da8c1a898c08ec82874f3bad41bf  *Miniconda3-4.1.11-Linux-x86_64.sh\" | sha256sum -c - && \\\n    /bin/bash Miniconda3-4.1.11-Linux-x86_64.sh -f -b -p $CONDA_DIR && \\\n    rm Miniconda3-4.1.11-Linux-x86_64.sh && \\\n    $CONDA_DIR/bin/conda install --quiet --yes conda==4.1.11 && \\\n    $CONDA_DIR/bin/conda config --system --add channels conda-forge && \\\n    $CONDA_DIR/bin/conda config --system --set auto_update_conda false && \\\n    conda clean -tipsy\n\n# Install Jupyter notebook as hdfs\nRUN conda install --quiet --yes \\\n    'notebook=4.2*' \\\n    jupyterhub=0.7 \\\n    && conda clean -tipsy\n\nUSER root\n\nEXPOSE 8888\nWORKDIR ${NB_HOME}/work\n\n# Configure container startup\nENTRYPOINT [\"tini\", \"--\"]\nCMD [\"start-notebook.sh\"]\n\n# Add local files as late as possible to avoid cache busting\nCOPY start.sh /usr/local/bin/\nCOPY start-notebook.sh /usr/local/bin/\nCOPY start-singleuser.sh /usr/local/bin/\nCOPY jupyter_notebook_config.py ${NB_HOME}/.jupyter/\nRUN chown -R $NB_USER:hdfs ${NB_HOME}/.jupyter\n\n# install toree\nCOPY toree-0.2.0.dev1.tar.gz /tmp\nRUN pip install --no-cache-dir --upgrade --pre /tmp/toree-0.2.0.dev1.tar.gz\n\n# install Spark\nENV APACHE_SPARK_VERSION 2.0.2\nENV HADOOP_VERSION 2.8.4\n\nRUN cd /tmp && \\\n        wget -q https://d3kbcqa49mib13.cloudfront.net/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \\\n        echo \"e6349dd38ded84831e3ff7d391ae7f2525c359fb452b0fc32ee2ab637673552a  *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\" | sha256sum -c - && \\\n        tar xzf spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /usr/local && \\\n        rm spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\nRUN cd /usr/local && ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark\n\n# Spark and Mesos config\nENV SPARK_HOME /usr/local/spark\nENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.3-src.zip\nENV MESOS_NATIVE_LIBRARY /usr/local/lib/libmesos.so\n\n# copy vegas libs to spark jars dir\nCOPY lib/* /usr/local/spark/jars/\n\n# Switch back to hdfs to avoid accidental container runs as root\nUSER $NB_USER\n\nENV GEOMESA_SPARK_HOME file:///opt/geomesa/dist/spark\nENV GEOMESA_SPARK_JARS ${GEOMESA_SPARK_HOME}/geomesa-accumulo-spark-runtime_2.11-${GEOMESA_VERSION}.jar,${GEOMESA_SPARK_HOME}/geomesa-spark-converter_2.11-${GEOMESA_VERSION}.jar,${GEOMESA_SPARK_HOME}/geomesa-spark-geotools_2.11-${GEOMESA_VERSION}.jar\n\n# install GeoMesa kernel\nRUN jupyter toree install       \\\n --replace                      \\\n --user                         \\\n --kernel_name=\"GeoMesa Spark\"  \\\n --spark_home=${SPARK_HOME}     \\\n --spark_opts=\"--driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info --master yarn --jars ${GEOMESA_SPARK_JARS}\"\n"
}
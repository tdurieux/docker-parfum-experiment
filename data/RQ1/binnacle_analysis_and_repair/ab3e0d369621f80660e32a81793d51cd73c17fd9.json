{
  "startTime": 1674250647126,
  "endTime": 1674250647576,
  "originalSmells": [
    {
      "rule": "aptGetInstallUseY",
      "position": {
        "lineStart": 49,
        "lineEnd": 50,
        "columnStart": 4,
        "columnEnd": 60
      }
    },
    {
      "rule": "aptGetInstallUseY",
      "position": {
        "lineStart": 52,
        "lineEnd": 53,
        "columnStart": 4,
        "columnEnd": 52
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 120,
        "lineEnd": 120,
        "columnStart": 4,
        "columnEnd": 39
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 120,
        "lineEnd": 120,
        "columnStart": 4,
        "columnEnd": 39
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM lablup/kernel-base:jail as jail-builder\nFROM lablup/kernel-base:hook as hook-builder\nFROM lablup/kernel-base:python3.6 as python-binary\nFROM lablup/common-tensorflow:1.12-py36-srv as tf-serving\n\nFROM nvidia/cuda:9.0-base-ubuntu16.04\nMAINTAINER Mario Cho \"m.cho@lablup.com\"\n\nENV LANG=C.UTF-8\nENV PYTHONUNBUFFERED 1\nENV NCCL_VERSION=2.2.13\nENV CUDNN_VERSION=7.2.1.38\nENV TF_TENSORRT_VERSION=4.1.2\n\nARG TF_SERVING_VERSION_GIT_BRANCH=master\nARG TF_SERVING_VERSION_GIT_COMMIT=head\nLABEL tensorflow_serving_github_branchtag=${TF_SERVING_VERSION_GIT_BRANCH}\nLABEL tensorflow_serving_github_commit=${TF_SERVING_VERSION_GIT_COMMIT}\n\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        ca-certificates \\\n        cuda-command-line-tools-9-0 \\\n        cuda-command-line-tools-9-0 \\\n        cuda-cublas-9-0 \\\n        cuda-cufft-9-0 \\\n        cuda-curand-9-0 \\\n        cuda-cusolver-9-0 \\\n        cuda-cusparse-9-0 \\\n        libcudnn7=${CUDNN_VERSION}-1+cuda9.0 \\\n        libnccl2=${NCCL_VERSION}-1+cuda9.0 \\\n        libgomp1 \\\n        wget \\\n        libexpat1 libgdbm3 libbz2-dev libffi6 libsqlite3-0 liblzma5 zlib1g \\\n\tlibmpdec2 \\\n        libssl1.0.0 \\\n\tlibssl-dev \\\n        libncursesw5 libtinfo5 libreadline6 \\\n\tproj-bin \\\n        libgeos-dev \\\n        mime-support \\\n\tgcc g++ \\\n        libproj-dev libgeos-dev \\\t\n        libzmq3-dev libuv1 \\\n\t&& \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n        nvinfer-runtime-trt-repo-ubuntu1604-4.0.1-ga-cuda9.0 && \\\n    apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n        libnvinfer4=${TF_TENSORRT_VERSION}-1+cuda9.0 && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/* && \\\n    rm /usr/lib/x86_64-linux-gnu/libnvinfer_plugin* && \\\n    rm /usr/lib/x86_64-linux-gnu/libnvcaffe_parser* && \\\n    rm /usr/lib/x86_64-linux-gnu/libnvparsers*\n\n\n# Install TensorFlow-serving\nCOPY --from=tf-serving /usr/local/bin/tensorflow_model_server /usr/bin/tensorflow_model_server\n\n# Expose ports\n# gRPC\nEXPOSE 8500\n\n# REST\nEXPOSE 8501\n\n\n# Copy the whole Python from the docker library image\nCOPY --from=python-binary /python.tar.gz /\nRUN cd /; tar xzpf python.tar.gz; rm python.tar.gz; ldconfig\nRUN export LD_LIBRARY_PATH=/usr/local/ssl/lib:$LD_LIBRARY_PATH\n# Test if Python is working\nRUN python -c 'import sys; print(sys.version_info); import ssl'\n\n# As we mostly have \"manylinux\" glibc-compatible binary packaes,\n# we don't have to rebuild these!\nRUN pip install --no-cache-dir pyzmq simplejson msgpack-python uvloop && \\\n    pip install --no-cache-dir aiozmq dataclasses tabulate namedlist six \"python-dateutil>=2\"\n\n# Install CUDA-9.0 + cuDNN 7.2\nRUN ln -s /usr/local/cuda-9.0 /usr/local/cuda && \\\n    ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.7.2.1 /usr/local/cuda/lib64/libcudnn.so && \\\n    ldconfig\nENV LD_LIBRARY_PATH=\"/usr/local/cuda/lib64:/usr/local/nvidia/lib64\" \\\n    PATH=\"/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n\n# python package install\nRUN pip install --no-cache-dir wheel && \\\n    pip install --no-cache-dir pyzmq simplejson msgpack-python uvloop && \\\n    pip install --no-cache-dir aiozmq dataclasses tabulate namedlist six \"python-dateutil>=2\" && \\\n    pip install --no-cache-dir keras && \\\n    pip install --no-cache-dir h5py && \\\n    pip install --no-cache-dir Cython && \\\n    pip install --no-cache-dir matplotlib bokeh && \\\n    pip install --no-cache-dir pyproj && \\\n    pip install --no-cache-dir Cartopy && \\\n    pip install --no-cache-dir ipython && \\\n    pip install --no-cache-dir pandas && \\\n    pip install --no-cache-dir seaborn && \\\n    pip install --no-cache-dir pillow && \\\n    pip install --no-cache-dir networkx cvxpy && \\\n    pip install --no-cache-dir scikit-learn scikit-image && \\\n    pip install --no-cache-dir scikit-image && \\\n    pip install --no-cache-dir pygments && \\\n    pip install --no-cache-dir requests && \\\n    rm -f /tmp/*.whl\n\n# Set where models should be stored in the container\n#ENV MODEL_BASE_PATH=/home/work/models\n#RUN mkdir -p ${MODEL_BASE_PATH}\n\n# The only required piece is the model name in order to differentiate endpoints\n#ENV MODEL_NAME=model\n\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y libseccomp2 gosu && \\\n    apt-get clean && \\\n    rm -r /var/lib/apt/lists /var/cache/apt/archives && \\\n    ln -s /usr/sbin/gosu /usr/sbin/su-exec && \\\n    mkdir /home/work && chmod 755 /home/work; rm -rf /var/lib/apt/lists/*; \\\n    mkdir /home/backend.ai && chmod 755 /home/backend.ai\nADD entrypoint.sh /usr/local/bin/entrypoint.sh\nRUN chmod +x /usr/local/bin/entrypoint.sh\n\n# Create a script that runs the model server so we can use environment variables\n# while also passing in arguments from the docker command line\n#RUN echo '#!/bin/bash \\n\\n\\\n#    tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n#        --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n#        \"$@\"' >> /usr/local/bin/entrypoint.sh && \\\n#    chmod +x /usr/local/bin/entrypoint.sh\n\nENTRYPOINT [\"/usr/local/bin/entrypoint.sh\"]\n\nCOPY policy.yml /home/backend.ai/policy.yml\n\n# Install jail\nCOPY --from=jail-builder /go/src/github.com/lablup/backend.ai-jail/backend.ai-jail /home/backend.ai/jail\nCOPY --from=hook-builder /root/backend.ai-hook/libbaihook.so /home/backend.ai/libbaihook.so\nENV LD_PRELOAD /home/backend.ai/libbaihook.so\n\n# Install kernel-runner scripts package\nRUN pip install --no-cache-dir \"backend.ai-kernel-runner[python]~=1.4.0\"\n\n# Matplotlib configuration and pre-heating\nENV MPLCONFIGDIR /home/backend.ai/.matplotlib\nRUN mkdir /home/backend.ai/.matplotlib\nCOPY matplotlibrc /home/backend.ai/.matplotlib/\nRUN echo 'import matplotlib.pyplot' > /tmp/matplotlib-fontcache.py \\\n    && python /tmp/matplotlib-fontcache.py \\\n    && rm /tmp/matplotlib-fontcache.py\n\nWORKDIR /home/work\nVOLUME [\"/home/work\"]\nEXPOSE 2000 2001 2002 2003\n\nLABEL ai.backend.nvidia.enabled=\"yes\" \\\n      com.nvidia.cuda.version=\"9.0.176\" \\\n      com.nvidia.volumes.needed=\"nvidia_driver\" \\\n      ai.backend.port=\"8500, 8501\" \\\n      ai.backend.timeout=\"0\" \\\n      ai.backend.maxmem=\"8g\" \\\n      ai.backend.maxcores=\"4\" \\\n      ai.backend.envs.corecount=\"OPENBLAS_NUM_THREADS,OMP_NUM_THREADS,NPROC\" \\\n      ai.backend.features=\"batch query uid-match user-input\"\n\n\nCMD [\"/home/backend.ai/jail\", \"-policy\", \"/home/backend.ai/policy.yml\", \\\n     \"/usr/local/bin/python\", \"-m\", \"ai.backend.kernel\", \"python\"]\n\n# vim: ft=dockerfile sts=4 sw=4 et tw=0"
}
{
  "startTime": 1674244702501,
  "endTime": 1674244702850,
  "originalSmells": [
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 77,
        "lineEnd": 82,
        "columnStart": 5,
        "columnEnd": 39
      }
    },
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 90,
        "lineEnd": 90,
        "columnStart": 5,
        "columnEnd": 111
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 59,
        "lineEnd": 59,
        "columnStart": 7,
        "columnEnd": 25
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 60,
        "lineEnd": 60,
        "columnStart": 7,
        "columnEnd": 23
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 61,
        "lineEnd": 61,
        "columnStart": 7,
        "columnEnd": 28
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 62,
        "lineEnd": 62,
        "columnStart": 7,
        "columnEnd": 34
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 63,
        "lineEnd": 63,
        "columnStart": 7,
        "columnEnd": 25
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 64,
        "lineEnd": 64,
        "columnStart": 7,
        "columnEnd": 25
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 65,
        "lineEnd": 65,
        "columnStart": 7,
        "columnEnd": 85
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 66,
        "lineEnd": 66,
        "columnStart": 7,
        "columnEnd": 40
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# VERSION 1.8.1-1\n# AUTHOR: Matthieu \"Puckel_\" Roisil\n# DESCRIPTION: Basic Airflow container\n# BUILD: docker build --rm -t puckel/docker-airflow .\n# SOURCE: https://github.com/puckel/docker-airflow\n\nFROM python:3.6\n\n# Never prompts the user for choices on installation/configuration of packages\nENV DEBIAN_FRONTEND noninteractive\nENV TERM linux\n\n# Java\nARG JAVA_MAJOR_VERSION=8\nARG JAVA_MINOR_VERSION=181\n\n# Spark\nARG SPARK_VERSION=2.3.1\n\n# Airflow\nARG AIRFLOW_VERSION=1.9.0\nARG AIRFLOW_HOME=/usr/local/airflow\nENV AIRFLOW_HOME=/usr/local/airflow\n\n# Define en_US.\nENV LANGUAGE en_US.UTF-8\nENV LANG en_US.UTF-8\nENV LC_ALL en_US.UTF-8\nENV LC_CTYPE en_US.UTF-8\nENV LC_MESSAGES en_US.UTF-8\nENV LC_ALL en_US.UTF-8\n\nRUN set -ex \\\n    && buildDeps=' \\\n        python3-dev \\\n        libkrb5-dev \\\n        libsasl2-dev \\\n        libssl-dev \\\n        libffi-dev \\\n        build-essential \\\n        libblas-dev \\\n        liblapack-dev \\\n        libpq-dev \\\n        git \\\n    ' \\\n    && apt-get update -yqq \\\n    && apt-get install -yqq --no-install-recommends \\\n        $buildDeps \\\n        python3-pip \\\n        python3-requests \\\n        apt-utils \\\n        curl \\\n        netcat \\\n        locales \\\n    && sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen \\\n    && locale-gen \\\n    && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \\\n    && useradd -ms /bin/bash -d ${AIRFLOW_HOME} airflow \\\n    && python -m pip install -U pip setuptools wheel \\\n    && pip install --no-cache-dir Cython \\\n    && pip install --no-cache-dir pytz \\\n    && pip install --no-cache-dir pyOpenSSL \\\n    && pip install --no-cache-dir ndg-httpsclient \\\n    && pip install --no-cache-dir pytest \\\n    && pip install --no-cache-dir pyasn1 \\\n    && pip install --no-cache-dir apache-airflow[crypto,celery,postgres,hive,jdbc]==$AIRFLOW_VERSION \\\n    && pip install --no-cache-dir celery[redis]==3.1.17 \\\n    && apt-get clean \\\n    && rm -rf \\\n        /var/lib/apt/lists/* \\\n        /tmp/* \\\n        /var/tmp/* \\\n        /usr/share/man \\\n        /usr/share/doc \\\n        /usr/share/doc-base\n# Java\nRUN cd /opt/ \\\n  && wget \\\n    --no-cookies \\\n    --no-check-certificate \\\n    --header \"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie\" \\\n    \"https://download.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-${JAVA_MAJOR_VERSION}u${JAVA_MINOR_VERSION}-linux-x64.tar.gz\" \\\n    -O jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n  && tar xzf jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n  && rm jdk-${JAVA_MAJOR_VERSION}.tar.gz \\\n  && update-alternatives --install /usr/bin/java java /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/java 100 \\\n  && update-alternatives --install /usr/bin/jar jar /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/jar 100 \\\n&& update-alternatives --install /usr/bin/javac javac /opt/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_MINOR_VERSION}/bin/javac 100\n# SPARK\nRUN cd /usr/ \\\n  && wget \"https://www-eu.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz\" \\\n  && tar xzf spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \\\n  && rm spark-${SPARK_VERSION}-bin-hadoop2.7.tgz \\\n  && mv spark-${SPARK_VERSION}-bin-hadoop2.7 spark\n\nENV SPARK_HOME /usr/spark\nENV SPARK_MAJOR_VERSION 2\nENV PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$SPARK_HOME/python/:$PYTHONPATH\n\nRUN mkdir -p /usr/spark/work/ \\\n  && chmod -R 777 /usr/spark/work/\n\nENV SPARK_MASTER_PORT 7077\n\nCOPY docker_files/entrypoint.sh /entrypoint.sh\nCOPY docker_files/airflow.cfg ${AIRFLOW_HOME}/airflow.cfg\n\nRUN chown -R airflow: ${AIRFLOW_HOME}\nRUN chown airflow: /entrypoint.sh\nRUN chmod +x /entrypoint.sh\n\nEXPOSE 8080 5555 8793\n\nWORKDIR ${AIRFLOW_HOME}\n# dev\nRUN mkdir -p ${AIRFLOW_HOME}/dags\nCOPY docker_files/populate_tables.py /usr/local/airflow/populate_tables.py\nRUN cd ${AIRFLOW_HOME}/dags && git clone https://github.com/danielvdende/data-testing-with-airflow.git development\nRUN cd ${AIRFLOW_HOME}/dags/development && git checkout development\nCOPY docker_files/dev.conf ${AIRFLOW_HOME}/dags/development/dags/environment.conf\n# tst\nRUN cd ${AIRFLOW_HOME}/dags && git clone https://github.com/danielvdende/data-testing-with-airflow.git test\nRUN cd ${AIRFLOW_HOME}/dags/test && git checkout test\nCOPY docker_files/tst.conf ${AIRFLOW_HOME}/dags/test/dags/environment.conf\n#\n## acc\nRUN cd ${AIRFLOW_HOME}/dags && git clone https://github.com/danielvdende/data-testing-with-airflow.git acceptance\nRUN cd ${AIRFLOW_HOME}/dags/acceptance && git checkout acceptance\nCOPY docker_files/acc.conf ${AIRFLOW_HOME}/dags/acceptance/dags/environment.conf\n#\n## prd\nRUN cd ${AIRFLOW_HOME}/dags && git clone https://github.com/danielvdende/data-testing-with-airflow.git production\nRUN cd ${AIRFLOW_HOME}/dags/production && git checkout master\nCOPY docker_files/prd.conf ${AIRFLOW_HOME}/dags/production/dags/environment.conf\n\nENTRYPOINT /entrypoint.sh\nRUN cd /usr/local/airflow && /usr/spark/bin/spark-submit --master local populate_tables.py\n"
}
{
  "startTime": 1674249793599,
  "endTime": 1674249793753,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 58,
        "lineEnd": 58,
        "columnStart": 4,
        "columnEnd": 53
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM python:2.7-stretch AS no-spark\n\n# Setup airflow\nRUN set -ex \\\n    && (echo 'deb http://deb.debian.org/debian stretch-backports main' > /etc/apt/sources.list.d/backports.list) \\\n    && apt-get update \\\n    && DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y --force-yes build-essential libkrb5-dev libsasl2-dev libffi-dev default-libmysqlclient-dev vim-tiny gosu krb5-user \\\n    && apt-get purge --auto-remove -yqq \\\n    && apt-get autoremove -yqq --purge \\\n    && apt-get clean \\\n    && rm -rf \\\n        /var/lib/apt/lists/* \\\n        /tmp/* \\\n        /var/tmp/* \\\n        /usr/share/doc \\\n        /usr/share/doc-base \\\n    && pip install --no-cache-dir \"apache-airflow[devel_hadoop,crypto,celery,redis,postgres,jdbc,ssh]==1.10.3\" psycopg2\n\nARG airflow_home=/airflow\nENV AIRFLOW_HOME=${airflow_home}\n\nWORKDIR ${AIRFLOW_HOME}\n\n# Setup airflow dags path\nENV AIRFLOW_DAG=${AIRFLOW_HOME}/dags\n\nRUN mkdir -p ${AIRFLOW_DAG}\n\nCOPY airflow.cfg ${AIRFLOW_HOME}/airflow.cfg\nCOPY unittests.cfg ${AIRFLOW_HOME}/unittests.cfg\nCOPY webserver_config.py ${AIRFLOW_HOME}/webserver_config.py\n\n# Create default user and group\nARG user=afpuser\nENV USER=${user}\nARG group=hadoop\nENV GROUP=${group}\nRUN groupadd -r \"${GROUP}\" && useradd -rmg \"${GROUP}\" \"${USER}\"\n\n# Number of times the Airflow scheduler will run before it terminates (and restarts)\nARG scheduler_runs=5\nENV SCHEDULER_RUNS=${scheduler_runs}\n# parallelism = number of physical python processes the scheduler can run\nARG airflow__core__parallelism=8\nENV AIRFLOW__CORE__PARALLELISM=${airflow__core__parallelism}\n# dag_concurrency = the number of TIs to be allowed to run PER-dag at once\nARG airflow__core__dag_concurrency=6\nENV AIRFLOW__CORE__DAG_CONCURRENCY=${airflow__core__dag_concurrency}\n# max_threads = number of processes to parallelize the scheduler over, cannot exceed the cpu count\nARG airflow__scheduler__max_threads=4\nENV AIRFLOW__SCHEDULER__MAX_THREADS=${airflow__scheduler__max_threads}\n\nENV AIRFLOW__CORE__EXECUTOR=LocalExecutor\n\nWORKDIR ${AIRFLOW_HOME}\n\n# Setup pipeline dependencies\nCOPY requirements.txt ${AIRFLOW_HOME}/requirements.txt\nRUN pip install --no-cache-dir -r \"${AIRFLOW_HOME}/requirements.txt\"\n\n# For optional S3 logging\nCOPY ./config/ ${AIRFLOW_HOME}/config/\n\nCOPY entrypoint.sh /entrypoint.sh\nENTRYPOINT [\"/entrypoint.sh\"]\n\n\nFROM no-spark AS with-spark-optional-dag\n\n# Install Java\nRUN apt-get update \\\n    && apt-get install --no-install-recommends -y openjdk-8-jre-headless \\\n    && rm -rf /var/lib/apt/lists/*\n\nENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n\nARG SPARK_VERSION=2.1.2\nARG HADOOP_VERSION=2.6.5\nARG SPARK_PY4J=python/lib/py4j-0.10.4-src.zip\n\nARG hadoop_home=/opt/hadoop\nENV HADOOP_HOME=${hadoop_home}\nENV PATH=${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin\n\nENV SPARK_HOME=/opt/spark-${SPARK_VERSION}\nENV PATH=$PATH:${SPARK_HOME}/bin\nENV PYTHONPATH=${SPARK_HOME}/${SPARK_PY4J}:${SPARK_HOME}/python\nENV PYSPARK_SUBMIT_ARGS=\"--driver-memory 8g --py-files ${SPARK_HOME}/python/lib/pyspark.zip pyspark-shell\"\n\n# Download Spark\nARG SPARK_EXTRACT_LOC=/sparkbin\nRUN [\"/bin/bash\", \"-c\", \"set -eoux pipefail && \\\n    (curl https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz | \\\n    tar -xz -C /opt/) && \\\n    mv /opt/hadoop-${HADOOP_VERSION} /opt/hadoop && \\\n    mkdir -p ${SPARK_EXTRACT_LOC} && \\\n    (curl https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION:0:3}.tgz | \\\n    tar -xz -C ${SPARK_EXTRACT_LOC}) && \\\n    mkdir -p ${SPARK_HOME} && \\\n    mv ${SPARK_EXTRACT_LOC}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION:0:3}/* ${SPARK_HOME} && \\\n    rm -rf ${SPARK_EXTRACT_LOC} && \\\n    echo SPARK_HOME is ${SPARK_HOME} && \\\n    ls -al --g ${SPARK_HOME}\"]\n\n# Less verbose logging\nCOPY log4j.properties.production ${SPARK_HOME}/conf/log4j.properties\n\n\nFROM with-spark-optional-dag AS with-spark\n\n## To build your own image:\nONBUILD COPY dags/ ${AIRFLOW_DAG}\n"
}
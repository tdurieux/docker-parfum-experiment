{
  "startTime": 1674251797051,
  "endTime": 1674251797145,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 10,
        "lineEnd": 10,
        "columnStart": 4,
        "columnEnd": 101
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 18,
        "lineEnd": 18,
        "columnStart": 4,
        "columnEnd": 100
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 10,
        "lineEnd": 10,
        "columnStart": 4,
        "columnEnd": 101
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 18,
        "lineEnd": 18,
        "columnStart": 4,
        "columnEnd": 100
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 26,
        "lineEnd": 26,
        "columnStart": 4,
        "columnEnd": 35
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM java:openjdk-8-jdk\n\nENV hadoop_ver 2.7.3\nENV spark_ver 2.0.1\n\n# Get Hadoop from US Apache mirror and extract just the native\n# libs. (Until we care about running HDFS with these containers, this\n# is all we need.)\nRUN mkdir -p /opt && \\\n    cd /opt && \\\n    curl -f https://www.us.apache.org/dist/hadoop/common/hadoop-${hadoop_ver}/hadoop-${hadoop_ver}.tar.gz | \\\n        tar -zx hadoop-${hadoop_ver}/lib/native && \\\n    ln -s hadoop-${hadoop_ver} hadoop && \\\n    echo Hadoop ${hadoop_ver} native libraries installed in /opt/hadoop/lib/native\n\n# Get Spark from US Apache mirror.\nRUN mkdir -p /opt && \\\n    cd /opt && \\\n    curl -f https://www.us.apache.org/dist/spark/spark-${spark_ver}/spark-${spark_ver}-bin-hadoop2.7.tgz | \\\n        tar -zx && \\\n    ln -s spark-${spark_ver}-bin-hadoop2.7 spark && \\\n    echo Spark ${spark_ver} installed in /opt\n\n# if numpy is installed on a driver it needs to be installed on all\n# workers, so install it everywhere\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y python-numpy && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\nADD common.sh spark-master spark-worker /\nADD spark-defaults.conf /opt/spark/conf/spark-defaults.conf\nENV PATH $PATH:/opt/spark/bin\n"
}
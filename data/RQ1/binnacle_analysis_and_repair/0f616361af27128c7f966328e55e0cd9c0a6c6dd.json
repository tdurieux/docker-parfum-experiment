{
  "startTime": 1674250518522,
  "endTime": 1674250518703,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 11,
        "lineEnd": 11,
        "columnStart": 4,
        "columnEnd": 88
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 35,
        "lineEnd": 35,
        "columnStart": 4,
        "columnEnd": 78
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 47,
        "lineEnd": 47,
        "columnStart": 4,
        "columnEnd": 92
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 74,
        "lineEnd": 74,
        "columnStart": 4,
        "columnEnd": 159
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 75,
        "lineEnd": 75,
        "columnStart": 4,
        "columnEnd": 156
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 11,
        "lineEnd": 11,
        "columnStart": 4,
        "columnEnd": 88
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 35,
        "lineEnd": 35,
        "columnStart": 4,
        "columnEnd": 78
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 47,
        "lineEnd": 47,
        "columnStart": 4,
        "columnEnd": 92
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 74,
        "lineEnd": 74,
        "columnStart": 4,
        "columnEnd": 159
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Daniel Malczyk\n# ThinkBig Analytics, a Teradata Company\n\n#basic image with CentOS and latest JDK\nFROM airhacks/java\n\nMAINTAINER Daniel Malczyk <dmalczyk@gmail.com>\n\n#install hadoop, spark and Hive clients\n#------------\n#Hadoop client config\nRUN curl -f -s https://www.eu.apache.org/dist/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz | tar -xz -C /usr/local/\n\n#alternative to the command above\n#COPY hadoopclient/hadoop-2.7.1.tar.gz .\n#RUN tar -xz -C /usr/local/ -f ./hadoop-2.7.1.tar.gz\n\nRUN cd /usr/local && ln -s ./hadoop-2.7.1 hadoop\n\nENV HADOOP_HOME /usr/local/hadoop\nENV HADOOP_INSTALL $HADOOP_HOME\nENV HADOOP_PREFIX $HADOOP_HOME\nENV PATH $PATH:$HADOOP_INSTALL/sbin\nENV HADOOP_MAPRED_HOME $HADOOP_INSTALL\nENV HADOOP_COMMON_HOME $HADOOP_INSTALL\nENV HADOOP_HDFS_HOME $HADOOP_INSTALL\nENV YARN_HOME $HADOOP_INSTALL\nENV PATH $HADOOP_HOME/bin:$PATH\n\nRUN mkdir -p $HADOOP_PREFIX/etc/hadoop\nCOPY conf/core-site.xml $HADOOP_PREFIX/etc/hadoop/core-site.xml\nCOPY conf/hdfs-site.xml $HADOOP_PREFIX/etc/hadoop/hdfs-site.xml\n\n#Install spark to /usr/local/spark\n#support for Hadoop 2.6.0\nRUN curl -f -s https://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz | tar -xz -C /usr/local/\n\n#alternative to the command above\n#COPY hadoopclient/spark-1.6.1-bin-hadoop2.6.tgz .\n#RUN tar -xz -C /usr/local/ -f ./spark-1.6.1-bin-hadoop2.6.tgz\n\nRUN cd /usr/local && ln -s spark-1.6.1-bin-hadoop2.6 spark\nENV SPARK_HOME /usr/local/spark\n\nENV PATH $SPARK_HOME/bin:$PATH\n\n# Install hive\nRUN curl -f -s https://apache.mirrors.spacedump.net/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz | tar -xz -C /usr/local/\n\n#alternative to the command above\n#COPY hadoopclient/apache-hive-2.1.1-bin.tar.gz .\n#RUN tar -xz -C /usr/local/ -f ./apache-hive-2.1.1-bin.tar.gz\n\nRUN cd /usr/local && ln -s apache-hive-2.1.1-bin hive\nCOPY conf/hive-site.xml /usr/local/hive/conf\nRUN echo \"export HIVE_HOME=/usr/local/hive\" >> /etc/profile\nRUN echo \"export PATH=$PATH:/usr/local/hive/bin\">> /etc/profile\nENV HIVE_HOME /usr/local/hive\nENV PATH $PATH:$HIVE_HOME/bin\n# Create directory for hive logs\nRUN mkdir -p /var/log/hive\n# Increase PermGen space for hiveserver2 to fix OOM pb.\nCOPY conf/hive-env.sh /usr/local/hive/conf/\n\nRUN echo \"HADOOP_HOME=/usr/local/hadoop\" >> /usr/local/hive/bin/hive-config.sh\n\n# Prepare spark-hive integration, so spark sql will use hive tables defined in hive metastore, see https://spark.apache.org/docs/1.6.0/sql-programming-guide.html#hive-tables\nRUN cp $HADOOP_PREFIX/etc/hadoop/hdfs-site.xml $SPARK_HOME/conf\nRUN cp $HIVE_HOME/conf/hive-site.xml $SPARK_HOME/conf\n\nRUN cp $HADOOP_PREFIX/etc/hadoop/core-site.xml $SPARK_HOME/conf\nRUN echo spark.yarn.jar hdfs://hadoopservice/spark/spark-assembly-1.6.0-hadoop2.6.0.jar > $SPARK_HOME/conf/spark-defaults.conf\n\n# Download mysql jdbc driver and prepare hive metastore.\nRUN curl -f -s -o $HIVE_HOME/lib/mysql-connector-java-5.1.41.jar https://central.maven.org/maven2/mysql/mysql-connector-java/5.1.41/mysql-connector-java-5.1.41.jar\nRUN curl -f -s -o $HIVE_HOME/lib/mariadb-java-client-1.5.7.jar https://downloads.mariadb.com/Connectors/java/connector-java-1.5.7/mariadb-java-client-1.5.7.jar\n# Make mysql driver available to kylo-spark-shell\nRUN cp $HIVE_HOME/lib/mysql-connector-java-5.1.41.jar $SPARK_HOME/lib\nRUN cp $HIVE_HOME/lib/mariadb-java-client-1.5.7.jar  $SPARK_HOME/lib\n#TODO check this at runtime\nRUN echo \"spark.executor.extraClassPath $SPARK_HOME/lib/mysql-connector-java-5.1.41.jar\" >> $SPARK_HOME/conf/spark-defaults.conf\nRUN echo \"spark.driver.extraClassPath $SPARK_HOME/lib/mysql-connector-java-5.1.41.jar\" >> $SPARK_HOME/conf/spark-defaults.conf\n\n"
}
{
  "startTime": 1674239654239,
  "endTime": 1674239654336,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 15,
        "lineEnd": 15,
        "columnStart": 4,
        "columnEnd": 19
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "###########################################\n## Spark 1.4.1 over Hadoop 2.6 over Ubuntu\n\nFROM sequenceiq/hadoop-ubuntu:2.6.0\nMAINTAINER \"Paolo D'Onorio De Meo <p.donoriodemeo@gmail.com>\"\n\n###########################################\n# From http://www.eu.apache.org/dist/spark/spark-1.4.1/\nENV SPARK_VERSION spark-1.4.1\nENV HADOOP_VERSION hadoop2.6\nENV SPARK_BIN \"$SPARK_VERSION-bin-$HADOOP_VERSION\"\nENV SPARK_URL \"http://www.eu.apache.org/dist/spark/$SPARK_VERSION/$SPARK_BIN.tgz\"\n\n###########################################\n# Spark\nRUN curl -f $SPARK_URL | tar -xz -C /usr/local/\nRUN cd /usr/local && ln -s $SPARK_BIN spark\nENV SPARK_HOME /usr/local/spark\n# HDFS with spark libs\nRUN service ssh start && \\\n    $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh && \\\n    $HADOOP_PREFIX/sbin/start-dfs.sh && \\\n    $HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave && \\\n    $HADOOP_PREFIX/bin/hdfs dfs -put $SPARK_HOME/lib /spark\n\n###########################################\nENV SPARK_JAR hdfs:///spark/$SPARK_BIN.jar\nENV PATH $PATH:$SPARK_HOME/bin:$HADOOP_PREFIX/bin\nRUN echo \"spark.master\\tspark://master:7077\" \\\n    > $SPARK_HOME/conf/spark-defaults.conf\n\n###########################################\n# This is necessary to make things work in a cluster\n# where slave datanodes will search for\nENV MYHOSTNAME master\n# Set the right name for configuration\nRUN sed \"s/HOSTNAME/$MYHOSTNAME/\" \\\n    $HADOOP_PREFIX/etc/hadoop/core-site.xml.template > \\\n    $HADOOP_PREFIX/etc/hadoop/core-site.xml\n# Bootstraps for master and workers\nENV BSMASTER /bootmaster.sh\n# sed -i.bak 's/value>1/value>0/'g $HADOOP_PREFIX/etc/hadoop/hdfs-site.xml\n# yes Y | hdfs namenode -format\nRUN echo \"#!/bin/bash\" > $BSMASTER && \\\n    echo \"spark-class org.apache.spark.deploy.master.Master &\\nsleep 5\" \\\n        >> $BSMASTER && \\\n    echo \"echo 'starting namenode'\" >> $BSMASTER && \\\n    echo \"hdfs namenode > /dev/null 2>&1 &\\nsleep 5\" >> $BSMASTER && \\\n    echo \"echo 'starting secondary namenode'\" >> $BSMASTER && \\\n    echo \"hdfs secondarynamenode > /dev/null 2>&1 &\\nsleep 5\" >> $BSMASTER && \\\n    echo \"echo 'starting datanode'\" >> $BSMASTER && \\\n    echo \"hdfs dfsadmin -safemode leave && hdfs datanode > /dev/null 2>&1\" \\\n        >> $BSMASTER\n# service ssh start && $HADOOP_PREFIX/sbin/start-dfs.sh\n# hdfs dfsadmin -safemode leave\nENV BSWORKER /bootslave.sh\nRUN echo \"#!/bin/bash\" > $BSWORKER && \\\n    echo \"spark-class org.apache.spark.deploy.worker.Worker spark://master:7077\" >> $BSWORKER\nRUN chmod +x $BSWORKER $BSMASTER\n\n#EXPOSE 7077\n#CMD [\"$BSMASTER\"]\n\n###########################################\n# WORKER test operation\n# hdfs dfs -put /data/books/pg1232.txt hdfs://master:9000/mybook.txt\n"
}
{
  "startTime": 1674234130675,
  "endTime": 1674234130849,
  "originalSmells": [
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 20,
        "lineEnd": 20,
        "columnStart": 4,
        "columnEnd": 32
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 23,
        "lineEnd": 23,
        "columnStart": 4,
        "columnEnd": 30
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM scalabase:latest\n\nEXPOSE 8081\nEXPOSE 8080\nEXPOSE 8088\nEXPOSE 7077\nEXPOSE 9870\nEXPOSE 4040\n\nRUN useradd -m -s /bin/bash hadoop\n\nWORKDIR /home/hadoop\n\nUSER hadoop\nRUN  wget https://archive.apache.org/dist/hadoop/core/hadoop-3.2.0/hadoop-3.2.0.tar.gz\nRUN  wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-without-hadoop.tgz\n#RUN chown hadoop /home/hadoop\n\nRUN tar -zxf hadoop-3.2.0.tar.gz && rm hadoop-3.2.0.tar.gz\nRUN mv hadoop-3.2.0 hadoop\n\nRUN tar -zxf spark-2.4.0-*.tgz && rm spark-2.4.0-*.tgz\nRUN rm *.tgz\nRUN mv spark-2.4.0-* spark\n#RUN chown hadoop spark -R\n\nRUN mkdir /home/hadoop/.ssh\nRUN mkdir /home/hadoop/hadoop/logs\nRUN touch /home/hadoop/hadoop/logs/fairscheduler-statedump.log\nRUN echo PubkeyAcceptedKeyTypes +ssh-dss >> /home/hadoop/.ssh/config\nRUN echo PasswordAuthentication no >> /home/hadoop/.ssh/config\n\nCOPY --chown=hadoop config/id_rsa.pub /home/hadoop/.ssh/id_rsa.pub\nCOPY --chown=hadoop config/id_rsa /home/hadoop/.ssh/id_rsa\nRUN cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys\n#RUN chown hadoop .ssh -R\n\nRUN echo PATH=/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin:/home/hadoop/spark/bin:$PATH >> /home/hadoop/.profile\nRUN echo PATH=/home/hadoop/hadoop/bin:/home/hadoop/hadoop/sbin:/home/hadoop/spark/bin:$PATH >> /home/hadoop/.bashrc\nRUN mkdir -p /home/hadoop/data/nameNode /home/hadoop/data/dataNode /home/hadoop/data/namesecondary /home/hadoop/data/tmp\n#RUN chown hadoop /home/hadoop/data/nameNode /home/hadoop/data/dataNode /home/hadoop/data/namesecondary /home/hadoop/data/tmp /home/hadoop/spark\nRUN echo HADOOP_HOME=/home/hadoop/hadoop >> /home/hadoop/.bashrc\n#RUN chown hadoop /home/hadoop/.profile /home/hadoop/.bashrc\nRUN echo JAVA_HOME=/usr/local/openjdk-8 >> /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh\nRUN echo HDFS_NAMENODE_USER=hadoop >> /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh\nRUN echo HDFS_DATANODE_USER=hadoop >> /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh\nRUN echo HDFS_SECONDARYNAMENODE_USER=hadoop >> /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh\n\n# Spark\nRUN echo \"export LD_LIBRARY_PATH=/home/hadoop/hadoop/lib/native:$LD_LIBRARY_PATH\" >> /home/hadoop/.bashrc\nRUN echo \"export LD_LIBRARY_PATH=/home/hadoop/hadoop/lib/native:$LD_LIBRARY_PATH\" >> /home/hadoop/.profile\nRUN echo \"export HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop\" >> /home/hadoop/.bashrc\nRUN echo \"export HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop\" >> /home/hadoop/.profile\nRUN echo \"export SPARK_DIST_CLASSPATH=\\$(hadoop classpath)\" >> /home/hadoop/.bashrc\nRUN echo \"export SPARK_DIST_CLASSPATH=\\$(hadoop classpath)\" >> /home/hadoop/.profile\nRUN echo \"export SPARK_HOME=/home/hadoop/spark\" >> /home/hadoop/.profile\nRUN echo \"export SPARK_HOME=/home/hadoop/spark\" >> /home/hadoop/.bashrc\nCOPY --chown=hadoop config/workers /home/hadoop/spark/conf/slaves\nCOPY --chown=hadoop config/sparkcmd.sh /home/hadoop/\n#RUN chown hadoop /home/hadoop/*\n\nCOPY --chown=hadoop config/core-site.xml config/hdfs-site.xml config/mapred-site.xml config/yarn-site.xml config/workers /home/hadoop/hadoop/etc/hadoop/\n#RUN chown hadoop /home/hadoop/hadoop/etc/hadoop/*\nUSER root\n#ENTRYPOINT [\"/home/hadoop/sparkcmd.sh\",\"start\"]\nCMD service ssh start && sleep infinity\n"
}
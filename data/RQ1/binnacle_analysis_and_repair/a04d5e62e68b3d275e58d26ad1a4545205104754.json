{
  "startTime": 1674249849582,
  "endTime": 1674249850003,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 10,
        "lineEnd": 10,
        "columnStart": 4,
        "columnEnd": 120
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 52,
        "lineEnd": 52,
        "columnStart": 4,
        "columnEnd": 59
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 10,
        "lineEnd": 10,
        "columnStart": 4,
        "columnEnd": 120
      }
    },
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 17,
        "lineEnd": 17,
        "columnStart": 4,
        "columnEnd": 97
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 55,
        "lineEnd": 55,
        "columnStart": 4,
        "columnEnd": 39
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 56,
        "lineEnd": 56,
        "columnStart": 4,
        "columnEnd": 21
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 57,
        "lineEnd": 57,
        "columnStart": 4,
        "columnEnd": 26
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 58,
        "lineEnd": 58,
        "columnStart": 4,
        "columnEnd": 22
      }
    },
    {
      "rule": "configureShouldUseBuildFlag",
      "position": {
        "lineStart": 33,
        "lineEnd": 33,
        "columnStart": 4,
        "columnEnd": 15
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 18,
        "lineEnd": 18,
        "columnStart": 4,
        "columnEnd": 57
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 31,
        "lineEnd": 31,
        "columnStart": 4,
        "columnEnd": 34
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 25,
        "lineEnd": 25,
        "columnStart": 4,
        "columnEnd": 69
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 26,
        "lineEnd": 26,
        "columnStart": 4,
        "columnEnd": 64
      }
    },
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 51,
        "lineEnd": 51,
        "columnStart": 4,
        "columnEnd": 65
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 25,
        "lineEnd": 25,
        "columnStart": 4,
        "columnEnd": 69
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 26,
        "lineEnd": 26,
        "columnStart": 4,
        "columnEnd": 64
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM java:openjdk-8-jdk\n\n# Get Spark from US Apache mirror.\nENV APACHE_SPARK_VERSION 2.4.0\nENV HADOOP_VERSION 3.2.0\nENV HADOOP_GIT_COMMIT=\"release-3.2.0-RC1\"\n\nRUN echo \"$LOG_TAG Getting SPARK_HOME\" && \\\n    mkdir -p /opt && \\\n    cd /opt && \\\n    curl -f https://apache.claz.org/spark/spark-${APACHE_SPARK_VERSION}/spark-${APACHE_SPARK_VERSION}-bin-without-hadoop.tgz | \\\n        tar -xz && \\\n    ln -s spark-${APACHE_SPARK_VERSION}-bin-without-hadoop spark && \\\n    echo Spark ${APACHE_SPARK_VERSION} installed in /opt/spark && \\\n    export SPARK_HOME=/opt/spark\n\nRUN echo \"$LOG_TAG Getting maven\" && \\\n    wget https://www.eu.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz && \\\n    tar -zxf apache-maven-3.3.9-bin.tar.gz -C /usr/local/ && \\\n    ln -s /usr/local/apache-maven-3.3.9/bin/mvn /usr/local/bin/mvn && rm apache-maven-3.3.9-bin.tar.gz\n\nRUN echo \"$LOG_TAG building hadoop\" && \\\n    echo \"deb http://deb.debian.org/debian stretch main\" >> /etc/apt/sources.list && \\\n    apt-get update && \\\n    # build deps and deps for c bindings for cntk\n    apt-get install --no-install-recommends -y g++ gcc-6 libstdc++-6-dev make build-essential && \\\n    apt-get install --no-install-recommends -y autoconf automake libtool curl make unzip && \\\n    cd  / && \\\n    git clone https://github.com/apache/hadoop.git  hadoop_src && \\\n    mkdir /hadoop_deps && cd /hadoop_deps && \\\n    wget https://github.com/protocolbuffers/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.bz2 && \\\n    tar xvf protobuf-2.5.0.tar.bz2 && \\\n    cd protobuf-2.5.0 && \\\n    ./configure --build=\"$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)\" && make && make install && ldconfig && \\\n    cd /hadoop_src && git checkout ${HADOOP_GIT_COMMIT} && mvn package -Pdist -DskipTests -Dtar && \\\n    mv hadoop-dist/target/hadoop-${HADOOP_VERSION} /opt/hadoop && \\\n    rm -r /hadoop_src && \\\n    export HADOOP_HOME=/opt/hadoop && \\\n    echo \"\\nexport HADOOP_CLASSPATH=/opt/hadoop/share/hadoop/tools/lib/*\" >> /opt/hadoop/etc/hadoop/hadoop-env.sh && \\\n    echo Hadoop ${HADOOP_VERSION} installed in /opt/hadoop && \\\n    apt-get purge -y --auto-remove g++ make build-essential autoconf automake && \\\n    cd  / && rm -rf /hadoop_deps && rm protobuf-2.5.0.tar.bz2 && rm -rf /var/lib/apt/lists/*;\n\nRUN echo \"\\nSPARK_DIST_CLASSPATH=/jars:/jars/*:$(/opt/hadoop/bin/hadoop classpath)\" >> /opt/spark/conf/spark-env.sh\nENV HADOOP_HOME=/opt/hadoop\nADD jars /jars\n\n\n# if numpy is installed on a driver it needs to be installed on all\n# workers, so install it everywhere\nRUN apt-get update && \\\n    apt-get install --no-install-recommends -y g++ python-dev build-essential python3-dev && \\\n    curl -f https://bootstrap.pypa.io/get-pip.py -o get-pip.py && \\\n    python get-pip.py && \\\n    rm get-pip.py && \\\n    pip install --no-cache-dir -U pip setuptools wheel && \\\n    pip install --no-cache-dir numpy && \\\n    pip install --no-cache-dir matplotlib && \\\n    pip install --no-cache-dir pandas && \\\n    apt-get purge -y --auto-remove python-dev build-essential python3-dev && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n\nADD log4j.properties /opt/spark/conf/log4j.properties\nADD start-common.sh start-worker start-master /\nADD core-site.xml /opt/spark/conf/core-site.xml\nADD spark-defaults.conf /opt/spark/conf/spark-defaults.conf\nENV PATH $PATH:/opt/spark/bin\n"
}
{
  "startTime": 1674252183326,
  "endTime": 1674252184034,
  "originalSmells": [
    {
      "rule": "wgetUseHttpsUrl",
      "position": {
        "lineStart": 126,
        "lineEnd": 126,
        "columnStart": 4,
        "columnEnd": 90
      }
    },
    {
      "rule": "tarSomethingRmTheSomething",
      "position": {
        "lineStart": 128,
        "lineEnd": 128,
        "columnStart": 4,
        "columnEnd": 38
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 105,
        "lineEnd": 105,
        "columnStart": 22,
        "columnEnd": 68
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 161,
        "lineEnd": 171,
        "columnStart": 4,
        "columnEnd": 19
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 176,
        "lineEnd": 177,
        "columnStart": 8,
        "columnEnd": 28
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 180,
        "lineEnd": 181,
        "columnStart": 8,
        "columnEnd": 36
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 328,
        "lineEnd": 330,
        "columnStart": 4,
        "columnEnd": 22
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Copyright (c) 2018-2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#\n# Multistage build.\n#\n\nARG BASE_IMAGE=nvcr.io/nvidia/tensorrtserver:19.05-py3\nARG PYTORCH_IMAGE=nvcr.io/nvidia/pytorch:19.05-py3\nARG TENSORFLOW_IMAGE=nvcr.io/nvidia/tensorflow:19.05-py3\n\n############################################################################\n## TensorFlow stage: Use TensorFlow container to build\n############################################################################\nFROM ${TENSORFLOW_IMAGE} AS trtserver_tf\n\n# Modify the TF model loader to allow us to set the default GPU for\n# multi-GPU support\nCOPY tools/patch/tensorflow /tmp/trtis/tools/patch/tensorflow\nRUN sha1sum -c /tmp/trtis/tools/patch/tensorflow/checksums && \\\n    patch -i /tmp/trtis/tools/patch/tensorflow/cc/saved_model/loader.cc \\\n          /opt/tensorflow/tensorflow/cc/saved_model/loader.cc && \\\n    patch -i /tmp/trtis/tools/patch/tensorflow/BUILD \\\n          /opt/tensorflow/tensorflow/BUILD && \\\n    patch -i /tmp/trtis/tools/patch/tensorflow/tf_version_script.lds \\\n          /opt/tensorflow/tensorflow/tf_version_script.lds && \\\n    patch -i /tmp/trtis/tools/patch/tensorflow/nvbuild.sh \\\n          /opt/tensorflow/nvbuild.sh && \\\n    patch -i /tmp/trtis/tools/patch/tensorflow/nvbuildopts \\\n          /opt/tensorflow/nvbuildopts && \\\n    patch -i /tmp/trtis/tools/patch/tensorflow/bazel_build.sh \\\n          /opt/tensorflow/bazel_build.sh\n\n# Copy tensorflow_backend_tf into TensorFlow so it builds into the\n# monolithic libtensorflow_cc library. We want tensorflow_backend_tf\n# to build against the TensorFlow protobuf since it interfaces with\n# that code.\nCOPY src/backends/tensorflow/tensorflow_backend_tf.* \\\n     /opt/tensorflow/tensorflow/\n\n# Build TensorFlow library for TRTIS\nWORKDIR /opt/tensorflow\nRUN ./nvbuild.sh --python3.5\n\n############################################################################\n## PyTorch stage: Use PyTorch container for Caffe2 and libtorch\n############################################################################\nFROM ${PYTORCH_IMAGE} AS trtserver_caffe2\n\n# Copy netdef_backend_c2 into Caffe2 core so it builds into the\n# libcaffe2 library. We want netdef_backend_c2 to build against the\n# Caffe2 protobuf since it interfaces with that code.\nCOPY src/backends/caffe2/netdef_backend_c2.* \\\n     /opt/pytorch/pytorch/caffe2/core/\n\n# Build same as in pytorch container... except for the NO_DISTRIBUTED\n# line where we turn off features not needed for trtserver\n# This will build both the caffe2 libraries needed by the Caffe2 NetDef backend\n# and the LibTorch library needed by the PyTorch backend.\nWORKDIR /opt/pytorch\nRUN pip uninstall -y torch\nRUN cd pytorch && \\\n    TORCH_CUDA_ARCH_LIST=\"5.2 6.0 6.1 7.0 7.5+PTX\" \\\n     CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" \\\n     USE_DISTRIBUTED=0 USE_MIOPEN=0 USE_NCCL=0 \\\n     USE_OPENCV=0 USE_LEVELDB=0 USE_LMDB=0 USE_REDIS=0 \\\n     BUILD_TEST=0 \\\n     pip install --no-cache-dir -v .\n\n############################################################################\n## Onnx Runtime stage: Build Onnx Runtime on CUDA 10, CUDNN 7\n############################################################################\nFROM ${BASE_IMAGE} AS trtserver_onnx\n\n# Currently the prebuilt Onnx Runtime library is built on CUDA 9, thus it\n# needs to be built from source\n\n# Onnx Runtime release version\nARG ONNX_RUNTIME_VERSION=0.4.0\n\n# Get release version of Onnx Runtime\nWORKDIR /workspace\nRUN apt-get update && apt-get install -y --no-install-recommends git && rm -rf /var/lib/apt/lists/*;\n\n# Check out master until new release to support cloud-based filesystems\nRUN git clone --recursive https://github.com/Microsoft/onnxruntime\n\nENV PATH=\"/opt/cmake/bin:${PATH}\"\nARG SCRIPT_DIR=/workspace/onnxruntime/tools/ci_build/github/linux/docker/scripts\nRUN cp -r ${SCRIPT_DIR} /tmp/scripts && \\\n    ${SCRIPT_DIR}/install_ubuntu.sh && ${SCRIPT_DIR}/install_deps.sh\n\n# Allow configure to pick up GDK and CuDNN where it expects it.\n# (Note: $CUDNN_VERSION is defined by NVidia's base image)\nRUN _CUDNN_VERSION=$(echo $CUDNN_VERSION | cut -d. -f1-2) && \\\n    mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/include && \\\n    ln -s /usr/include/cudnn.h /usr/local/cudnn-$_CUDNN_VERSION/cuda/include/cudnn.h && \\\n    mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64 && \\\n    ln -s /etc/alternatives/libcudnn_so /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64/libcudnn.so\n\n# Build and Install LLVM\nARG LLVM_VERSION=6.0.1\nRUN cd /tmp && \\\n    wget --no-verbose https://releases.llvm.org/$LLVM_VERSION/llvm-$LLVM_VERSION.src.tar.xz && \\\n    xz -d llvm-$LLVM_VERSION.src.tar.xz && \\\n    tar xvf llvm-$LLVM_VERSION.src.tar && \\\n    cd llvm-$LLVM_VERSION.src && \\\n    mkdir -p build && \\\n    cd build && \\\n    cmake .. -DCMAKE_BUILD_TYPE=Release && \\\n    cmake --build . -- -j$(nproc) && \\\n    cmake -DCMAKE_INSTALL_PREFIX=/usr/local/llvm-$LLVM_VERSION -DBUILD_TYPE=Release -P cmake_install.cmake && \\\n    cd /tmp && \\\n    rm -rf llvm* && rm llvm-$LLVM_VERSION.src.tar\n\nENV LD_LIBRARY_PATH /usr/local/openblas/lib:$LD_LIBRARY_PATH\n\n# Build files will be in /workspace/build\nARG COMMON_BUILD_ARGS=\"--skip_submodule_sync --parallel --build_shared_lib --use_openmp\"\nRUN mkdir -p /workspace/build\nRUN python3 /workspace/onnxruntime/tools/ci_build/build.py --build_dir /workspace/build \\\n            --config Release $COMMON_BUILD_ARGS \\\n            --use_cuda \\\n            --cuda_home /usr/local/cuda \\\n            --cudnn_home /usr/local/cudnn-$(echo $CUDNN_VERSION | cut -d. -f1-2)/cuda \\\n            --update \\\n            --build\n\n############################################################################\n## Build stage: Build inference server\n############################################################################\nFROM ${BASE_IMAGE} AS trtserver_build\n\nARG TRTIS_VERSION=1.4.0dev\nARG TRTIS_CONTAINER_VERSION=19.07dev\n\n# libgoogle-glog0v5 is needed by caffe2 libraries.\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n            software-properties-common \\\n            autoconf \\\n            automake \\\n            build-essential \\\n            cmake \\\n            git \\\n            libgoogle-glog0v5 \\\n            libre2-dev \\\n            libssl-dev \\\n            libtool && rm -rf /var/lib/apt/lists/*;\n\n# libcurl4-openSSL-dev is needed for GCS\nRUN if [ $(cat /etc/os-release | grep 'VERSION_ID=\"16.04\"' | wc -l) -ne 0 ]; then \\\n        apt-get update && \\\n        apt-get install -y --no-install-recommends \\\n                libcurl3-dev; rm -rf /var/lib/apt/lists/*; \\\n    elif [ $(cat /etc/os-release | grep 'VERSION_ID=\"18.04\"' | wc -l) -ne 0 ]; then \\\n        apt-get update && \\\n        apt-get install -y --no-install-recommends \\\n                libcurl4-openssl-dev; rm -rf /var/lib/apt/lists/*; \\\n    else \\\n        echo \"Ubuntu version must be either 16.04 or 18.04\" && \\\n        exit 1; \\\n    fi\n\n# TensorFlow libraries. Install the monolithic libtensorflow_cc and\n# create a link libtensorflow_framework.so -> libtensorflow_cc.so so\n# that custom tensorflow operations work correctly. Custom TF\n# operations link against libtensorflow_framework.so so it must be\n# present (and its functionality is provided by libtensorflow_cc.so).\nCOPY --from=trtserver_tf \\\n     /usr/local/lib/tensorflow/libtensorflow_cc.so /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib && \\\n    ln -s libtensorflow_cc.so libtensorflow_framework.so\n\n# Caffe2 libraries\nCOPY --from=trtserver_caffe2 \\\n     /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_detectron_ops_gpu.so \\\n     /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 \\\n     /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2.so \\\n     /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 \\\n     /opt/conda/lib/python3.6/site-packages/torch/lib/libcaffe2_gpu.so \\\n     /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 \\\n     /opt/conda/lib/python3.6/site-packages/torch/lib/libc10.so \\\n     /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 \\\n     /opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so \\\n     /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_avx2.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_core.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_def.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_gnu_thread.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_intel_lp64.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_rt.so /opt/tensorrtserver/lib/\nCOPY --from=trtserver_caffe2 /opt/conda/lib/libmkl_vml_def.so /opt/tensorrtserver/lib/\n\n# LibTorch headers and library\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/include \\\n     /opt/tensorrtserver/include/torch\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so.1 \\\n      /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib && \\\n    ln -s libtorch.so.1 libtorch.so\nCOPY --from=trtserver_caffe2 /opt/conda/lib/python3.6/site-packages/torch/lib/libthnvrtc.so \\\n      /opt/tensorrtserver/lib/\n\n# Onnx Runtime headers and library\nARG ONNX_RUNTIME_VERSION=0.4.0\nCOPY --from=trtserver_onnx /workspace/onnxruntime/include/onnxruntime \\\n     /opt/tensorrtserver/include/onnxruntime/\nCOPY --from=trtserver_onnx /workspace/build/Release/libonnxruntime.so.${ONNX_RUNTIME_VERSION} \\\n     /opt/tensorrtserver/lib/\nRUN cd /opt/tensorrtserver/lib && \\\n    ln -s libonnxruntime.so.${ONNX_RUNTIME_VERSION} libonnxruntime.so\n\n# Copy entire repo into container even though some is not needed for\n# build itself... because we want to be able to copyright check on\n# files that aren't directly needed for build.\nWORKDIR /workspace\nRUN rm -fr *\nCOPY . .\n\n# Build the server.\n#\n# - Need to find CUDA stubs if they are available since some backends\n# may need to link against them. This is identical to the login in TF\n# container nvbuild.sh\n#\n# - The build can fail the first time due to a protobuf_generate_cpp\n# error that doesn't repeat on subsequent builds, which is why there\n# are 2 make invocations below.\nRUN LIBCUDA_FOUND=$(ldconfig -p | grep -v compat | awk '{print $1}' | grep libcuda.so | wc -l) && \\\n    if [[ \"$LIBCUDA_FOUND\" -eq 0 ]]; then \\\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64/stubs; \\\n        ln -fs /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1; \\\n    fi && \\\n    echo $LD_LIBRARY_PATH && \\\n    rm -fr builddir && mkdir -p builddir && \\\n    (cd builddir && \\\n            cmake -DCMAKE_BUILD_TYPE=Release \\\n                  -DTRTIS_ENABLE_METRICS=ON \\\n                  -DTRTIS_ENABLE_GCS=ON\\\n                  -DTRTIS_ENABLE_CUSTOM=ON \\\n                  -DTRTIS_ENABLE_TENSORFLOW=ON \\\n                  -DTRTIS_ENABLE_TENSORRT=ON \\\n                  -DTRTIS_ENABLE_CAFFE2=ON \\\n                  -DTRTIS_ENABLE_ONNXRUNTIME=ON \\\n                  -DTRTIS_ENABLE_PYTORCH=ON \\\n                  -DTRTIS_ONNXRUNTIME_INCLUDE_PATHS=\"/opt/tensorrtserver/include/onnxruntime\" \\\n                  -DTRTIS_PYTORCH_INCLUDE_PATHS=\"/opt/tensorrtserver/include/torch\" \\\n                  -DTRTIS_EXTRA_LIB_PATHS=\"/opt/tensorrtserver/lib\" \\\n                  ../build && \\\n            (make -j16 trtis || true) && \\\n            make -j16 trtis && \\\n            mkdir -p /opt/tensorrtserver/include && \\\n            cp -r trtis/install/bin /opt/tensorrtserver/. && \\\n            cp -r trtis/install/lib /opt/tensorrtserver/. && \\\n            cp -r trtis/install/include /opt/tensorrtserver/include/trtserver) && \\\n    (cd /opt/tensorrtserver && ln -s /workspace/qa qa)\n\nENV TENSORRT_SERVER_VERSION ${TRTIS_VERSION}\nENV NVIDIA_TENSORRT_SERVER_VERSION ${TRTIS_CONTAINER_VERSION}\n\nENV LD_LIBRARY_PATH /opt/tensorrtserver/lib:${LD_LIBRARY_PATH}\nENV PATH /opt/tensorrtserver/bin:${PATH}\n\nCOPY nvidia_entrypoint.sh /opt/tensorrtserver\nENTRYPOINT [\"/opt/tensorrtserver/nvidia_entrypoint.sh\"]\n\n############################################################################\n##  Production stage: Create container with just inference server executable\n############################################################################\nFROM ${BASE_IMAGE}\n\nARG TRTIS_VERSION=1.4.0dev\nARG TRTIS_CONTAINER_VERSION=19.07dev\n\nENV TENSORRT_SERVER_VERSION ${TRTIS_VERSION}\nENV NVIDIA_TENSORRT_SERVER_VERSION ${TRTIS_CONTAINER_VERSION}\nLABEL com.nvidia.tensorrtserver.version=\"${TENSORRT_SERVER_VERSION}\"\n\nENV LD_LIBRARY_PATH /opt/tensorrtserver/lib:${LD_LIBRARY_PATH}\nENV PATH /opt/tensorrtserver/bin:${PATH}\n\nENV TF_ADJUST_HUE_FUSED         1\nENV TF_ADJUST_SATURATION_FUSED  1\nENV TF_ENABLE_WINOGRAD_NONFUSED 1\nENV TF_AUTOTUNE_THRESHOLD       2\n\n# Needed by Caffe2 libraries to avoid:\n# Intel MKL FATAL ERROR: Cannot load libmkl_intel_thread.so\nENV MKL_THREADING_LAYER GNU\n\n# Create a user that can be used to run the tensorrt-server as\n# non-root. Make sure that this user to given ID 1000.\nENV TENSORRT_SERVER_USER=tensorrt-server\nRUN id -u $TENSORRT_SERVER_USER > /dev/null 2>&1 || \\\n    useradd $TENSORRT_SERVER_USER && \\\n    [ `id -u $TENSORRT_SERVER_USER` -eq 1000 ] && \\\n    [ `id -g $TENSORRT_SERVER_USER` -eq 1000 ]\n\n# libgoogle-glog0v5 is needed by caffe2 libraries.\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n            libgoogle-glog0v5 \\\n            libre2-1v5 && rm -rf /var/lib/apt/lists/*;\n\nWORKDIR /opt/tensorrtserver\nRUN rm -fr /opt/tensorrtserver/*\nCOPY LICENSE .\nCOPY --from=trtserver_onnx /workspace/onnxruntime/LICENSE LICENSE.onnxruntime\nCOPY --from=trtserver_tf /opt/tensorflow/LICENSE LICENSE.tensorflow\nCOPY --from=trtserver_caffe2 /opt/pytorch/pytorch/LICENSE LICENSE.pytorch\nCOPY --from=trtserver_build /opt/tensorrtserver/bin/trtserver bin/\nCOPY --from=trtserver_build /opt/tensorrtserver/lib lib\nCOPY --from=trtserver_build /opt/tensorrtserver/include include\n\nRUN chmod ugo-w+rx /opt/tensorrtserver/lib/*.so\n\n# Extra defensive wiring for CUDA Compat lib\nRUN ln -sf ${_CUDA_COMPAT_PATH}/lib.real ${_CUDA_COMPAT_PATH}/lib \\\n && echo ${_CUDA_COMPAT_PATH}/lib > /etc/ld.so.conf.d/00-cuda-compat.conf \\\n && ldconfig \\\n && rm -f ${_CUDA_COMPAT_PATH}/lib\n\nCOPY nvidia_entrypoint.sh /opt/tensorrtserver\nENTRYPOINT [\"/opt/tensorrtserver/nvidia_entrypoint.sh\"]\n\nARG NVIDIA_BUILD_ID\nENV NVIDIA_BUILD_ID ${NVIDIA_BUILD_ID:-<unknown>}\nLABEL com.nvidia.build.id=\"${NVIDIA_BUILD_ID}\"\nARG NVIDIA_BUILD_REF\nLABEL com.nvidia.build.ref=\"${NVIDIA_BUILD_REF}\"\n"
}
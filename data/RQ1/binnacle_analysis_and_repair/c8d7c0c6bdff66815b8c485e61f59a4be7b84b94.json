{
  "startTime": 1674252593402,
  "endTime": 1674252593504,
  "originalSmells": [
    {
      "rule": "aptGetInstallUseNoRec",
      "position": {
        "lineStart": 3,
        "lineEnd": 7,
        "columnStart": 22,
        "columnEnd": 12
      }
    },
    {
      "rule": "aptGetInstallThenRemoveAptLists",
      "position": {
        "lineStart": 3,
        "lineEnd": 7,
        "columnStart": 22,
        "columnEnd": 12
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM ubuntu:16.04\nMaintainer Ethan J. Jackson\n\nRUN apt-get update && apt-get install --no-install-recommends -y \\\n        default-jre-headless \\\n\t# wget is used to download Spark, and is used (once the container starts) to\n\t# get the container's public IP address by reading checkip.amazonaws.com.\n        wget \\\n&& wget -qO- https://www-us.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz | tar -xzf - \\\n&& mv /spark* /spark \\\n# Add the AWS jars so Spark can connect to S3.\n&& wget -q -O /spark/jars/aws-java-sdk.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar \\\n&& wget -q -O /spark/jars/hadoop-aws.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar \\\n&& rm -rf /var/lib/lists/* /tmp/* /var/tmp/* && rm -rf /var/lib/apt/lists/*;\n\n# Create a directory for the Spark event log, which stores information about\n# past jobs that have completed. Spark requires this directory to be created;\n# Spark will not create the directory itself. This filepath is the default\n# location that Spark uses.  If the user changes the default location by\n# configuring spark.eventLog.dir, this path will need to be updated to match.\nRUN mkdir -p /tmp/spark-events\n\nENV PATH /spark/sbin:/spark/bin:$PATH\n"
}
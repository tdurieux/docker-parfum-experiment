{
  "startTime": 1674249054274,
  "endTime": 1674249054516,
  "originalSmells": [
    {
      "rule": "yumInstallRmVarCacheYum",
      "position": {
        "lineStart": 23,
        "lineEnd": 23,
        "columnStart": 4,
        "columnEnd": 96
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM centos:centos7\n\nARG DATAWAVE_COMMIT_ID\nARG DATAWAVE_BRANCH_NAME\n\nUSER root\n\n# ENV params for hadoop here are a hack/fix for the \"chown: missing operand after '**/hadoop/logs'\"\n# errors thrown by hadoop startup scripts for both the resourcemanager and historyserver. Those\n# errors occur due to $USER being mysteriously null/undefined at the point that log file names are\n# established and subsequently chowned in those scripts. But aside from the chown errors and a few\n# irregularly-named log files, the null $USER issue doesn't seem to have any other negative impact.\n# Mostly an annoyance and odd that it only seems to occur in Docker, which is why I'm documenting\n# it here (issue has been observed w/ both centos6 and centos7 base images)\n\nENV YARN_IDENT_STRING=root HADOOP_MAPRED_IDENT_STRING=root\n\n# Build context should be the DataWave source root, minus .git and other dirs. See .dockerignore\n\nCOPY . /opt/datawave\n\n# Install dependencies, configure password-less/zero-prompt SSH...\n\nRUN yum -y install openssl openssh openssh-server openssh-clients openssl-libs which bc wget git && \\\n    yum clean all && \\\n    ssh-keygen -q -N \"\" -t rsa -f ~/.ssh/id_rsa && \\\n    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \\\n    chmod 0600 ~/.ssh/authorized_keys && \\\n    ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key && \\\n    ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key && \\\n    ssh-keygen -q -N \"\" -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key && \\\n    ssh-keygen -q -N \"\" -t ed25519 -f /etc/ssh/ssh_host_ed25519_key && \\\n    echo \"StrictHostKeyChecking no\" >> /etc/ssh/ssh_config && \\\n    echo \"UserKnownHostsFile /dev/null\" >> /etc/ssh/ssh_config && \\\n    echo \"LogLevel QUIET\" >> /etc/ssh/ssh_config && rm -rf /var/cache/yum\n\nWORKDIR /opt/datawave\n\n# Create new Git repo for convenience...\n\nRUN rm -f .dockerignore && \\\n    git init && \\\n    git add . && \\\n    git config user.email \"root@localhost.local\" && \\\n    git config user.name \"Root User\" && \\\n    git commit -m \"Source Branch :: $DATAWAVE_BRANCH_NAME :: Source Commit :: $DATAWAVE_COMMIT_ID\"\n\n# This works exactly like the setup for a non-containerized instance of the datawave-quickstart\n# environment. That is, ~/.bashrc and datawave-quickstart/bin/env.sh are sourced, bootstrapping\n# the quickstart environment. Likewise, 'allInstall' and 'datawaveStart' wrapper functions are\n# used to initialize services and their log dirs. Finally, web services are tested, services are\n# stopped gracefully, and any cruft is purged.\n\n# By design, if 'datawaveWebTest' exits with non-zero status, the docker build will fail\n\nRUN /bin/bash -c \"/usr/bin/nohup /usr/sbin/sshd -D &\" && \\\n    echo \"export DW_ACCUMULO_DIST_URI=file://$(find /opt/datawave/contrib/datawave-quickstart/bin/services/accumulo -name 'accumulo*.tar.gz')\" >> ~/.bashrc && \\\n    echo \"export DW_ZOOKEEPER_DIST_URI=file://$(find /opt/datawave/contrib/datawave-quickstart/bin/services/accumulo -name 'zookeeper*.tar.gz')\" >> ~/.bashrc && \\\n    echo \"export DW_JAVA_DIST_URI=file://$(find /opt/datawave/contrib/datawave-quickstart/bin/services/java -name '*tar.gz')\" >> ~/.bashrc && \\\n    echo \"export DW_MAVEN_DIST_URI=file://$(find /opt/datawave/contrib/datawave-quickstart/bin/services/maven -name 'apache-maven*.tar.gz')\" >> ~/.bashrc && \\\n    echo \"export DW_HADOOP_DIST_URI=file://$(find /opt/datawave/contrib/datawave-quickstart/bin/services/hadoop -name 'hadoop*.tar.gz')\" >> ~/.bashrc && \\\n    echo \"export DW_WILDFLY_DIST_URI=file://$(find /opt/datawave/contrib/datawave-quickstart/bin/services/datawave -name 'wildfly*.tar.gz')\" >> ~/.bashrc && \\\n    echo \"source /opt/datawave/contrib/datawave-quickstart/bin/env.sh\" >> ~/.bashrc && \\\n    /bin/bash -c \"source ~/.bashrc && allInstall && datawaveStart && datawaveWebTest --verbose && allStop\" && \\\n    echo \"0.0.0.0\" > contrib/datawave-quickstart/accumulo/conf/monitor && \\\n    rm -rf contrib/datawave-quickstart/datawave-ingest/logs/* && \\\n    rm -rf contrib/datawave-quickstart/hadoop/logs/* && \\\n    rm -rf contrib/datawave-quickstart/accumulo/logs/* && \\\n    rm -rf contrib/datawave-quickstart/wildfly/standalone/log/*\n\n# Lastly, establish volumes for data, logs & other directories, wire up\n# the entrypoint & bootstrap scripts, expose ports, and set default CMD...\n\n# Primary data volume (for HDFS, Accumulo, ZooKeeper, etc)\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/data\"]\n\n# In case user wants to rebuild DW\nVOLUME [\"~/.m2/repository\"]\n\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/hadoop/logs\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/accumulo/logs\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/wildfly/standalone/log\"]\nVOLUME [\"/opt/datawave/contrib/datawave-quickstart/datawave-ingest/logs\"]\n\nEXPOSE 8443 9995 50070 8088 9000 2181\n\nWORKDIR /opt/datawave/contrib/datawave-quickstart\n\nRUN ln -s /opt/datawave/contrib/datawave-quickstart/docker/docker-entrypoint.sh /usr/local/bin/docker-entrypoint.sh && \\\n    ln -s /opt/datawave/contrib/datawave-quickstart/docker/datawave-bootstrap.sh /usr/local/bin/datawave-bootstrap.sh\n\n# The entrypoint script will ensure that sshd is started, and it'll simply 'exec \"$@\"' whatever command is passed\n\nENTRYPOINT [\"docker-entrypoint.sh\"]\n\n# Default CMD uses the bootstrap script to start up DataWave's web services. Due to the --bash flag, it'll\n# exec /bin/bash for the container process, intended for 'docker run -it ...' usage.\n\nCMD [\"datawave-bootstrap.sh\", \"--web\", \"--bash\"]\n\n# Without the --bash flag, datawave-bootstrap.sh will go into an infinite loop to prevent the container\n# from exiting, better for 'docker run -d ...' usage\n"
}
{
  "startTime": 1674239560881,
  "endTime": 1674239561320,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 120,
        "lineEnd": 120,
        "columnStart": 12,
        "columnEnd": 33
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 121,
        "lineEnd": 121,
        "columnStart": 12,
        "columnEnd": 38
      }
    },
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 122,
        "lineEnd": 122,
        "columnStart": 12,
        "columnEnd": 34
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# WARNING: THIS DOCKERFILE IS NOT INTENDED FOR PRODUCTION USE OR DEPLOYMENT. AT\n#          THIS POINT, THIS IS ONLY INTENDED FOR USE IN AUTOMATED TESTS.\n\nFROM ubuntu:xenial\n\nUSER root\n\nENV DEBIAN_FRONTEND noninteractive\nENV LANGUAGE en_US.UTF-8\nENV LANG en_US.UTF-8\nENV LC_ALL en_US.UTF-8\nENV LC_CTYPE en_US.UTF-8\nENV LC_MESSAGES en_US.UTF-8\n\nENV HADOOP_VERSION 2.6.0\nENV HADOOP_DISTRO=cdh\nENV HADOOP_HOME=/tmp/hadoop-${HADOOP_DISTRO}\nENV HIVE_HOME=/tmp/hive\nENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/\n\nRUN  mkdir ${HADOOP_HOME} && \\\n     mkdir ${HIVE_HOME}  && \\\n     mkdir /tmp/minicluster  && \\\n     mkdir -p /user/hive/warehouse && \\\n     chmod -R 777 ${HIVE_HOME} && \\\n     chmod -R 777 /user/\n\n# Add nodejs repo and key\nADD nodesource.gpg.key /tmp/nodesource.gpg.key\nRUN apt-key add /tmp/nodesource.gpg.key\nRUN echo 'deb http://deb.nodesource.com/node_8.x xenial main' > /etc/apt/sources.list.d/nodesource.list\nRUN echo 'deb-src http://deb.nodesource.com/node_8.x xenial main' >> /etc/apt/sources.list.d/nodesource.list\n\nRUN apt-get update && apt-get install --no-install-recommends -y \\\n      openjdk-8-jdk \\\n      wget curl \\\n      gcc \\\n      g++ \\\n      python-dev \\\n      python3-dev \\\n      python-pip \\\n      python3-pip \\\n      python-virtualenv \\\n      python3-venv \\\n      python-setuptools \\\n      python-pkg-resources \\\n      python3-setuptools \\\n      python3-pkg-resources \\\n      make \\\n      nodejs \\\n      vim \\\n      less \\\n      git \\\n      unzip \\\n      sudo \\\n      ldap-utils \\\n      mysql-client-core-5.7 \\\n      mysql-client-5.7 \\\n      libmysqlclient-dev \\\n      postgresql-client \\\n      sqlite3 \\\n      libkrb5-dev \\\n      libsasl2-dev \\\n      krb5-user \\\n      openssh-client \\\n      openssh-server \\\n      python-selinux \\\n      sasl2-bin \\\n      libsasl2-2 \\\n      libsasl2-dev \\\n      libsasl2-modules \\\n      locales \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen \\\n    && locale-gen \\\n    && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8\n\n# Install Hadoop\n# --absolute-names is a work around to avoid this issue https://github.com/docker/hub-feedback/issues/727\nRUN cd /tmp && \\\n    wget -q https://archive.cloudera.com/cdh5/cdh/5/hadoop-${HADOOP_VERSION}-cdh5.11.0.tar.gz && \\\n    tar xzf hadoop-${HADOOP_VERSION}-cdh5.11.0.tar.gz --absolute-names --strip-components 1 -C $HADOOP_HOME && \\\n    rm hadoop-${HADOOP_VERSION}-cdh5.11.0.tar.gz\n\n# Install Hive\nRUN cd /tmp && \\\n    wget -q https://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.11.0.tar.gz && \\\n    tar xzf hive-1.1.0-cdh5.11.0.tar.gz --strip-components 1 -C $HIVE_HOME && \\\n    rm hive-1.1.0-cdh5.11.0.tar.gz\n\n# Install MiniCluster\nRUN cd /tmp && \\\n    wget -q https://github.com/bolkedebruin/minicluster/releases/download/1.1/minicluster-1.1-SNAPSHOT-bin.zip && \\\n    unzip minicluster-1.1-SNAPSHOT-bin.zip -d /tmp && \\\n    rm minicluster-1.1-SNAPSHOT-bin.zip\n\nRUN adduser airflow && \\\n    echo \"airflow ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/airflow && \\\n    chmod 0440 /etc/sudoers.d/airflow\n\n# Install Python requirements\nRUN sudo -H pip install --upgrade pip && \\\n    sudo -H pip install --no-cache-dir wheel tox && \\\n    sudo -H pip3 install --no-cache-dir --upgrade pip && \\\n    sudo -H pip3 install --no-cache-dir wheel tox && \\\n    rm -rf ~/.cache\n\nEXPOSE 8080\n\nWORKDIR /home/airflow\n\nENV PATH \"$PATH:/tmp/hive/bin:$ADDITIONAL_PATH\"\n\nUSER airflow\n"
}
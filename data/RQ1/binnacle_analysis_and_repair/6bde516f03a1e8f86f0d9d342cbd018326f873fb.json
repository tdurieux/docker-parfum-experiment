{
  "startTime": 1674244690069,
  "endTime": 1674244690419,
  "originalSmells": [
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 26,
        "lineEnd": 26,
        "columnStart": 4,
        "columnEnd": 113
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 29,
        "lineEnd": 29,
        "columnStart": 4,
        "columnEnd": 88
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 81,
        "lineEnd": 81,
        "columnStart": 4,
        "columnEnd": 78
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 91,
        "lineEnd": 91,
        "columnStart": 4,
        "columnEnd": 92
      }
    },
    {
      "rule": "curlUseFlagF",
      "position": {
        "lineStart": 106,
        "lineEnd": 106,
        "columnStart": 4,
        "columnEnd": 181
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 29,
        "lineEnd": 29,
        "columnStart": 4,
        "columnEnd": 88
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 81,
        "lineEnd": 81,
        "columnStart": 4,
        "columnEnd": 78
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 91,
        "lineEnd": 91,
        "columnStart": 4,
        "columnEnd": 92
      }
    },
    {
      "rule": "curlUseHttpsUrl",
      "position": {
        "lineStart": 106,
        "lineEnd": 106,
        "columnStart": 4,
        "columnEnd": 181
      }
    },
    {
      "rule": "yumInstallRmVarCacheYum",
      "position": {
        "lineStart": 11,
        "lineEnd": 11,
        "columnStart": 4,
        "columnEnd": 82
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "# Daniel Malczyk\n# ThinkBig Analytics, a Teradata Company\n\n#image for a separate Hadoop/Spark container for Kylo\nFROM airhacks/java\n\nMAINTAINER Daniel Malczyk <dmalczyk@gmail.com>\n\n# install dev tools\nRUN yum clean all; \\\n    rpm --rebuilddb; \\\n    yum install -y curl which tar sudo openssh-server openssh-clients rsync mysql; rm -rf /var/cache/yum \\\n    yum clean all\n# update libselinux. see https://github.com/sequenceiq/hadoop-docker/issues/14\nRUN yum update -y libselinux\n\n# passwordless ssh\nRUN ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key\nRUN ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa\nRUN ssh-keygen -q -N \"\" -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key\nRUN ssh-keygen -q -N \"\" -t ed25519 -f /etc/ssh/ssh_host_ed25519_key\nRUN cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n\n# download native support\nRUN mkdir -p /tmp/native\nRUN curl -f -L https://github.com/sequenceiq/docker-hadoop-build/releases/download/v2.7.1/hadoop-native-64-2.7.1.tgz | tar -xz -C /tmp/native\n\n#Install hadoop to /usr/local/hadoop\nRUN curl -f -s https://www.eu.apache.org/dist/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz | tar -xz -C /usr/local/\nRUN cd /usr/local && ln -s ./hadoop-2.7.1 hadoop\n\nENV HADOOP_HOME /usr/local/hadoop\nENV HADOOP_INSTALL $HADOOP_HOME\nENV HADOOP_PREFIX $HADOOP_HOME\nENV PATH $PATH:$HADOOP_INSTALL/sbin\nENV HADOOP_MAPRED_HOME $HADOOP_INSTALL\nENV HADOOP_COMMON_HOME $HADOOP_INSTALL\nENV HADOOP_HDFS_HOME $HADOOP_INSTALL\nENV YARN_HOME $HADOOP_INSTALL\nENV PATH $HADOOP_HOME/bin:$PATH\n\nRUN sed -i '/^export JAVA_HOME/ s:.*:export JAVA_HOME=/etc/alternatives/java_sdk\\nexport HADOOP_PREFIX=/usr/local/hadoop\\nexport HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\nRUN sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n\nRUN mkdir $HADOOP_PREFIX/input\nRUN cp $HADOOP_PREFIX/etc/hadoop/*.xml $HADOOP_PREFIX/input\n\n# pseudo distributed\nCOPY conf/core-site.xml.template2 $HADOOP_PREFIX/etc/hadoop/\nRUN sed s/HOSTNAME/localhost/ /usr/local/hadoop/etc/hadoop/core-site.xml.template2 > /usr/local/hadoop/etc/hadoop/core-site.xml\nADD conf/hdfs-site.xml $HADOOP_PREFIX/etc/hadoop/hdfs-site.xml\n\nADD conf/mapred-site.xml $HADOOP_PREFIX/etc/hadoop/mapred-site.xml\nADD conf/yarn-site.xml $HADOOP_PREFIX/etc/hadoop/yarn-site.xml\n\nRUN $HADOOP_PREFIX/bin/hdfs namenode -format\n\n# fixing the libhadoop.so like a boss\nRUN rm -rf /usr/local/hadoop/lib/native\nRUN mv /tmp/native /usr/local/hadoop/lib\n\nADD conf/ssh_config /root/.ssh/config\nRUN chmod 600 /root/.ssh/config\nRUN chown root:root /root/.ssh/config\n\n# workingaround docker.io build error\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh\nRUN ls -la /usr/local/hadoop/etc/hadoop/*-env.sh\n\n# fix the 254 error code\nRUN sed  -i \"/^[^#]*UsePAM/ s/.*/#&/\"  /etc/ssh/sshd_config\nRUN echo \"UsePAM no\" >> /etc/ssh/sshd_config\nRUN echo \"Port 2122\" >> /etc/ssh/sshd_config\n\nRUN /usr/sbin/sshd && $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh && $HADOOP_PREFIX/sbin/start-dfs.sh && $HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/root\nRUN /usr/sbin/sshd && $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh && $HADOOP_PREFIX/sbin/start-dfs.sh && $HADOOP_PREFIX/bin/hdfs dfs -put $HADOOP_PREFIX/etc/hadoop/ input\n\n#Install spark to /usr/local/spark\n#support for Hadoop 2.6.0\nRUN curl -f -s https://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz | tar -xz -C /usr/local/\nRUN cd /usr/local && ln -s spark-1.6.1-bin-hadoop2.6 spark\nENV SPARK_HOME /usr/local/spark\n\nENV PATH $SPARK_HOME/bin:$PATH\n\n# add spark and hadoop path to PATH env variable for kylo user\nRUN echo \"export PATH=$PATH:/usr/java/default/bin:/usr/local/spark/bin:/usr/local/hadoop/bin\" >> /etc/profile\n\n# Install hive\nRUN curl -f -s https://apache.mirrors.spacedump.net/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz | tar -xz -C /usr/local/\nRUN cd /usr/local && ln -s apache-hive-2.1.1-bin hive\nCOPY conf/hive-site.xml /usr/local/hive/conf\nRUN echo \"export HIVE_HOME=/usr/local/hive\" >> /etc/profile\nRUN echo \"export PATH=$PATH:/usr/local/hive/bin\">> /etc/profile\nENV HIVE_HOME /usr/local/hive\nENV PATH $PATH:$HIVE_HOME/bin\n# Create directory for hive logs\nRUN mkdir -p /var/log/hive\n# Increase PermGen space for hiveserver2 to fix OOM pb.\nCOPY conf/hive-env.sh /usr/local/hive/conf/\n\nRUN echo \"HADOOP_HOME=/usr/local/hadoop\" >> /usr/local/hive/bin/hive-config.sh\n\n# Download mysql jdbc driver and prepare hive metastore.\nRUN curl -f -s -o /usr/local/apache-hive-2.1.1-bin/lib/mysql-connector-java-5.1.41.jar https://central.maven.org/maven2/mysql/mysql-connector-java/5.1.41/mysql-connector-java-5.1.41.jar\n\n# create hiveserver2 service\nCOPY conf/hive-server2 /etc/init.d/\nRUN chmod +x /etc/init.d/hive-server2\n#RUN chkconfig --add /etc/init.d/hive-server2\n# ---- Hive installation finished -------\n\n# Prepare spark-hive integration, so spark sql will use hive tables defined in hive metastore, see https://spark.apache.org/docs/1.6.0/sql-programming-guide.html#hive-tables\nRUN cp /usr/local/hadoop/etc/hadoop/hdfs-site.xml /usr/local/spark/conf\nRUN cp /usr/local/hive/conf/hive-site.xml /usr/local/spark/conf\nRUN cp /usr/local/hive/lib/mysql-connector-java-5.1.41.jar /usr/local/spark/lib\n# ----- Spark-Hive integration finished ---------\n\n\n# create nifi user and group\nRUN /bin/bash -c 'useradd -r -m -s /bin/bash nifi'\n# create kylo user and group\nRUN /bin/bash -c 'useradd -r -m -s /bin/bash kylo'\n# Add kylo and nifi user to supergroup otherwise kylo-spark-shell service which runs as kylo user will not be able to create database in hive.\nRUN groupadd supergroup\nRUN usermod -a -G supergroup kylo\nRUN usermod -a -G supergroup nifi\n\nCOPY scripts/hadoop_bootstrap.sh /etc/hadoop_bootstrap.sh\nRUN chown root.root /etc/hadoop_bootstrap.sh\nRUN chmod 700 /etc/hadoop_bootstrap.sh\n\nENV BOOTSTRAP /etc/hadoop_bootstrap.sh\n\nENTRYPOINT [\"/etc/hadoop_bootstrap.sh\"]\n\n# Hdfs ports\nEXPOSE 50010 50020 50070 50075 50090 8020 9000\n# Mapred ports\nEXPOSE 10020 19888\n#Yarn ports\nEXPOSE 8030 8031 8032 8033 8040 8042 8088\n#Other ports\nEXPOSE 49707 2122\n#Hive\nEXPOSE 10000\n#Spark\nEXPOSE 8450 8451 4040"
}
{
  "startTime": 1674252024201,
  "endTime": 1674252024262,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 32,
        "lineEnd": 32,
        "columnStart": 4,
        "columnEnd": 53
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "FROM vnijs/rsm-msba:latest\n\nLABEL Vincent Nijs \"radiant@rady.ucsd.edu\"\n\nARG DOCKERHUB_VERSION_UPDATE\nENV DOCKERHUB_VERSION=${DOCKERHUB_VERSION_UPDATE}\n\nENV DEBIAN_FRONTEND=noninteractive\n# installing java\nRUN apt-get -y update \\\n  && apt-get install --no-install-recommends -y openjdk-8-jre-headless openjdk-8-jdk-headless ca-certificates-java \\\n  && apt-get clean \\\n  && rm -rf /var/lib/apt/lists/* \\\n  && R CMD javareconf\n\n# don't upgrade to 2.4.0 yet as it requires mesos and there is no repo for ubuntu 18.04 yet\nENV SPARK_VERSION=2.3.2 \\\n  HADOOP_VERSION=2.7\n\n# install the R kernel for Jupyter Lab\nRUN R -e 'options(spark.install.dir = \"/opt\")' \\\n  -e 'sparklyr::spark_install(version = Sys.getenv(\"SPARK_VERSION\"), hadoop_version = Sys.getenv(\"HADOOP_VERSION\"))'\n\n# setting environment variables for pyspark\nENV PYSPARK_PYTHON=/usr/bin/python3 \\\n  PYSPARK_DRIVER_PYTHON=jupyter \\\n  PYSPARK_DRIVER_PYTHON_OPTS=lab \\\n  SPARK_HOME=/opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}\nRUN echo \"SPARK_HOME=${SPARK_HOME}\" >> /etc/R/Renviron.site\n\n# install python packages\nCOPY requirements.txt /home/${NB_USER}/requirements.txt\nRUN pip3 install --no-cache-dir -r /home/${NB_USER}/requirements.txt \\\n  && rm /home/${NB_USER}/requirements.txt\n\n# update R-packages\nRUN R -e 'radiant.update::radiant.update()'\n\nEXPOSE 8080 8787 8989 8765\n\nCMD [\"/usr/bin/supervisord\", \"-c\", \"/etc/supervisor/conf.d/supervisord.conf\"]\n"
}
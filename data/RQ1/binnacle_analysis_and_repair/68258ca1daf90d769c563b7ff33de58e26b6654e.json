{
  "startTime": 1674235660716,
  "endTime": 1674235660868,
  "originalSmells": [
    {
      "rule": "pipUseNoCacheDir",
      "position": {
        "lineStart": 29,
        "lineEnd": 29,
        "columnStart": 4,
        "columnEnd": 40
      }
    }
  ],
  "repairedSmells": [],
  "repairedDockerfile": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nFROM spark-base\n\n# If this docker file is being used in the context of building your images from a Spark distribution, the docker build\n# command should be invoked from the top level directory of the Spark distribution. E.g.:\n# docker build -t spark-driver-py:latest -f dockerfiles/driver-py/Dockerfile .\n\nADD examples /opt/spark/examples\nADD python /opt/spark/python\n\nRUN apk add --no-cache python && \\\n    python -m ensurepip && \\\n    rm -r /usr/lib/python*/ensurepip && \\\n    pip install --no-cache-dir --upgrade pip setuptools && \\\n    rm -r /root/.cache\n# UNCOMMENT THE FOLLOWING TO START PIP INSTALLING PYTHON PACKAGES\n# RUN apk add --update alpine-sdk python-dev\n# RUN pip install numpy\n\nENV PYTHON_VERSION 2.7.13\nENV PYSPARK_PYTHON python\nENV PYSPARK_DRIVER_PYTHON python\nENV PYTHONPATH ${SPARK_HOME}/python/:${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${PYTHONPATH}\n\nCMD SPARK_CLASSPATH=\"${SPARK_HOME}/jars/*\" && \\\n    env | grep SPARK_JAVA_OPT_ | sed 's/[^=]*=\\(.*\\)/\\1/g' > /tmp/java_opts.txt && \\\n    readarray -t SPARK_DRIVER_JAVA_OPTS < /tmp/java_opts.txt && \\\n    if ! [ -z ${SPARK_MOUNTED_CLASSPATH+x} ]; then SPARK_CLASSPATH=\"$SPARK_MOUNTED_CLASSPATH:$SPARK_CLASSPATH\"; fi && \\\n    if ! [ -z ${SPARK_SUBMIT_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH=\"$SPARK_SUBMIT_EXTRA_CLASSPATH:$SPARK_CLASSPATH\"; fi && \\\n    if ! [ -z ${SPARK_EXTRA_CLASSPATH+x} ]; then SPARK_CLASSPATH=\"$SPARK_EXTRA_CLASSPATH:$SPARK_CLASSPATH\"; fi && \\\n    if ! [ -z ${HADOOP_CONF_DIR+x} ]; then SPARK_CLASSPATH=\"$HADOOP_CONF_DIR:$SPARK_CLASSPATH\"; fi && \\\n    if ! [ -z ${SPARK_MOUNTED_FILES_DIR+x} ]; then cp -R \"$SPARK_MOUNTED_FILES_DIR/.\" .; fi && \\\n    if ! [ -z ${SPARK_MOUNTED_FILES_FROM_SECRET_DIR+x} ]; then cp -R \"$SPARK_MOUNTED_FILES_FROM_SECRET_DIR/.\" .; fi && \\\n    ${JAVA_HOME}/bin/java \"${SPARK_DRIVER_JAVA_OPTS[@]}\" -cp \"$SPARK_CLASSPATH\" -Xms$SPARK_DRIVER_MEMORY -Xmx$SPARK_DRIVER_MEMORY -Dspark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS $SPARK_DRIVER_CLASS $PYSPARK_PRIMARY $PYSPARK_FILES $SPARK_DRIVER_ARGS\n"
}